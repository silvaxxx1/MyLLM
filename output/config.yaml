batch_size: 8
block_size: 1024
data_path: null
dataloader_num_workers: 4
dataset_name: null
deepspeed_config: null
device: auto
distributed: false
eval_steps: 500
extra_config: {}
gradient_accumulation_steps: 2
greater_is_better: false
learning_rate: 5.0e-05
load_best_model_at_end: true
local_rank: -1
log_model: true
log_predictions: false
logging_backend: wandb
logging_steps: 50
max_grad_norm: 1.0
max_seq_length: 1024
metric_for_best_model: eval_loss
mixed_precision: true
mlm_probability: 0.15
model_name_or_path: gpt2
num_epochs: 3
optimizer: adamw
output_dir: ./output
overwrite_cache: false
preprocessing_num_workers: 4
report_to:
- wandb
- tensorboard
resume_from_checkpoint: null
save_steps: 1000
save_total_limit: 3
scheduler: linear
seed: 42
tensorboard_log_dir: null
wandb_config_exclude: []
wandb_entity: null
wandb_notes: Demo run with enhanced logging framework
wandb_project: ml-training-demo
wandb_resume: false
wandb_run_name: gpt2-pretraining-example
wandb_tags:
- pretraining
- gpt2
- demo
warmup_steps: 100
weight_decay: 0.01
world_size: 1
