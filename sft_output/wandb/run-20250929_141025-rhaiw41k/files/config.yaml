_wandb:
    value:
        cli_version: 0.21.4
        e:
            kj6396cfousxe1carcggi4jmxgcopydv:
                email: silvapi1994@gmail.com
                executable: /home/silva/SILVA.AI/Projects/MyLLM/.venv/bin/python
                git:
                    commit: 33dd9335cf87566e7c81c46b1e3d5ce4235a2817
                    remote: git@github.com:silvaxxx1/MyLLM.git
                host: SilvaLAB
                os: Linux-6.14.0-32-generic-x86_64-with-glibc2.39
                program: -m myllm.Train.sft_test
                python: CPython 3.12.3
                root: ./sft_output
                startedAt: "2025-09-29T11:10:25.048756Z"
                writerId: kj6396cfousxe1carcggi4jmxgcopydv
        m: []
        python_version: 3.12.3
        t:
            "1":
                - 1
                - 2
                - 3
                - 49
            "2":
                - 1
                - 2
                - 3
                - 49
            "3":
                - 15
                - 16
            "4": 3.12.3
            "5": 0.21.4
            "12": 0.21.4
            "13": linux-x86_64
batch_size:
    value: 2
beta1:
    value: null
beta2:
    value: null
data_path:
    value: null
dataloader_num_workers:
    value: 4
dataset_name:
    value: null
dataset_text_field:
    value: text
deepspeed_config_path:
    value: null
device:
    value: AUTO
distributed_backend:
    value: nccl
eval_steps:
    value: 5
gradient_accumulation_steps:
    value: 1
greater_is_better:
    value: true
instruction_template:
    value: |-
        ### Instruction:
        {instruction}

        ### Response:
        {response}
learning_rate:
    value: 5e-05
load_best_model_at_end:
    value: false
local_rank:
    value: -1
log_model:
    value: true
log_predictions:
    value: false
logging_steps:
    value: 50
max_grad_norm:
    value: 1
max_seq_length:
    value: null
metric_for_best_model:
    value: null
mixed_precision:
    value: true
model_config_name:
    value: gpt2-small
model_config_path:
    value: null
num_epochs:
    value: 2
optimizer_type:
    value: ADAMW
packing:
    value: false
peft_config:
    value: null
preprocessing_num_workers:
    value: 4
report_to:
    value:
        - wandb
response_template:
    value: '### Response:'
resume_from_checkpoint:
    value: null
save_steps:
    value: 10
save_total_limit:
    value: 3
scheduler_type:
    value: LINEAR
seed:
    value: 42
tensorboard_log_dir:
    value: null
tokenizer_name:
    value: gpt2
use_deepspeed:
    value: false
use_peft:
    value: false
warmup_steps:
    value: 0
weight_decay:
    value: null
