{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **Welcome to Notebook 2.1: Diving Deep into Tokenization** üåä\n",
    "\n",
    "üìö **Previous Adventure**:  \n",
    "In our last notebook, we dipped our toes into tokenization with `tiktoken` and built our first dataset. Now, we‚Äôre strapping on our scuba gear to **explore the ocean of tokenization** ‚Äî where every byte, subword, and special character hides secrets!  \n",
    "\n",
    "## üß© **What is Tokenization? Breaking the Code** üîç  \n",
    "\n",
    "**Tokenization** is like chopping a üçé *sentence* into bite-sized üç£ *pieces* (tokens!). These tokens can be:  \n",
    "- **Words** (\"hello\")  \n",
    "- **Subwords** (\"un+break+able\")  \n",
    "- **Characters** (\"h\", \"e\", \"l\", \"l\", \"o\")  \n",
    "- **Special symbols** (\"<|EOS|>\")  \n",
    "\n",
    "**Why does this matter?**  \n",
    "```python\n",
    "# Without tokenization, models see this:\n",
    "\"Th!s 1s gibrish_2_LLMs\"\n",
    "# With tokenization:\n",
    "[\"Th\", \"!s\", \" 1\", \"s\", \" gib\", \"rish\", \"_2\", \"_LLMs\"]\n",
    "```\n",
    "\n",
    "## üåü **Why Tokenization Rules NLP** üëë\n",
    "\n",
    "### üìå **4 Reasons Tokenization is ESSENTIAL**  \n",
    "1. **üî¢ Text ‚Üí Numbers**:  \n",
    "   *\"Hello\"* ‚Üí `[15496, 23]` (models speak math, not poetry!)  \n",
    "\n",
    "2. **üõ†Ô∏è Vocabulary Control**:  \n",
    "   Slash 100k+ word chaos into reusable subword Lego bricks üß±.  \n",
    "\n",
    "3. **‚ö° Turbo Efficiency**:  \n",
    "   Smaller token sets = Faster training + Smarter models üöÑ.  \n",
    "\n",
    "4. **üéØ Context Mastery**:  \n",
    "   Tokens preserve meaning like a puzzle ‚Äî `\"bank\"` (üí∞ river) vs. `\"bank\"` (üè¶ financial).  \n",
    "\n",
    "## üí• **Tokenization Gone Wrong: A Horror Story** üëª  \n",
    "\n",
    "**Example: The Math Meltdown**  \n",
    "```diff\n",
    "+ GOOD TOKENIZER:\n",
    "  \"2 + 2 = 4\" ‚Üí [\"2\", \"+\", \"2\", \"=\", \"4\"] ‚Üí Model thinks: 4 ‚úÖ\n",
    "\n",
    "- BAD TOKENIZER:\n",
    "  \"2 + 2 = 4\" ‚Üí [\"2+\", \"2=\", \"4\"] ‚Üí Model outputs: \"5\" üò±\n",
    "```\n",
    "\n",
    "**Why care?**  \n",
    "A single tokenization error can turn GPT-4 into a üßü *math zombie*!  \n",
    "\n",
    "---\n",
    "\n",
    "**üî• Ready to become a Tokenization Titan? Let‚Äôs dive in!** üèä‚ôÇÔ∏è  \n",
    "\n",
    "```diff\n",
    "+ PRO TIP: Keep this notebook handy ‚Äî tokenization skills are the skeleton key to NLP mastery! üîë\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load some data to work with\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of th text : 20479)\n",
      "first 1000 chars:I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
      "\n",
      "Well!--even through th)\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as f:\n",
    "    text = f.read() \n",
    "\n",
    "print(f\"the length of th text : {len(text)})\")\n",
    "print(f\"first 1000 chars:{text[:1000]})\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the previous notebook we take a look at the all the unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 62\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "0 -> \n",
      "\n",
      "1 ->  \n",
      "2 -> !\n",
      "3 -> \"\n",
      "4 -> '\n",
      "5 -> (\n",
      "6 -> )\n",
      "7 -> ,\n",
      "8 -> -\n",
      "9 -> .\n",
      "10 -> :\n",
      "11 -> ;\n",
      "12 -> ?\n",
      "13 -> A\n",
      "14 -> B\n",
      "15 -> C\n",
      "16 -> D\n",
      "17 -> E\n",
      "18 -> F\n",
      "19 -> G\n",
      "20 -> H\n",
      "21 -> I\n",
      "22 -> J\n",
      "23 -> L\n",
      "24 -> M\n",
      "25 -> N\n",
      "26 -> O\n",
      "27 -> P\n",
      "28 -> R\n",
      "29 -> S\n",
      "30 -> T\n",
      "31 -> U\n",
      "32 -> V\n",
      "33 -> W\n",
      "34 -> Y\n",
      "35 -> _\n",
      "36 -> a\n",
      "37 -> b\n",
      "38 -> c\n",
      "39 -> d\n",
      "40 -> e\n",
      "41 -> f\n",
      "42 -> g\n",
      "43 -> h\n",
      "44 -> i\n",
      "45 -> j\n",
      "46 -> k\n",
      "47 -> l\n",
      "48 -> m\n",
      "49 -> n\n",
      "50 -> o\n",
      "51 -> p\n",
      "52 -> q\n",
      "53 -> r\n",
      "54 -> s\n",
      "55 -> t\n",
      "56 -> u\n",
      "57 -> v\n",
      "58 -> w\n",
      "59 -> x\n",
      "60 -> y\n",
      "61 -> z\n"
     ]
    }
   ],
   "source": [
    "chars  = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters: {vocab_size}\")\n",
    "print(chars)    \n",
    "for i, char in enumerate(chars):\n",
    "    print(f\"{i} -> {char}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very vasic tokenizer is built with a simple mapping between the char and thier int as you see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [43, 40, 47, 47, 50]\n",
      "Decoded: hello\n"
     ]
    }
   ],
   "source": [
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s:[char_to_int[c] for c in s]\n",
    "decode = lambda x:\"\".join([int_to_char[i] for i in x])\n",
    "\n",
    "\n",
    "print(f\"Encoded: {encode('hello')}\")\n",
    "print(f\"Decoded: {decode(encode('hello'))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an **embedding lookup table** where each token (represented by an integer index) maps to a dense vector of size `embedding_dim`. \n",
    "\n",
    "For example:\n",
    "- Token `0` ‚Üí Vector of shape `(10,)`\n",
    "- Token `1` ‚Üí Vector of shape `(10,)`\n",
    "- ...\n",
    "- Token `vocab_size-1` ‚Üí Vector of shape `(10,)`\n",
    "\n",
    "The `nn.Embedding` layer acts like a dictionary:  \n",
    "`Input: token index (int)` ‚Üí `Output: corresponding vector (torch.Tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 1000  # Define vocabulary size\n",
    "embedding_dim = 10\n",
    "\n",
    "# Create embedding table\n",
    "embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Shape: (num_embeddings, embedding_dim)\n",
    "print(embedding_table.weight.shape)  # Output: torch.Size([1000, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visulize this for better understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets visualize the embedding table\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_embedding_table(embedding_table, int_to_char):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(embedding_table.weight.detach().numpy(), cmap='viridis')\n",
    "    ax.set_yticks(list(int_to_char.keys()))\n",
    "    ax.set_yticklabels(list(int_to_char.values()))\n",
    "    ax.set_xticks(list(range(embedding_table.weight.shape[1])))\n",
    "    ax.set_xticklabels(list(range(embedding_table.weight.shape[1])))\n",
    "    plt.show()\n",
    "\n",
    "plot_embedding_table(embesdding_table, int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UTF-8 & Text Encoding Fundamentals** üî§\n",
    "\n",
    "**What is UTF-8?**  \n",
    "UTF-8 (Unicode Transformation Format - 8-bit) is the dominant character encoding standard that:\n",
    "- Represents **every Unicode character** (supports 1.1M+ symbols)\n",
    "- Uses **1-4 bytes per character** (dynamic width)\n",
    "- Backward-compatible with ASCII (first 128 characters match)\n",
    "\n",
    "**Why UTF-8 for Tokenization?**  \n",
    "Modern tokenizers use UTF-8 encoding to:\n",
    "1. Handle **multi-language text** seamlessly\n",
    "2. Process **special characters** (emojis, symbols, etc.)\n",
    "3. Create consistent byte-level representations before tokenization\n",
    "\n",
    "\n",
    "This byte representation becomes the foundation for tokenization algorithms like BPE (Byte-Pair Encoding) used in modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text raw: b'Hi world \\xf0\\x9f\\x8c\\x8d, how are you? \\xf0\\x9f\\x98\\x8a'\n",
      "Encoded text: [72, 105, 32, 119, 111, 114, 108, 100, 32, 240, 159, 140, 141, 44, 32, 104, 111, 119, 32, 97, 114, 101, 32, 121, 111, 117, 63, 32, 240, 159, 152, 138]\n"
     ]
    }
   ],
   "source": [
    "# lets use sample text with an emoji\n",
    "sampel_text = \"Hi world üåç, how are you? üòä\"\n",
    "\n",
    "# using encoding UTF-8\n",
    "encoded_text = sampel_text.encode(\"utf-8\")\n",
    "print(f\"Encoded text raw: {encoded_text}\")\n",
    "print(f\"Encoded text: {list(encoded_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's why UTF-8 is preferred over UTF-16/32 for text processing and tokenization in modern NLP:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Space Efficiency** üíæ\n",
    "| Encoding | ASCII (1 byte) | Common Latin (2 bytes) | Emojis/Special (4 bytes) |\n",
    "|----------|----------------|------------------------|--------------------------|\n",
    "| UTF-8    | 1 byte         | 2 bytes                | 4 bytes                  |\n",
    "| UTF-16   | 2 bytes        | 2 bytes                | 4 bytes                  |\n",
    "| UTF-32   | 4 bytes        | 4 bytes                | 4 bytes                  |\n",
    "\n",
    "**Example with your text**:\n",
    "```python\n",
    "text = \"Hi world üåç, how are you? üòä\"\n",
    "\n",
    "# Byte counts:\n",
    "UTF-8  ‚Üí 25 bytes  \n",
    "UTF-16 ‚Üí 46 bytes  (+84% larger)\n",
    "UTF-32 ‚Üí 92 bytes  (+268% larger)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Compatibility & Adoption** üåê\n",
    "- **UTF-8**:\n",
    "  - Backward compatible with ASCII\n",
    "  - Used by 98% of web pages ([W3Techs](https://w3techs.com/technologies/overview/character_encoding))\n",
    "  - Default encoding in Python/Linux/Modern APIs\n",
    "\n",
    "- **UTF-16/32**:\n",
    "  - Requires byte-order mark (BOM) for endianness\n",
    "  - Mainly used in legacy systems (Windows APIs, Java)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Tokenization Advantages** üî†\n",
    "Modern tokenizers (BERT, GPT, etc.) use **byte-level operations**:\n",
    "```python\n",
    "# UTF-8 allows cleaner byte-to-token mapping\n",
    "text = \"üåç\"\n",
    "utf8_bytes = [240, 159, 140, 141]  # Clear 4-byte emoji\n",
    "utf16_bytes = [255, 254, 61, 216, 13, 220]  # BOM + surrogate pairs\n",
    "```\n",
    "\n",
    "**Why this matters**:\n",
    "- BPE (Byte-Pair Encoding) works directly with UTF-8 bytes\n",
    "- Fixed 4-byte UTF-32 wastes space for common characters\n",
    "- UTF-16's surrogate pairs complicate token merging\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Real-World Impact** üöÄ\n",
    "| Scenario               | UTF-8 Advantage                                |\n",
    "|------------------------|-----------------------------------------------|\n",
    "| Model Training          | Smaller datasets ‚Üí Faster training            |\n",
    "| API Payloads            | Reduced bandwidth usage                       |\n",
    "| Multilingual Support    | Efficient mixing of Latin/Non-Latin scripts   |\n",
    "| Emoji Handling          | Consistent 4-byte representation              |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use UTF-16/32?**\n",
    "- **UTF-16**: Memory-constrained Asian language apps (fixed 2 bytes per common Hanzi)\n",
    "- **UTF-32**: Rare text processing requiring O(1) character indexing\n",
    "\n",
    "For 99% of NLP use cases, **UTF-8 is the optimal choice** for balancing efficiency, compatibility, and modern tokenization needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text in raw bytes: [255, 254, 72, 0, 105, 0, 32, 0, 119, 0, 111, 0, 114, 0, 108, 0, 100, 0, 32, 0, 60, 216, 13, 223, 44, 0, 32, 0, 104, 0, 111, 0, 119, 0, 32, 0, 97, 0, 114, 0, 101, 0, 32, 0, 121, 0, 111, 0, 117, 0, 63, 0, 32, 0, 61, 216, 10, 222]\n",
      "Encoded text in raw bytes: [255, 254, 0, 0, 72, 0, 0, 0, 105, 0, 0, 0, 32, 0, 0, 0, 119, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 108, 0, 0, 0, 100, 0, 0, 0, 32, 0, 0, 0, 13, 243, 1, 0, 44, 0, 0, 0, 32, 0, 0, 0, 104, 0, 0, 0, 111, 0, 0, 0, 119, 0, 0, 0, 32, 0, 0, 0, 97, 0, 0, 0, 114, 0, 0, 0, 101, 0, 0, 0, 32, 0, 0, 0, 121, 0, 0, 0, 111, 0, 0, 0, 117, 0, 0, 0, 63, 0, 0, 0, 32, 0, 0, 0, 10, 246, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# why no using UTF-16 or UTF-32\n",
    "encoded_text = sampel_text.encode(\"utf-16\")\n",
    "print(f\"Encoded text in raw bytes: {list(encoded_text)}\")\n",
    "\n",
    "encoded_text = sampel_text.encode(\"utf-32\")\n",
    "print(f\"Encoded text in raw bytes: {list(encoded_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________\n",
      "\n",
      "Hello, world! üåé I hope you're all doing fantastic today. üåü Life is full of surprises, and every day is a new opportunity to grow, learn, and smile. üòä  \n",
      "Take a deep breath, step outside, and enjoy the little things üå∏‚Äîlike the sound of birds chirping üê¶, the warmth of the sun ‚òÄÔ∏è, or even a good cup of coffee ‚òï.  \n",
      "\n",
      "Remember, challenges are just opportunities in disguise. üí™ So keep pushing forward, believe in yourself, and never stop dreaming. üåà  \n",
      "Oh, and don‚Äôt forget to share kindness and spread positivity wherever you go. ‚ù§Ô∏è  \n",
      "Have an amazing day ahead! üöÄ\n",
      "\n",
      "length of the text:  560\n",
      "_________________________\n",
      "Encoded text:  [10, 72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33, 32, 240, 159, 140, 142, 32, 73, 32, 104, 111, 112, 101, 32, 121, 111, 117, 39, 114, 101, 32, 97, 108, 108, 32, 100, 111, 105, 110, 103, 32, 102, 97, 110, 116, 97, 115, 116, 105, 99, 32, 116, 111, 100, 97, 121, 46, 32, 240, 159, 140, 159, 32, 76, 105, 102, 101, 32, 105, 115, 32, 102, 117, 108, 108, 32, 111, 102, 32, 115, 117, 114, 112, 114, 105, 115, 101, 115, 44, 32, 97, 110, 100, 32, 101, 118, 101, 114, 121, 32, 100, 97, 121, 32, 105, 115, 32, 97, 32, 110, 101, 119, 32, 111, 112, 112, 111, 114, 116, 117, 110, 105, 116, 121, 32, 116, 111, 32, 103, 114, 111, 119, 44, 32, 108, 101, 97, 114, 110, 44, 32, 97, 110, 100, 32, 115, 109, 105, 108, 101, 46, 32, 240, 159, 152, 138, 32, 32, 10, 84, 97, 107, 101, 32, 97, 32, 100, 101, 101, 112, 32, 98, 114, 101, 97, 116, 104, 44, 32, 115, 116, 101, 112, 32, 111, 117, 116, 115, 105, 100, 101, 44, 32, 97, 110, 100, 32, 101, 110, 106, 111, 121, 32, 116, 104, 101, 32, 108, 105, 116, 116, 108, 101, 32, 116, 104, 105, 110, 103, 115, 32, 240, 159, 140, 184, 226, 128, 148, 108, 105, 107, 101, 32, 116, 104, 101, 32, 115, 111, 117, 110, 100, 32, 111, 102, 32, 98, 105, 114, 100, 115, 32, 99, 104, 105, 114, 112, 105, 110, 103, 32, 240, 159, 144, 166, 44, 32, 116, 104, 101, 32, 119, 97, 114, 109, 116, 104, 32, 111, 102, 32, 116, 104, 101, 32, 115, 117, 110, 32, 226, 152, 128, 239, 184, 143, 44, 32, 111, 114, 32, 101, 118, 101, 110, 32, 97, 32, 103, 111, 111, 100, 32, 99, 117, 112, 32, 111, 102, 32, 99, 111, 102, 102, 101, 101, 32, 226, 152, 149, 46, 32, 32, 10, 10, 82, 101, 109, 101, 109, 98, 101, 114, 44, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 115, 32, 97, 114, 101, 32, 106, 117, 115, 116, 32, 111, 112, 112, 111, 114, 116, 117, 110, 105, 116, 105, 101, 115, 32, 105, 110, 32, 100, 105, 115, 103, 117, 105, 115, 101, 46, 32, 240, 159, 146, 170, 32, 83, 111, 32, 107, 101, 101, 112, 32, 112, 117, 115, 104, 105, 110, 103, 32, 102, 111, 114, 119, 97, 114, 100, 44, 32, 98, 101, 108, 105, 101, 118, 101, 32, 105, 110, 32, 121, 111, 117, 114, 115, 101, 108, 102, 44, 32, 97, 110, 100, 32, 110, 101, 118, 101, 114, 32, 115, 116, 111, 112, 32, 100, 114, 101, 97, 109, 105, 110, 103, 46, 32, 240, 159, 140, 136, 32, 32, 10, 79, 104, 44, 32, 97, 110, 100, 32, 100, 111, 110, 226, 128, 153, 116, 32, 102, 111, 114, 103, 101, 116, 32, 116, 111, 32, 115, 104, 97, 114, 101, 32, 107, 105, 110, 100, 110, 101, 115, 115, 32, 97, 110, 100, 32, 115, 112, 114, 101, 97, 100, 32, 112, 111, 115, 105, 116, 105, 118, 105, 116, 121, 32, 119, 104, 101, 114, 101, 118, 101, 114, 32, 121, 111, 117, 32, 103, 111, 46, 32, 226, 157, 164, 239, 184, 143, 32, 32, 10, 72, 97, 118, 101, 32, 97, 110, 32, 97, 109, 97, 122, 105, 110, 103, 32, 100, 97, 121, 32, 97, 104, 101, 97, 100, 33, 32, 240, 159, 154, 128, 10]\n",
      "length of the encoded text:  598\n"
     ]
    }
   ],
   "source": [
    "longer_text =  \"\"\"\n",
    "Hello, world! üåé I hope you're all doing fantastic today. üåü Life is full of surprises, and every day is a new opportunity to grow, learn, and smile. üòä  \n",
    "Take a deep breath, step outside, and enjoy the little things üå∏‚Äîlike the sound of birds chirping üê¶, the warmth of the sun ‚òÄÔ∏è, or even a good cup of coffee ‚òï.  \n",
    "\n",
    "Remember, challenges are just opportunities in disguise. üí™ So keep pushing forward, believe in yourself, and never stop dreaming. üåà  \n",
    "Oh, and don‚Äôt forget to share kindness and spread positivity wherever you go. ‚ù§Ô∏è  \n",
    "Have an amazing day ahead! üöÄ\n",
    "\"\"\"\n",
    "# using encoding UTF-8 to get the raw bytes stream\n",
    "tokens = longer_text.encode(\"utf-8\")\n",
    "tokens=list(map(int , tokens))\n",
    "print(\"_________________________\")\n",
    "print(longer_text)\n",
    "print(\"length of the text: \", len(longer_text))\n",
    "print(\"_________________________\")\n",
    "print(\"Encoded text: \", tokens)\n",
    "print(\"length of the encoded text: \", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE) Explained Concisely üß©\n",
    "\n",
    "## **What is BPE?**  \n",
    "A *subword tokenization* algorithm that merges frequent character pairs to create reusable units, balancing word-level meaning and character-level flexibility.\n",
    "\n",
    "## **How BPE Works** (Visual Walkthrough)\n",
    "\n",
    "### **1. Training Corpus**  \n",
    "`[\"low\", \"lower\", \"newest\", \"newer\"]`\n",
    "\n",
    "### **2. Step-by-Step Process**  \n",
    "**Initial Split** ‚Üí Characters:  \n",
    "`l o w | l o w e r | n e w e s t | n e w e r`\n",
    "\n",
    "**Merge Most Frequent Pairs**:  \n",
    "1Ô∏è‚É£ `l` + `o` ‚Üí **`lo`** (2x)  \n",
    "   ‚Üí `lo w | lo w e r | n e w e s t | n e w e r`  \n",
    "2Ô∏è‚É£ `lo` + `w` ‚Üí **`low`** (2x)  \n",
    "   ‚Üí `low | low e r | n e w e s t | n e w e r`  \n",
    "3Ô∏è‚É£ `e` + `r` ‚Üí **`er`** (2x)  \n",
    "   ‚Üí `low | lower | n e w e s t | newer`\n",
    "\n",
    "### **3. Final Tokens**  \n",
    "`[\"low\", \"er\", \"n\", \"e\", \"w\", \"es\", \"t\"]`\n",
    "\n",
    "Here's a **concise technical blueprint** for building a BPE tokenizer:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Core Components**  \n",
    "1. **Frequency Analyzer**  \n",
    "   - *Goal*: Identify most common adjacent token pairs in a corpus  \n",
    "   - *Method*: Sliding window scan + hashmap counting  \n",
    "   - *Key*: Optimized for large datasets (stream processing)  \n",
    "\n",
    "2. **Pair Merger**  \n",
    "   - *Goal*: Replace frequent pairs with new unified tokens  \n",
    "   - *Method*: Iterative replacement + vocabulary expansion  \n",
    "   - *Key*: Preserve merge order for consistent encoding  \n",
    "\n",
    "3. **Encoder**  \n",
    "   - *Pipeline*: Text ‚Üí UTF-8 bytes ‚Üí apply merges ‚Üí token IDs  \n",
    "   - *Optimization*: O(1) pair lookups via precomputed merge rules  \n",
    "\n",
    "4. **Decoder**  \n",
    "   - *Pipeline*: Token IDs ‚Üí byte sequences ‚Üí UTF-8 text  \n",
    "   - *Edge Handling*: Graceful unknown token fallback  \n",
    "\n",
    "5. **Tokenizer Class**  \n",
    "   - *State*: `merges`, `vocab`, `next_id`  \n",
    "   - *Methods*: `train()`, `encode()`, `decode()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pairs: 4\n",
      "[((2, 2), 2), ((2, 3), 2), ((1, 2), 1), ((3, 2), 1)]\n"
     ]
    }
   ],
   "source": [
    "def most_common_pairs(ids):\n",
    "    \"\"\"Counts consecutive ID pairs in a token sequence.\n",
    "    \n",
    "    Args:\n",
    "        ids: List of token IDs representing encoded text\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with (id1, id2) pairs as keys and counts as values\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    # Iterate through consecutive pairs using zip:\n",
    "    # zip(ids, ids[1:]) creates pairs like (ids[0], ids[1]), (ids[1], ids[2]), etc.\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        # Get current count or default to 0, then increment\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# Example usage with token IDs (pretend these are byte values)\n",
    "tokens = [1, 2, 2, 3, 2, 2, 3]  # Sample token sequence\n",
    "counts = most_common_pairs(tokens)\n",
    "\n",
    "print(f\"Number of unique pairs: {len(counts)}\")\n",
    "# Output: Number of unique pairs: 3\n",
    "\n",
    "# Sort pairs by frequency descending using:\n",
    "# - Generator expression to iterate through dict items\n",
    "# - sorted() with reverse=True for descending order\n",
    "sorted_counts = sorted(((k,v) for k,v in counts.items()), key=lambda x: -x[1])\n",
    "print(sorted_counts)\n",
    "# Output: [((2, 2), 2), ((2, 3), 2), ((1, 2), 1), ((3, 2), 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what pair with the most appreance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(counts, key=counts.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to implemet a funtion that merge the pair we selected in the previuos function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 3, 4, 2, 3, 5]\n",
      "After merging (2, 3) ‚Üí 9: [1, 9, 4, 9, 5]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    Replaces all occurrences of a specified pair in a token sequence with a new index.\n",
    "    \n",
    "    Args:\n",
    "        ids (list): List of token IDs to process\n",
    "        pair (tuple): Pair of consecutive token IDs to replace (left, right)\n",
    "        idx (int): New token ID to substitute for the pair\n",
    "    \n",
    "    Returns:\n",
    "        list: New list of token IDs with replacements applied\n",
    "    \"\"\"\n",
    "    new_ids = []  # Initialize list to store merged tokens\n",
    "    i = 0  # Index pointer for original ids list\n",
    "    \n",
    "    # Iterate through the token sequence\n",
    "    while i < len(ids):\n",
    "        # Check if we can look ahead and find the target pair\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
    "            # Replace pair with new index\n",
    "            new_ids.append(idx)\n",
    "            # Skip over the second element of the pair\n",
    "            i += 2\n",
    "        else:\n",
    "            # Keep original token if no pair found\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "            \n",
    "    return new_ids\n",
    "\n",
    "# Example usage\n",
    "ids = [1, 2, 3, 4, 2, 3, 5]  # Original token sequence\n",
    "pair = (2, 3)  # Pair to find and replace\n",
    "idx = 9  # Replacement token ID\n",
    "\n",
    "result = merge(ids, pair, idx)\n",
    "print(f\"Original: {ids}\")\n",
    "print(f\"After merging {pair} ‚Üí {idx}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the new tokens:  5\n",
      "length of the original tokens:  7\n"
     ]
    }
   ],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(\"length of the new tokens: \", len(tokens2))\n",
    "print(\"length of the original tokens: \", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try woking with larger text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text2 = text.encode(\"utf-8\")\n",
    "full_tokens = list(map(int, encoded_text2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"A Programmer‚Äôs Introduction to Unicode March 3, 2017 ¬∑ Coding ¬∑ 22 Comments  ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫\\u200cüá≥\\u200cüáÆ\\u200cüá®\\u200cüá¥\\u200cüá©\\u200cüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I‚Äôll give an introduction to it from a programmer‚Äôs point of view.  I‚Äôm going to focus on the character set and what‚Äôs involved in working with strings and files of Unicode text. However, in this article I‚Äôm not going to talk about fonts, text layout/shaping/rendering, or localization in detail‚Äîthose are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More‚Ä¶ Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It‚Äôs not just that Unicode contains a much larger number of characters, although that‚Äôs part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere ‚Äúcharacter set‚Äù to be. We‚Äôll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it‚Äôs hard not to find oneself asking, ‚ÄúWhy do we need all this? Is this really necessary? Couldn‚Äôt it be simplified?‚Äù  However, Unicode aims to faithfully represent the entire world‚Äôs writing systems. The Unicode Consortium‚Äôs stated goal is ‚Äúenabling people around the world to use computers in any language‚Äù. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there‚Äôs still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it‚Äôs inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn‚Äôt make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text‚Äîwhich introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you‚Äôll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don‚Äôt be discouraged‚Äîthink about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let‚Äôs start with some general orientation. The basic elements of Unicode‚Äîits ‚Äúcharacters‚Äù, although that term isn‚Äôt quite right‚Äîare called code points. Code points are identified by number, customarily written in hexadecimal with the prefix ‚ÄúU+‚Äù, such as U+0041 ‚ÄúA‚Äù latin capital letter a or U+03B8 ‚ÄúŒ∏‚Äù greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them‚Äîabout 12% of the codespace‚Äîare actually assigned, to date. There‚Äôs plenty of room for growth! Unicode also reserves an additional 137,468 code points as ‚Äúprivate use‚Äù areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it‚Äôs helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It‚Äôs arranged in tiles for visual coherence; each small square is 16√ó16 = 256 code points, and each large square is a ‚Äúplane‚Äù of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the ‚ÄúBasic Multilingual Plane‚Äù, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more‚ÄîUnicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15‚Äì16 are reserved entirely for private use.  Scripts Let‚Äôs zoom in on the first three planes, since that‚Äôs where the action is:  Map of scripts in Unicode planes 0‚Äì2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility‚Äîit‚Äôs easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage‚Äîin other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0‚Äì2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0‚Äì2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1‚Äì2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We‚Äôve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = ‚ÄúUnicode Transformation Format‚Äù), but it‚Äôs rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you‚Äôll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it‚Äôs a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000‚ÄìU+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080‚ÄìU+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800‚ÄìU+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000‚ÄìU+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128‚Äì255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms‚Äîsuch as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)‚Äîwill just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it‚Äôs relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn‚Äôt a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the ‚Äúcharacters‚Äù in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters‚Äîmore about those later), not bytes. When you measure the ‚Äúlength‚Äù of a string, you‚Äôll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you‚Äôre likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000‚ÄìU+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000‚ÄìU+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called ‚Äúsurrogates‚Äù. All the code points in the range U+D800‚ÄìU+DFFF‚Äîor in other words, the code points that match the binary prefixes 110110 and 110111 in the table above‚Äîare reserved specifically for UTF-16 encoding, and don‚Äôt represent any valid characters on their own. They‚Äôre only meant to occur in the 2-word encoding pattern above, which is called a ‚Äúsurrogate pair‚Äù. Surrogate code points are illegal in any other context! They‚Äôre not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different ‚Äúencodings‚Äù; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn‚Äôt originally plan for. Surrogates were then introduced, as‚Äîto put it bluntly‚Äîa kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn‚Äôt support UTF-8‚Äîonly legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! üòä)  By the way, UTF-16‚Äôs words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn‚Äôt match the system‚Äôs endianness, the BOM will be decoded as U+FFFE, which isn‚Äôt a valid code point.)  Combining Marks In the story so far, we‚Äôve been focusing on code points. But in Unicode, a ‚Äúcharacter‚Äù can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet‚Äîand in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called ‚Äúcombining marks‚Äù, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character ‚Äú√Å‚Äù can be expressed as a string of two code points: U+0041 ‚ÄúA‚Äù latin capital letter a plus U+0301 ‚Äú‚óåÃÅ‚Äù combining acute accent. This string automatically gets rendered as a single character: ‚Äú√Å‚Äù.  Now, Unicode does also include many ‚Äúprecomposed‚Äù code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 ‚Äú√Å‚Äù latin capital letter a with acute or U+1EC7 ‚Äú·ªá‚Äù latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don‚Äôt use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ÕñÕüÕÖrÕûa·πãÃ´Ã†ÃñÕàÃódÕñÃªÃπ√≥mÃ™ÕôÕïÃóÃùƒºÕáÃ∞ÕìÃ≥Ã´√ΩÕìÃ•ÃüÕç ÃïsÃ´tÃ´Ã±ÕïÃóÃ∞ÃºÃòÕúaÃºÃ©ÕñÕáÃ†ÕàÃ£ÕùcÃôÕçkÃñÃ±ÃπÕçÕòiÃ¢nÃ®Ã∫ÃùÕáÕáÃüÕôƒ£Ã´ÃÆÕéÃªÃüÕÖ ÃïnÃºÃ∫ÕàÕûuÃÆÕômÃ∫Ã≠ÃüÃóÕûeÃûÕìÃ∞Ã§ÕìÃ´rÃµoÃñ·π∑s“âÃ™ÕçÃ≠Ã¨ÃùÃ§ ÃÆÕâÃùÃûÃóÃüÕ†dÃ¥ÃüÃúÃ±ÕïÕöiÕáÃ´ÃºÃØÃ≠ÃúÕ°·∏ÅÕôÃªÃºcÃ≤Ã≤ÃπrÃ®Ã†ÃπÃ£Ã∞Ã¶iÃ±tÃ§ÃªÃ§ÕçÕôÃòÃïiÃµÃúÃ≠Ã§Ã±ÕécÃµs ÕòoÃ±Ã≤ÕàÃôÕñÕáÃ≤Õ¢nÕò ÃúÕàeÃ¨Ã≤Ã†Ã©acÕïÃ∫Ã†ÕâhÃ∑Ã™ Ã∫Ã£ÕñÃ±·∏ªÃ´Ã¨ÃùÃπ·∏ôÃôÃ∫ÕôÃ≠ÕìÃ≤tÃûÃûÕáÃ≤ÕâÕçtÃ∑ÕîÃ™ÕâÃ≤ÃªÃ†ÕôeÃ¶ÃªÕàÕâÕárÕáÃ≠Ã≠Ã¨Õñ,ÃñÃÅ ÃúÕôÕìÃ£Ã≠sÃòÃòÕàoÃ±Ã∞Ã§Ã≤ÕÖ ÃõÃ¨ÃúÃôtÃºÃ¶ÕïÃ±ÃπÕïÃ•hÃ≥Ã≤ÕàÕùÕÖaÃ¶tÃªÃ≤ ÃªÃüÃ≠Ã¶ÃñtÃõÃ∞Ã©hÃ†ÕïÃ≥ÃùÃ´ÕïeÕàÃ§ÃòÕñÃûÕòy“âÃùÕô Ã∑ÕâÕîÃ∞Ã†oÃûÃ∞vÕàÕàÃ≥ÃòÕúerÃ∂fÃ∞ÕàÕî·∏ªÕïÃòÃ´Ã∫Ã≤oÃ≤Ã≠ÕôÕ†ÕÖwÃ±Ã≥Ã∫ ÕútÃ∏hÕáÃ≠ÕïÃ≥ÕçeÃñÃØÃüÃ† ÕçÃûÃúÕîÃ©Ã™ÕúƒºÕéÃ™Ã≤ÕöiÃùÃ≤ÃπÃôÃ©ÃπnÃ®Ã¶Ã©Ãñ·∏ôÃºÃ≤ÃºÕ¢ÕÖ Ã¨ÕùsÃºÕöÃòÃûÕùpÕôÃòÃªaÃôc“âÕâÃúÃ§ÕàÃØÃñiÃ•Õ°nÃ¶Ã†Ã±ÕügÃ∏ÃóÃªÃ¶Ã≠ÃÆÃüÕÖ Ã≥Ã™Ã†ÕñÃ≥ÃØÃïaÃ´ÕúnÕùdÕ° Ã£Ã¶ÃôÕÖcÃ™ÃórÃ¥ÕôÃÆÃ¶ÃπÃ≥eÕáÕöÃûÕîÃπÃ´ÕüaÃôÃ∫Ãô»õÕîÕéÃòÃπÕÖeÃ•Ã©Õç aÕñÃ™ÃúÃÆÕôÃπnÃ¢ÕâÃù ÕáÕâÕìÃ¶ÃºÃÅaÃ≥ÕñÃ™Ã§Ã±pÃñÕîÕîÃüÕáÕéÕ†pÃ±ÕçÃ∫ƒôÃ≤ÕéÕàÃ∞Ã≤Ã§Ã´aÃØÕúrÃ®ÃÆÃ´Ã£ÃòaÃ©ÃØÕñnÃπÃ¶Ã∞ÕéÃ£ÃûÃûcÃ®Ã¶Ã±ÕîÕéÕçÕñeÃ¨ÕìÕò Ã§Ã∞Ã©ÕôÃ§Ã¨ÕôoÃµÃºÃªÃ¨ÃªÕáÃÆÃ™fÃ¥ Ã°ÃôÃ≠ÕìÕñÃ™Ã§‚ÄúÃ∏ÕôÃ†ÃºcÃ≥ÃóÕúoÕèÃºÕôÕîÃÆrÃûÃ´Ã∫ÃûÃ•Ã¨ruÃ∫ÃªÃØÕâÃ≠ÃªÃØpÃ∞Ã•ÕìÃ£Ã´ÃôÃ§Õ¢tÃ≥ÕçÃ≥ÃñÕÖiÃ∂ÕàÃùÕôÃºÃôÃπoÃ°ÕînÃôÃ∫ÃπÃñÃ©ÕùÕÖ‚ÄùÃ®ÃóÕñÕöÃ©.ÃØÕì  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children‚Äôs books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\t◊ê÷∂◊™ ◊ì÷∑◊ú÷∞◊™÷¥÷º◊ô ◊î÷µ◊ñ÷¥◊ô◊ñ ◊î÷µ◊†÷¥◊ô◊¢÷∑, ◊ß÷∂◊ò÷∂◊ë ◊ú÷¥◊©÷∞◊Å◊õ÷∑÷º◊™÷¥÷º◊ô ◊ô÷∏◊©◊Å◊ï÷π◊ì Normal writing (no niqqud):\\t◊ê◊™ ◊ì◊ú◊™◊ô ◊î◊ñ◊ô◊ñ ◊î◊†◊ô◊¢, ◊ß◊ò◊ë ◊ú◊©◊õ◊™◊ô ◊ô◊©◊ï◊ì Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, ‚Äú‡§π‚Äù + ‚Äú\\u200b‡§ø‚Äù = ‚Äú‡§π‡§ø‚Äù (‚Äúh‚Äù + ‚Äúi‚Äù = ‚Äúhi‚Äù). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it‚Äôs also possible to dynamically compose them by concatenating their jamo. For example, ‚Äú·Ñí‚Äù + ‚Äú·Ö°‚Äù + ‚Äú·Ü´‚Äù = ‚ÄúÌïú‚Äù (‚Äúh‚Äù + ‚Äúa‚Äù + ‚Äún‚Äù = ‚Äúhan‚Äù). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express ‚Äúthe same‚Äù string‚Äîdifferent sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character ‚Äú√Å‚Äù either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: ‚Äú«°‚Äù (dot, then macron) is different from ‚ÄúƒÅÃá‚Äù (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn‚Äôt affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter ‚Äú·ªá‚Äù can be expressed in five different ways:  Fully precomposed: U+1EC7 ‚Äú·ªá‚Äù Partially precomposed: U+1EB9 ‚Äú·∫π‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù Partially precomposed: U+00EA ‚Äú√™‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù Fully decomposed: U+0065 ‚Äúe‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù Fully decomposed: U+0065 ‚Äúe‚Äù + U+0302 ‚Äú‚óåÃÇ‚Äù + U+0323 ‚Äú‚óåÃ£‚Äù Unicode refers to set of strings like this as ‚Äúcanonically equivalent‚Äù. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a ‚Äúfind in file‚Äù operation and the user searches for ‚Äú·ªá‚Äù, it should, by default, find occurrences of any of the five versions of ‚Äú·ªá‚Äù above!  Normalization Forms To address the problem of ‚Äúhow to handle canonically equivalent strings‚Äù, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The ‚ÄúNFD‚Äù normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn‚Äôt reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The ‚ÄúNFC‚Äù form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The ‚ÄúK‚Äù here refers to compatibility decompositions, which cover characters that are ‚Äúsimilar‚Äù in some sense but not visually identical. However, I‚Äôm not going to cover that here.  Grapheme Clusters As we‚Äôve seen, Unicode contains various cases where a thing that a user thinks of as a single ‚Äúcharacter‚Äù might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single ‚Äúuser-perceived character‚Äù.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It‚Äôs approximately ‚Äúa base code point followed by any number of combining marks‚Äù, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they‚Äôre often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can‚Äôt accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit‚Äîsay, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn‚Äôt want to enforce that by just truncating bytes. At a minimum, you‚Äôd want to ‚Äúround down‚Äù to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More‚Ä¶ There‚Äôs much more that could be said about Unicode from a programmer‚Äôs perspective! I haven‚Äôt gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues‚Äîhow to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I‚Äôll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and ‚Äúcharacters‚Äù. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it‚Äôs clear that we‚Äôre never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)‚ÄîC/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts‚Äîset of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BPE Training** üîÑ \n",
    "I know trainingmaybe a scary word but it simple as the following: \n",
    "1. **Goal**: Build a vocabulary of common byte pairs (like \"th\", \"ing\")  \n",
    "2. **How**:  \n",
    "   - Run a loop `N` times (e.g., `N=20` merges)  \n",
    "   - Each iteration:  \n",
    "     1. **Find** the most frequent adjacent byte pair  \n",
    "     2. **Replace** all instances of that pair with a new token ID  \n",
    "     3. **Save** the rule: `{(old1, old2): new_id}`  \n",
    "3. **Result**: A \"vocabulary\" of merged tokens for efficient text encoding.  \n",
    "\n",
    "No gradients, no neural networks‚Äîjust counting and merging!  \n",
    "*(Like combining LEGO pieces you often use together.)* üß±‚Üíüè∞  \n",
    "\n",
    "--- \n",
    "\n",
    "This is the essence‚Äîeverything else is implementation details! üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) into 256 (frequency: 646)\n",
      "Merging (105, 110) into 257 (frequency: 446)\n",
      "Merging (115, 32) into 258 (frequency: 424)\n",
      "Merging (116, 104) into 259 (frequency: 337)\n",
      "Merging (101, 114) into 260 (frequency: 294)\n",
      "Merging (99, 111) into 261 (frequency: 290)\n",
      "Merging (116, 32) into 262 (frequency: 285)\n",
      "Merging (226, 128) into 263 (frequency: 254)\n",
      "Merging (44, 32) into 264 (frequency: 243)\n",
      "Merging (97, 110) into 265 (frequency: 229)\n",
      "Merging (111, 114) into 266 (frequency: 214)\n",
      "Merging (100, 32) into 267 (frequency: 213)\n",
      "Merging (97, 114) into 268 (frequency: 181)\n",
      "Merging (101, 110) into 269 (frequency: 174)\n",
      "Merging (257, 103) into 270 (frequency: 166)\n",
      "Merging (261, 100) into 271 (frequency: 165)\n",
      "Merging (121, 32) into 272 (frequency: 154)\n",
      "Merging (46, 32) into 273 (frequency: 154)\n",
      "Merging (97, 108) into 274 (frequency: 146)\n",
      "Merging (259, 256) into 275 (frequency: 144)\n"
     ]
    }
   ],
   "source": [
    "# Byte-Pair Encoding (BPE) Training Process\n",
    "# ------------------------------------------\n",
    "# This implements the core BPE algorithm to learn merge operations\n",
    "# 1. We start with base vocabulary (256 possible byte values)\n",
    "# 2. Iteratively find/replace most frequent adjacent token pairs\n",
    "# 3. Grow vocabulary until reaching target size\n",
    "\n",
    "vocab_size = 276  # Hyperparameter: Final vocabulary size (256 base + 20 merges)\n",
    "num_merges = vocab_size - 256  # Number of merge operations needed (276-256=20)\n",
    "\n",
    "# Initialize with raw byte tokens from input text\n",
    "# Example: \"hello\" ‚Üí [104, 101, 108, 108, 111]\n",
    "ids = list(tokens)  # tokens should be byte values from .encode('utf-8')\n",
    "\n",
    "# Dictionary to store learned merge rules: (old_id1, old_id2) ‚Üí new_id\n",
    "merges = {}\n",
    "\n",
    "# Core BPE Training Loop\n",
    "for i in range(num_merges):\n",
    "    # 1. Find most frequent adjacent pair\n",
    "    counts = most_common_pairs(ids)  # Returns frequency count of all pairs\n",
    "    pair = max(counts, key=counts.get)  # Select pair with highest frequency\n",
    "    \n",
    "    # 2. Create new token ID (starting after byte values 0-255)\n",
    "    idx = 256 + i  # New token ID (256, 257, ..., 275)\n",
    "    \n",
    "    # 3. Replace all pair occurrences with new ID\n",
    "    print(f\"Merging {pair} into {idx} (frequency: {counts[pair]})\")\n",
    "    ids = merge(ids, pair, idx)  # Apply merge to entire corpus\n",
    "    \n",
    "    # 4. Record merge operation for encoding/decoding\n",
    "    merges[pair] = idx\n",
    "\n",
    "# Final merges dictionary contains all learned merge rules\n",
    "# Example: {(101, 115): 256, (256, 116): 257, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 24597\n",
      "ids length: 19438\n",
      "compression ratio: 1.27X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Tokenizer Essentials: Encoding ‚Üî Decoding üîÑ\n",
    "\n",
    "## Core Requirement\n",
    "Every modern tokenizer **must** implement two fundamental operations:\n",
    "```python\n",
    "def encode(text: str) -> list[int]: ...  # Text ‚Üí Compact Token IDs\n",
    "def decode(ids: list[int]) -> str: ...   # Token IDs ‚Üí Human-Readable Text\n",
    "```\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/ENCODE_DECODE.jpg\" alt=\"Tokenizer Encoding/Decoding Workflow\" width=\"600\"/>\n",
    "  <br>\n",
    "  <em>Tokenizer Lifecycle: Text ‚Üî Token IDs conversion requires shared vocabulary</em>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "## Foundation First: Building the Vocabulary üß±\n",
    "\n",
    "### Why Vocabulary Matters\n",
    "- **Encode/Decode Bridge**: Shared reference for converting between formats\n",
    "- **Byte Foundation**: Starts with 256 raw byte tokens (0-255)\n",
    "- **Merge Rules**: Grows through learned BPE pair combinations\n",
    "\n",
    "### Vocabulary Construction\n",
    "```python\n",
    "# 1. Base byte vocabulary\n",
    "vocab = {\n",
    "    0: b'\\x00', \n",
    "    1: b'\\x01',\n",
    "    ...,\n",
    "    255: b'\\xff'\n",
    "}\n",
    "\n",
    "# 2. Add merged tokens from training\n",
    "# Format: {(parent1, parent2): new_id}\n",
    "merges = {(101, 115): 256, (256, 116): 257}  # Example: \"es\"‚Üí256, \"est\"‚Üí257\n",
    "\n",
    "for (p1, p2), new_id in merges.items():\n",
    "    vocab[new_id] = vocab[p1] + vocab[p2]  # Combine parent bytes\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "| Aspect          | Description                                  |\n",
    "|-----------------|----------------------------------------------|\n",
    "| **Reversibility** | Every ID maps to exact byte sequence        |\n",
    "| **Extensibility** | New merges don't alter existing mappings    |\n",
    "| **Multi-Lingual** | Byte-level foundation handles all UTF-8 text |\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Order Matters ‚öôÔ∏è\n",
    "1. **Vocabulary First** ‚Üí No encoding/decoding without merge rules\n",
    "2. **Training** ‚Üí Discovers frequent patterns to grow vocabulary\n",
    "3. **Encoding** ‚Üí Relies on vocabulary to compress text\n",
    "4. **Decoding** ‚Üí Uses vocabulary to reconstruct text\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Vocabulary] --> B[Encoder]\n",
    "    A --> C[Decoder]\n",
    "    B --> D[\"Token IDs\"]\n",
    "    C --> E[\"Original Text\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Checklist ‚úÖ\n",
    "- [ ] Base byte vocabulary (0-255)\n",
    "- [ ] Merge rules from BPE training\n",
    "- [ ] Encode: Text ‚Üí IDs using merge rules\n",
    "- [ ] Decode: IDs ‚Üí Text with error handling\n",
    "- [ ] Validation: `decode(encode(text)) == text`\n",
    "\n",
    "Without a properly constructed vocabulary, tokenizers become glorified random mappers! üîÑ‚û°Ô∏è‚ùå\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base vocabulary with individual bytes (0-255)\n",
    "# Each ID maps to its corresponding single byte\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "# Example: vocab[100] = b'd', vocab[101] = b'e', etc.\n",
    "\n",
    "# Populate vocabulary with merged tokens from BPE merges\n",
    "# merges dictionary format: {(p1, p2): new_idx}\n",
    "# Where p1 and p2 are existing token IDs, new_idx is their merged representation\n",
    "for (p1, p2), idx in merges.items():\n",
    "    # Combine bytes from parent tokens to create merged token\n",
    "    vocab[idx] = vocab[p1] + vocab[p2]\n",
    "    # Example: If merge (100, 101) ‚Üí 256, then vocab[256] = b'de'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function as simple as this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "def decode(ids):\n",
    "    \"\"\"Convert list of token IDs back to human-readable text\n",
    "    \n",
    "    Args:\n",
    "        ids: List of integers representing token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Decoded UTF-8 string\n",
    "    \n",
    "    Process:\n",
    "        1. Convert each ID to its byte representation using vocab\n",
    "        2. Concatenate all bytes\n",
    "        3. Decode byte sequence to UTF-8 text\n",
    "    \"\"\"\n",
    "    # Convert token IDs to their corresponding byte sequences\n",
    "    byte_sequence = b\"\".join(vocab[idx] for idx in ids)\n",
    "    \n",
    "    # Convert bytes to UTF-8 string \n",
    "    text = byte_sequence.decode(\"utf-8\")\n",
    "    return text\n",
    "\n",
    "# Test decoding: Convert token ID 100 to text\n",
    "# Since vocab[100] = b'd', this should print 'd'\n",
    "print(decode([100]))  # Output: 'd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost correct , there is a small edge case we need to take care of.\n",
    "\n",
    "look at this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(ids)\u001b[0m\n\u001b[0;32m     16\u001b[0m byte_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convert bytes to UTF-8 string \u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mbyte_sequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what is going on here\n",
    "The byte sequence b'\\x80' (from vocab[128] = bytes([128])) is not a valid UTF-8 character.\n",
    "UTF-8 requires multi-byte sequences for values ‚â• 128, but b'\\x80' is an invalid standalone byte.\n",
    "\n",
    "but the soultion is rather simple too: we use the error handle in the fucntion\n",
    "Options for errors:\n",
    "\n",
    "replace: Replace invalid bytes with ÔøΩ (U+FFFD)\n",
    "\n",
    "ignore: Skip invalid bytes entirely\n",
    "\n",
    "backslashreplace: Replace with hex escape (e.g., \\x80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÔøΩ\n"
     ]
    }
   ],
   "source": [
    "def decode(ids):\n",
    "    \"\"\"Convert list of token IDs back to human-readable text\n",
    "    \n",
    "    Args:\n",
    "        ids: List of integers representing token IDs\n",
    "    \n",
    "    Returns:\n",
    "        Decoded UTF-8 string\n",
    "    \n",
    "    Process:\n",
    "        1. Convert each ID to its byte representation using vocab\n",
    "        2. Concatenate all bytes\n",
    "        3. Decode byte sequence to UTF-8 text\n",
    "    \"\"\"\n",
    "    # Convert token IDs to their corresponding byte sequences\n",
    "    byte_sequence = b\"\".join(vocab[idx] for idx in ids)\n",
    "    \n",
    "    # Convert bytes to UTF-8 string \n",
    "    text = byte_sequence.decode(\"utf-8\", errors='replace') # replace unknown characters with ÔøΩ - this is the only change\n",
    "    return text\n",
    "\n",
    "# Test decoding: Convert token ID 100 to text\n",
    "# Since vocab[100] = b'd', this should print 'd'\n",
    "print(decode([128]))  # Output: 'd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets take a look at the merges, we will notice that the dict is sorted in descending order.\n",
    "this is very important to us when we will code the encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (105, 110): 257,\n",
       " (115, 32): 258,\n",
       " (116, 104): 259,\n",
       " (101, 114): 260,\n",
       " (99, 111): 261,\n",
       " (116, 32): 262,\n",
       " (226, 128): 263,\n",
       " (44, 32): 264,\n",
       " (97, 110): 265,\n",
       " (111, 114): 266,\n",
       " (100, 32): 267,\n",
       " (97, 114): 268,\n",
       " (101, 110): 269,\n",
       " (257, 103): 270,\n",
       " (261, 100): 271,\n",
       " (121, 32): 272,\n",
       " (46, 32): 273,\n",
       " (97, 108): 274,\n",
       " (259, 256): 275}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(65, 32): 8,\n",
       " (32, 80): 7,\n",
       " (80, 114): 1,\n",
       " (114, 111): 57,\n",
       " (111, 103): 23,\n",
       " (103, 114): 25,\n",
       " (114, 97): 40,\n",
       " (97, 109): 47,\n",
       " (109, 109): 20,\n",
       " (109, 260): 10,\n",
       " (260, 263): 6,\n",
       " (263, 153): 64,\n",
       " (153, 258): 29,\n",
       " (258, 73): 3,\n",
       " (73, 110): 15,\n",
       " (110, 116): 18,\n",
       " (116, 114): 50,\n",
       " (111, 100): 18,\n",
       " (100, 117): 8,\n",
       " (117, 99): 19,\n",
       " (99, 116): 62,\n",
       " (116, 105): 134,\n",
       " (105, 111): 76,\n",
       " (111, 110): 143,\n",
       " (110, 32): 78,\n",
       " (32, 116): 66,\n",
       " (116, 111): 110,\n",
       " (111, 32): 137,\n",
       " (32, 85): 66,\n",
       " (85, 110): 60,\n",
       " (110, 105): 71,\n",
       " (105, 271): 59,\n",
       " (271, 256): 111,\n",
       " (256, 77): 3,\n",
       " (77, 268): 3,\n",
       " (268, 99): 4,\n",
       " (99, 104): 89,\n",
       " (104, 32): 45,\n",
       " (32, 51): 5,\n",
       " (51, 264): 2,\n",
       " (264, 50): 1,\n",
       " (50, 48): 1,\n",
       " (48, 49): 7,\n",
       " (49, 55): 2,\n",
       " (55, 32): 5,\n",
       " (32, 194): 2,\n",
       " (194, 183): 2,\n",
       " (183, 32): 2,\n",
       " (32, 67): 8,\n",
       " (67, 111): 17,\n",
       " (100, 270): 7,\n",
       " (270, 32): 107,\n",
       " (32, 50): 3,\n",
       " (50, 50): 1,\n",
       " (50, 32): 14,\n",
       " (111, 109): 43,\n",
       " (109, 269): 10,\n",
       " (269, 116): 36,\n",
       " (116, 258): 68,\n",
       " (258, 32): 1,\n",
       " (32, 239): 1,\n",
       " (239, 188): 1,\n",
       " (188, 181): 1,\n",
       " (181, 239): 1,\n",
       " (239, 189): 6,\n",
       " (189, 142): 1,\n",
       " (142, 239): 1,\n",
       " (189, 137): 1,\n",
       " (137, 239): 1,\n",
       " (189, 131): 1,\n",
       " (131, 239): 1,\n",
       " (189, 143): 1,\n",
       " (143, 239): 1,\n",
       " (189, 132): 1,\n",
       " (132, 239): 1,\n",
       " (189, 133): 1,\n",
       " (133, 33): 1,\n",
       " (33, 32): 10,\n",
       " (32, 240): 4,\n",
       " (240, 159): 16,\n",
       " (159, 133): 7,\n",
       " (133, 164): 1,\n",
       " (164, 240): 1,\n",
       " (133, 157): 1,\n",
       " (157, 240): 1,\n",
       " (133, 152): 1,\n",
       " (152, 240): 1,\n",
       " (133, 146): 1,\n",
       " (146, 240): 1,\n",
       " (133, 158): 1,\n",
       " (158, 240): 1,\n",
       " (133, 147): 1,\n",
       " (147, 240): 1,\n",
       " (133, 148): 1,\n",
       " (148, 263): 1,\n",
       " (263, 189): 1,\n",
       " (189, 32): 1,\n",
       " (159, 135): 7,\n",
       " (135, 186): 1,\n",
       " (186, 263): 1,\n",
       " (263, 140): 6,\n",
       " (140, 240): 6,\n",
       " (135, 179): 1,\n",
       " (179, 263): 1,\n",
       " (135, 174): 1,\n",
       " (174, 263): 1,\n",
       " (135, 168): 1,\n",
       " (168, 263): 1,\n",
       " (135, 180): 1,\n",
       " (180, 263): 1,\n",
       " (135, 169): 1,\n",
       " (169, 263): 1,\n",
       " (135, 170): 1,\n",
       " (170, 33): 1,\n",
       " (159, 152): 2,\n",
       " (152, 132): 1,\n",
       " (132, 32): 1,\n",
       " (32, 84): 17,\n",
       " (84, 104): 40,\n",
       " (104, 256): 21,\n",
       " (256, 118): 7,\n",
       " (118, 260): 47,\n",
       " (260, 272): 5,\n",
       " (272, 110): 6,\n",
       " (110, 97): 13,\n",
       " (109, 256): 29,\n",
       " (256, 115): 45,\n",
       " (115, 116): 104,\n",
       " (114, 105): 58,\n",
       " (105, 107): 12,\n",
       " (107, 101): 7,\n",
       " (101, 258): 74,\n",
       " (258, 102): 18,\n",
       " (102, 101): 10,\n",
       " (101, 268): 14,\n",
       " (268, 32): 5,\n",
       " (32, 265): 33,\n",
       " (265, 267): 70,\n",
       " (267, 97): 18,\n",
       " (97, 119): 3,\n",
       " (119, 256): 6,\n",
       " (256, 257): 8,\n",
       " (257, 116): 76,\n",
       " (32, 259): 121,\n",
       " (259, 256): 144,\n",
       " (256, 104): 5,\n",
       " (104, 101): 25,\n",
       " (268, 116): 7,\n",
       " (258, 111): 35,\n",
       " (111, 102): 116,\n",
       " (102, 32): 116,\n",
       " (32, 112): 35,\n",
       " (112, 114): 61,\n",
       " (260, 258): 25,\n",
       " (258, 119): 18,\n",
       " (119, 266): 21,\n",
       " (266, 108): 5,\n",
       " (108, 100): 9,\n",
       " (100, 119): 2,\n",
       " (119, 105): 40,\n",
       " (105, 100): 27,\n",
       " (100, 101): 46,\n",
       " (101, 273): 23,\n",
       " (273, 87): 6,\n",
       " (87, 256): 1,\n",
       " (256, 274): 7,\n",
       " (274, 108): 63,\n",
       " (108, 32): 51,\n",
       " (32, 107): 6,\n",
       " (107, 110): 5,\n",
       " (110, 111): 30,\n",
       " (111, 119): 57,\n",
       " (119, 32): 29,\n",
       " (32, 119): 50,\n",
       " (256, 111): 24,\n",
       " (111, 117): 81,\n",
       " (117, 103): 9,\n",
       " (103, 104): 22,\n",
       " (104, 262): 10,\n",
       " (262, 116): 12,\n",
       " (32, 263): 44,\n",
       " (263, 156): 71,\n",
       " (156, 115): 4,\n",
       " (115, 117): 40,\n",
       " (117, 112): 25,\n",
       " (112, 112): 26,\n",
       " (112, 266): 13,\n",
       " (266, 262): 6,\n",
       " (262, 85): 8,\n",
       " (271, 101): 32,\n",
       " (101, 263): 25,\n",
       " (263, 157): 71,\n",
       " (157, 32): 53,\n",
       " (32, 257): 43,\n",
       " (257, 32): 93,\n",
       " (32, 111): 80,\n",
       " (117, 114): 34,\n",
       " (114, 32): 13,\n",
       " (32, 115): 105,\n",
       " (115, 111): 41,\n",
       " (102, 116): 6,\n",
       " (116, 119): 7,\n",
       " (119, 268): 5,\n",
       " (268, 256): 38,\n",
       " (256, 40): 6,\n",
       " (40, 119): 1,\n",
       " (119, 104): 37,\n",
       " (104, 97): 24,\n",
       " (97, 116): 107,\n",
       " (116, 101): 88,\n",
       " (101, 118): 28,\n",
       " (260, 32): 93,\n",
       " (259, 97): 40,\n",
       " (97, 262): 47,\n",
       " (262, 109): 16,\n",
       " (109, 101): 20,\n",
       " (101, 265): 11,\n",
       " (265, 115): 3,\n",
       " (115, 263): 14,\n",
       " (263, 148): 25,\n",
       " (148, 108): 1,\n",
       " (108, 105): 43,\n",
       " (107, 256): 13,\n",
       " (256, 117): 14,\n",
       " (117, 115): 59,\n",
       " (115, 270): 19,\n",
       " (119, 99): 1,\n",
       " (104, 268): 43,\n",
       " (268, 95): 1,\n",
       " (95, 262): 1,\n",
       " (262, 102): 10,\n",
       " (102, 266): 58,\n",
       " (266, 32): 82,\n",
       " (32, 274): 7,\n",
       " (114, 270): 27,\n",
       " (270, 115): 10,\n",
       " (115, 264): 71,\n",
       " (264, 114): 6,\n",
       " (105, 103): 30,\n",
       " (104, 116): 4,\n",
       " (116, 63): 1,\n",
       " (63, 41): 1,\n",
       " (41, 273): 11,\n",
       " (273, 66): 8,\n",
       " (66, 117): 4,\n",
       " (117, 262): 32,\n",
       " (256, 99): 26,\n",
       " (99, 265): 20,\n",
       " (265, 32): 42,\n",
       " (32, 98): 57,\n",
       " (98, 256): 29,\n",
       " (256, 97): 30,\n",
       " (97, 98): 40,\n",
       " (98, 115): 4,\n",
       " (114, 117): 11,\n",
       " (115, 101): 99,\n",
       " (101, 264): 36,\n",
       " (264, 265): 26,\n",
       " (267, 100): 8,\n",
       " (100, 105): 59,\n",
       " (105, 118): 27,\n",
       " (118, 270): 1,\n",
       " (256, 259): 39,\n",
       " (259, 111): 13,\n",
       " (115, 265): 1,\n",
       " (265, 100): 14,\n",
       " (100, 45): 3,\n",
       " (45, 112): 8,\n",
       " (112, 97): 31,\n",
       " (97, 103): 31,\n",
       " (103, 256): 18,\n",
       " (256, 85): 18,\n",
       " (256, 83): 2,\n",
       " (83, 116): 3,\n",
       " (116, 265): 9,\n",
       " (100, 268): 7,\n",
       " (268, 267): 6,\n",
       " (267, 112): 7,\n",
       " (112, 108): 70,\n",
       " (108, 117): 19,\n",
       " (117, 258): 7,\n",
       " (258, 105): 13,\n",
       " (105, 116): 111,\n",
       " (258, 100): 6,\n",
       " (100, 111): 24,\n",
       " (111, 122): 1,\n",
       " (122, 269): 1,\n",
       " (269, 258): 1,\n",
       " (108, 101): 89,\n",
       " (101, 109): 42,\n",
       " (116, 268): 4,\n",
       " (268, 272): 7,\n",
       " (272, 265): 4,\n",
       " (265, 110): 3,\n",
       " (110, 101): 27,\n",
       " (101, 120): 75,\n",
       " (120, 101): 6,\n",
       " (101, 115): 117,\n",
       " (114, 101): 101,\n",
       " (101, 112): 19,\n",
       " (266, 116): 11,\n",
       " (116, 115): 34,\n",
       " (267, 110): 4,\n",
       " (111, 116): 11,\n",
       " (258, 99): 17,\n",
       " (256, 109): 17,\n",
       " (109, 266): 23,\n",
       " (266, 256): 19,\n",
       " (259, 265): 3,\n",
       " (32, 97): 86,\n",
       " (97, 32): 93,\n",
       " (32, 108): 46,\n",
       " (116, 116): 35,\n",
       " (116, 108): 7,\n",
       " (108, 256): 48,\n",
       " (105, 109): 26,\n",
       " (109, 105): 24,\n",
       " (100, 97): 9,\n",
       " (116, 270): 27,\n",
       " (270, 273): 5,\n",
       " (273, 73): 18,\n",
       " (73, 32): 13,\n",
       " (32, 100): 43,\n",
       " (110, 263): 17,\n",
       " (153, 262): 17,\n",
       " (262, 98): 14,\n",
       " (98, 108): 26,\n",
       " (108, 97): 31,\n",
       " (256, 112): 81,\n",
       " (105, 108): 46,\n",
       " (108, 108): 44,\n",
       " (32, 102): 38,\n",
       " (102, 257): 9,\n",
       " (257, 100): 11,\n",
       " (256, 119): 16,\n",
       " (104, 111): 20,\n",
       " (111, 108): 7,\n",
       " (259, 270): 13,\n",
       " (32, 109): 53,\n",
       " (109, 121): 1,\n",
       " (121, 115): 11,\n",
       " (116, 260): 99,\n",
       " (260, 105): 4,\n",
       " (264, 101): 10,\n",
       " (118, 269): 8,\n",
       " (269, 32): 28,\n",
       " (51, 48): 6,\n",
       " (48, 32): 7,\n",
       " (32, 121): 11,\n",
       " (121, 101): 3,\n",
       " (268, 258): 1,\n",
       " (258, 97): 46,\n",
       " (97, 102): 2,\n",
       " (258, 257): 34,\n",
       " (257, 99): 10,\n",
       " (99, 101): 37,\n",
       " (112, 116): 19,\n",
       " (110, 273): 8,\n",
       " (273, 32): 44,\n",
       " (32, 65): 15,\n",
       " (101, 119): 11,\n",
       " (109, 111): 24,\n",
       " (110, 259): 1,\n",
       " (259, 258): 1,\n",
       " (103, 111): 12,\n",
       " (111, 264): 3,\n",
       " (264, 73): 3,\n",
       " (32, 103): 14,\n",
       " (111, 262): 14,\n",
       " (262, 257): 14,\n",
       " (260, 101): 15,\n",
       " (101, 267): 105,\n",
       " (267, 257): 14,\n",
       " (256, 265): 7,\n",
       " (101, 99): 22,\n",
       " (99, 105): 14,\n",
       " (267, 116): 16,\n",
       " (115, 112): 31,\n",
       " (112, 269): 2,\n",
       " (269, 267): 2,\n",
       " (267, 115): 12,\n",
       " (256, 116): 19,\n",
       " (256, 108): 20,\n",
       " (268, 110): 1,\n",
       " (110, 270): 2,\n",
       " (98, 111): 28,\n",
       " (262, 105): 14,\n",
       " (105, 262): 35,\n",
       " (101, 116): 37,\n",
       " (116, 97): 35,\n",
       " (97, 105): 17,\n",
       " (108, 273): 2,\n",
       " (259, 105): 16,\n",
       " (105, 258): 63,\n",
       " (258, 268): 16,\n",
       " (105, 99): 87,\n",
       " (99, 108): 22,\n",
       " (73, 263): 5,\n",
       " (153, 108): 6,\n",
       " (103, 105): 2,\n",
       " (118, 256): 16,\n",
       " (32, 105): 46,\n",
       " (102, 114): 14,\n",
       " (109, 32): 30,\n",
       " (258, 112): 9,\n",
       " (112, 111): 110,\n",
       " (111, 257): 70,\n",
       " (257, 262): 17,\n",
       " (262, 111): 17,\n",
       " (32, 118): 13,\n",
       " (118, 105): 12,\n",
       " (105, 101): 20,\n",
       " (119, 273): 3,\n",
       " (32, 73): 6,\n",
       " (153, 109): 4,\n",
       " (111, 270): 5,\n",
       " (102, 111): 5,\n",
       " (111, 99): 10,\n",
       " (99, 117): 16,\n",
       " (268, 97): 42,\n",
       " (97, 99): 131,\n",
       " (101, 262): 9,\n",
       " (262, 265): 6,\n",
       " (267, 119): 4,\n",
       " (116, 263): 24,\n",
       " (257, 118): 2,\n",
       " (118, 111): 6,\n",
       " (108, 118): 2,\n",
       " (118, 101): 20,\n",
       " (266, 107): 8,\n",
       " (107, 270): 5,\n",
       " (105, 259): 30,\n",
       " (259, 32): 31,\n",
       " (270, 258): 17,\n",
       " (258, 265): 10,\n",
       " (267, 102): 11,\n",
       " (102, 105): 28,\n",
       " (120, 116): 16,\n",
       " (116, 273): 13,\n",
       " (273, 72): 5,\n",
       " (72, 111): 8,\n",
       " (119, 101): 20,\n",
       " (260, 264): 18,\n",
       " (264, 257): 7,\n",
       " (256, 73): 4,\n",
       " (32, 110): 19,\n",
       " (262, 103): 6,\n",
       " (116, 274): 6,\n",
       " (274, 107): 2,\n",
       " (107, 32): 20,\n",
       " (264, 116): 7,\n",
       " (120, 262): 13,\n",
       " (262, 108): 6,\n",
       " (97, 121): 11,\n",
       " (121, 111): 25,\n",
       " (117, 116): 20,\n",
       " (116, 47): 2,\n",
       " (47, 115): 1,\n",
       " (115, 104): 9,\n",
       " (97, 112): 38,\n",
       " (112, 270): 4,\n",
       " (270, 47): 1,\n",
       " (47, 114): 2,\n",
       " (114, 269): 12,\n",
       " (269, 100): 18,\n",
       " (100, 260): 25,\n",
       " (260, 270): 5,\n",
       " (270, 264): 13,\n",
       " (264, 266): 11,\n",
       " (108, 111): 30,\n",
       " (99, 274): 30,\n",
       " (274, 105): 12,\n",
       " (105, 122): 12,\n",
       " (122, 97): 6,\n",
       " (108, 263): 1,\n",
       " (148, 259): 2,\n",
       " (111, 115): 53,\n",
       " (115, 256): 27,\n",
       " (256, 268): 14,\n",
       " (112, 268): 7,\n",
       " (116, 256): 23,\n",
       " (256, 105): 11,\n",
       " (105, 115): 33,\n",
       " (115, 115): 34,\n",
       " (117, 101): 6,\n",
       " (264, 98): 14,\n",
       " (98, 101): 15,\n",
       " (101, 121): 5,\n",
       " (110, 267): 10,\n",
       " (267, 109): 6,\n",
       " (109, 272): 1,\n",
       " (272, 115): 13,\n",
       " (115, 261): 2,\n",
       " (261, 112): 4,\n",
       " (112, 256): 1,\n",
       " (40, 265): 3,\n",
       " (267, 107): 2,\n",
       " (119, 108): 2,\n",
       " (101, 100): 23,\n",
       " (100, 103): 2,\n",
       " (103, 101): 31,\n",
       " (101, 41): 5,\n",
       " (41, 32): 17,\n",
       " (32, 104): 24,\n",
       " (104, 260): 20,\n",
       " (32, 68): 5,\n",
       " (68, 105): 3,\n",
       " (260, 115): 24,\n",
       " (115, 105): 54,\n",
       " (116, 272): 15,\n",
       " (267, 73): 2,\n",
       " (110, 104): 2,\n",
       " (260, 269): 11,\n",
       " (269, 262): 19,\n",
       " (262, 67): 2,\n",
       " (109, 112): 70,\n",
       " (120, 105): 11,\n",
       " (272, 84): 1,\n",
       " (256, 67): 7,\n",
       " (99, 256): 26,\n",
       " (256, 65): 4,\n",
       " (65, 108): 4,\n",
       " (99, 97): 23,\n",
       " (32, 83): 4,\n",
       " (83, 99): 2,\n",
       " (99, 114): 40,\n",
       " (105, 112): 21,\n",
       " (258, 85): 11,\n",
       " (85, 115): 4,\n",
       " (115, 97): 18,\n",
       " (256, 70): 2,\n",
       " (70, 114): 3,\n",
       " (101, 113): 17,\n",
       " (113, 117): 28,\n",
       " (117, 269): 14,\n",
       " (269, 99): 18,\n",
       " (99, 272): 6,\n",
       " (272, 69): 2,\n",
       " (69, 110): 2,\n",
       " (110, 271): 2,\n",
       " (271, 270): 22,\n",
       " (85, 84): 40,\n",
       " (84, 70): 40,\n",
       " (70, 45): 39,\n",
       " (45, 56): 22,\n",
       " (56, 32): 21,\n",
       " (45, 49): 16,\n",
       " (49, 54): 25,\n",
       " (54, 32): 20,\n",
       " (109, 98): 29,\n",
       " (98, 257): 25,\n",
       " (257, 270): 17,\n",
       " (32, 77): 9,\n",
       " (268, 107): 14,\n",
       " (107, 258): 10,\n",
       " (258, 67): 1,\n",
       " (67, 265): 3,\n",
       " (265, 111): 6,\n",
       " (274, 32): 36,\n",
       " (32, 69): 6,\n",
       " (69, 113): 2,\n",
       " (117, 105): 8,\n",
       " (118, 274): 9,\n",
       " (274, 269): 5,\n",
       " (256, 78): 3,\n",
       " (78, 266): 4,\n",
       " (266, 109): 22,\n",
       " (109, 274): 17,\n",
       " (32, 70): 10,\n",
       " (70, 266): 11,\n",
       " (109, 258): 7,\n",
       " (258, 71): 1,\n",
       " (71, 114): 3,\n",
       " (112, 104): 11,\n",
       " (67, 108): 2,\n",
       " (258, 65): 2,\n",
       " (65, 110): 5,\n",
       " (267, 77): 2,\n",
       " (77, 266): 4,\n",
       " (266, 101): 16,\n",
       " (263, 166): 2,\n",
       " (166, 32): 2,\n",
       " (272, 65): 2,\n",
       " (65, 258): 3,\n",
       " (258, 115): 15,\n",
       " (111, 111): 13,\n",
       " (97, 258): 61,\n",
       " (258, 121): 4,\n",
       " (117, 32): 18,\n",
       " (268, 262): 4,\n",
       " (116, 117): 11,\n",
       " (117, 100): 9,\n",
       " (100, 272): 4,\n",
       " (272, 85): 2,\n",
       " (264, 105): 12,\n",
       " (101, 261): 19,\n",
       " (261, 109): 71,\n",
       " (262, 114): 10,\n",
       " (115, 269): 13,\n",
       " (108, 268): 7,\n",
       " (268, 103): 7,\n",
       " (256, 106): 2,\n",
       " (106, 117): 9,\n",
       " (117, 109): 16,\n",
       " (112, 32): 21,\n",
       " (32, 261): 40,\n",
       " (272, 111): 11,\n",
       " (111, 118): 15,\n",
       " (32, 99): 33,\n",
       " (258, 108): 7,\n",
       " (65, 83): 9,\n",
       " (83, 67): 9,\n",
       " (67, 73): 9,\n",
       " (73, 73): 9,\n",
       " (262, 121): 4,\n",
       " (109, 97): 39,\n",
       " (97, 272): 6,\n",
       " (272, 98): 3,\n",
       " (256, 102): 17,\n",
       " (102, 97): 6,\n",
       " (105, 268): 1,\n",
       " (259, 273): 1,\n",
       " (73, 116): 3,\n",
       " (258, 110): 6,\n",
       " (262, 106): 2,\n",
       " (115, 262): 27,\n",
       " (262, 259): 22,\n",
       " (256, 261): 21,\n",
       " (261, 110): 30,\n",
       " (97, 257): 13,\n",
       " (257, 258): 5,\n",
       " (109, 117): 14,\n",
       " (103, 260): 3,\n",
       " (110, 117): 9,\n",
       " (98, 260): 6,\n",
       " (264, 274): 4,\n",
       " (274, 259): 2,\n",
       " (273, 85): 9,\n",
       " (274, 115): 12,\n",
       " (101, 97): 32,\n",
       " (262, 100): 6,\n",
       " (101, 274): 3,\n",
       " (260, 110): 8,\n",
       " (110, 274): 6,\n",
       " (264, 102): 4,\n",
       " (112, 101): 20,\n",
       " (105, 274): 9,\n",
       " (97, 115): 46,\n",
       " (264, 109): 5,\n",
       " (97, 107): 9,\n",
       " (110, 256): 12,\n",
       " (262, 101): 4,\n",
       " (120, 112): 13,\n",
       " (99, 262): 6,\n",
       " (262, 97): 15,\n",
       " (260, 256): 22,\n",
       " (256, 263): 10,\n",
       " (156, 99): 7,\n",
       " (87, 101): 3,\n",
       " (101, 256): 7,\n",
       " (32, 87): 6,\n",
       " (87, 104): 7,\n",
       " (104, 269): 8,\n",
       " (110, 102): 2,\n",
       " (258, 261): 7,\n",
       " (116, 121): 10,\n",
       " (121, 264): 16,\n",
       " (108, 272): 55,\n",
       " (272, 97): 4,\n",
       " (32, 269): 15,\n",
       " (269, 103): 8,\n",
       " (103, 257): 7,\n",
       " (257, 101): 7,\n",
       " (101, 260): 1,\n",
       " (258, 104): 8,\n",
       " (257, 267): 3,\n",
       " (267, 111): 12,\n",
       " (101, 108): 43,\n",
       " (108, 102): 1,\n",
       " (115, 107): 3,\n",
       " (264, 263): 3,\n",
       " (156, 87): 1,\n",
       " (104, 272): 1,\n",
       " (272, 100): 8,\n",
       " (256, 110): 13,\n",
       " (101, 101): 10,\n",
       " (267, 274): 2,\n",
       " (115, 63): 1,\n",
       " (63, 32): 3,\n",
       " (73, 258): 2,\n",
       " (258, 259): 31,\n",
       " (258, 114): 4,\n",
       " (115, 268): 2,\n",
       " (268, 121): 7,\n",
       " (121, 63): 1,\n",
       " (117, 108): 34,\n",
       " (100, 110): 3,\n",
       " (105, 102): 20,\n",
       " (100, 63): 1,\n",
       " (63, 263): 1,\n",
       " (32, 32): 19,\n",
       " (32, 72): 8,\n",
       " (264, 85): 10,\n",
       " (258, 116): 18,\n",
       " (259, 102): 1,\n",
       " (102, 117): 8,\n",
       " (272, 114): 3,\n",
       " (256, 269): 8,\n",
       " (105, 114): 22,\n",
       " (114, 256): 9,\n",
       " (100, 263): 3,\n",
       " (119, 114): 8,\n",
       " (115, 121): 14,\n",
       " (109, 115): 5,\n",
       " (115, 273): 37,\n",
       " (273, 84): 25,\n",
       " (110, 115): 13,\n",
       " (115, 266): 6,\n",
       " (105, 117): 1,\n",
       " (109, 263): 3,\n",
       " (267, 103): 2,\n",
       " (111, 274): 1,\n",
       " (258, 263): 5,\n",
       " (156, 269): 2,\n",
       " (269, 97): 4,\n",
       " (108, 270): 2,\n",
       " (101, 111): 6,\n",
       " (111, 112): 20,\n",
       " (268, 111): 3,\n",
       " (117, 110): 27,\n",
       " (267, 259): 9,\n",
       " (108, 267): 8,\n",
       " (32, 117): 10,\n",
       " (112, 117): 9,\n",
       " (265, 272): 23,\n",
       " (272, 108): 4,\n",
       " (108, 265): 31,\n",
       " (265, 103): 21,\n",
       " (103, 117): 17,\n",
       " (117, 97): 16,\n",
       " (157, 273): 7,\n",
       " (273, 65): 8,\n",
       " (264, 259): 30,\n",
       " (256, 100): 25,\n",
       " (116, 269): 11,\n",
       " (269, 115): 4,\n",
       " (101, 33): 3,\n",
       " (84, 111): 4,\n",
       " (258, 49): 7,\n",
       " (49, 51): 3,\n",
       " (51, 53): 2,\n",
       " (53, 32): 4,\n",
       " (102, 102): 14,\n",
       " (102, 260): 9,\n",
       " (262, 115): 20,\n",
       " (115, 99): 15,\n",
       " (264, 261): 7,\n",
       " (261, 118): 4,\n",
       " (256, 49): 6,\n",
       " (49, 49): 19,\n",
       " (49, 48): 24,\n",
       " (48, 48): 32,\n",
       " (259, 260): 34,\n",
       " (110, 103): 5,\n",
       " (103, 32): 4,\n",
       " (32, 49): 20,\n",
       " (111, 259): 15,\n",
       " (267, 104): 3,\n",
       " (104, 105): 34,\n",
       " (116, 266): 14,\n",
       " (266, 105): 13,\n",
       " (274, 264): 2,\n",
       " (264, 119): 21,\n",
       " (97, 100): 16,\n",
       " (100, 100): 7,\n",
       " (100, 273): 4,\n",
       " (32, 71): 3,\n",
       " (71, 105): 1,\n",
       " (258, 269): 4,\n",
       " (269, 266): 1,\n",
       " (111, 106): 4,\n",
       " (106, 101): 1,\n",
       " (256, 101): 7,\n",
       " (98, 114): 9,\n",
       " (99, 99): 10,\n",
       " (272, 257): 7,\n",
       " (257, 104): 2,\n",
       " (258, 109): 8,\n",
       " (100, 256): 13,\n",
       " (104, 117): 2,\n",
       " (109, 265): 13,\n",
       " (73, 262): 8,\n",
       " (111, 101): 8,\n",
       " (115, 110): 8,\n",
       " (101, 45): 15,\n",
       " (45, 111): 4,\n",
       " (102, 258): 1,\n",
       " (110, 264): 12,\n",
       " (267, 105): 4,\n",
       " (258, 101): 14,\n",
       " (120, 99): 3,\n",
       " (110, 258): 9,\n",
       " (119, 110): 7,\n",
       " (32, 114): 18,\n",
       " (272, 116): 8,\n",
       " (114, 259): 2,\n",
       " (270, 108): 9,\n",
       " (98, 117): 14,\n",
       " (262, 274): 7,\n",
       " (108, 116): 10,\n",
       " (261, 101): 1,\n",
       " (262, 119): 13,\n",
       " (259, 257): 4,\n",
       " (148, 119): 2,\n",
       " (121, 273): 4,\n",
       " (77, 111): 1,\n",
       " (262, 112): 4,\n",
       " (109, 270): 2,\n",
       " (97, 118): 14,\n",
       " (105, 98): 17,\n",
       " (114, 268): 5,\n",
       " (268, 105): 13,\n",
       " (118, 97): 9,\n",
       " (104, 265): 7,\n",
       " (100, 108): 7,\n",
       " (256, 103): 4,\n",
       " (103, 266): 2,\n",
       " (266, 272): 4,\n",
       " (119, 45): 1,\n",
       " (45, 108): 2,\n",
       " (108, 258): 5,\n",
       " (265, 105): 2,\n",
       " (264, 121): 6,\n",
       " (117, 263): 5,\n",
       " (262, 99): 5,\n",
       " (99, 260): 2,\n",
       " (260, 116): 5,\n",
       " (32, 266): 13,\n",
       " (266, 100): 15,\n",
       " (272, 259): 7,\n",
       " (259, 101): 22,\n",
       " (109, 273): 4,\n",
       " (97, 267): 2,\n",
       " (267, 268): 3,\n",
       " (108, 264): 2,\n",
       " (261, 117): 7,\n",
       " (257, 107): 3,\n",
       " (256, 98): 19,\n",
       " (98, 105): 30,\n",
       " (259, 114): 4,\n",
       " (101, 105): 17,\n",
       " (273, 69): 2,\n",
       " (69, 109): 1,\n",
       " (121, 33): 1,\n",
       " (256, 76): 1,\n",
       " (76, 101): 2,\n",
       " (103, 269): 1,\n",
       " (269, 260): 1,\n",
       " (260, 274): 4,\n",
       " (105, 269): 6,\n",
       " (98, 97): 14,\n",
       " (99, 32): 10,\n",
       " (32, 101): 32,\n",
       " (148, 105): 2,\n",
       " (157, 264): 7,\n",
       " (260, 109): 4,\n",
       " (262, 113): 1,\n",
       " (256, 114): 18,\n",
       " (148, 268): 3,\n",
       " (267, 271): 12,\n",
       " (273, 67): 3,\n",
       " (100, 269): 5,\n",
       " (267, 98): 9,\n",
       " (98, 272): 14,\n",
       " (264, 99): 4,\n",
       " (109, 268): 12,\n",
       " (272, 119): 5,\n",
       " (120, 97): 11,\n",
       " (101, 102): 19,\n",
       " (105, 120): 6,\n",
       " (120, 32): 12,\n",
       " (156, 85): 2,\n",
       " (85, 43): 39,\n",
       " (43, 263): 1,\n",
       " (264, 115): 12,\n",
       " (43, 48): 24,\n",
       " (48, 52): 3,\n",
       " (52, 49): 3,\n",
       " (49, 32): 9,\n",
       " (156, 65): 2,\n",
       " (65, 263): 2,\n",
       " (116, 257): 7,\n",
       " (112, 105): 9,\n",
       " (48, 51): 10,\n",
       " (51, 66): 1,\n",
       " (66, 56): 1,\n",
       " (156, 206): 1,\n",
       " (206, 184): 1,\n",
       " (184, 263): 1,\n",
       " (101, 107): 2,\n",
       " (115, 109): 8,\n",
       " (97, 273): 1,\n",
       " (69, 97): 2,\n",
       " (32, 271): 32,\n",
       " (104, 266): 1,\n",
       " (262, 110): 3,\n",
       " (267, 113): 1,\n",
       " (112, 260): 12,\n",
       " (67, 104): 2,\n",
       " (68, 97): 1,\n",
       " (256, 271): 26,\n",
       " (49, 44): 1,\n",
       " (44, 49): 2,\n",
       " (49, 52): 2,\n",
       " (52, 44): 1,\n",
       " (49, 50): 6,\n",
       " (264, 111): 2,\n",
       " (110, 108): 8,\n",
       " (272, 49): 3,\n",
       " (50, 56): 4,\n",
       " (56, 44): 1,\n",
       " (44, 50): 1,\n",
       " (50, 51): 4,\n",
       " (51, 55): 2,\n",
       " (148, 97): 2,\n",
       " (262, 49): 2,\n",
       " (50, 37): 1,\n",
       " (37, 32): 1,\n",
       " (117, 274): 15,\n",
       " (103, 110): 8,\n",
       " (100, 264): 9,\n",
       " (108, 269): 8,\n",
       " (119, 259): 1,\n",
       " (259, 33): 1,\n",
       " (115, 260): 7,\n",
       " (260, 118): 3,\n",
       " (55, 44): 1,\n",
       " (44, 52): 1,\n",
       " (52, 54): 1,\n",
       " (54, 56): 1,\n",
       " (156, 112): 3,\n",
       " (32, 268): 5,\n",
       " (268, 101): 11,\n",
       " (268, 100): 1,\n",
       " (122, 101): 2,\n",
       " (265, 270): 1,\n",
       " (257, 256): 1,\n",
       " (114, 112): 3,\n",
       " (105, 267): 5,\n",
       " (116, 264): 18,\n",
       " (108, 112): 1,\n",
       " (112, 102): 1,\n",
       " (122, 256): 4,\n",
       " (66, 101): 2,\n",
       " (268, 114): 2,\n",
       " (114, 265): 6,\n",
       " (261, 104): 1,\n",
       " (101, 59): 2,\n",
       " (59, 32): 7,\n",
       " (115, 113): 4,\n",
       " (117, 268): 4,\n",
       " (54, 195): 1,\n",
       " (195, 151): 1,\n",
       " (151, 49): 1,\n",
       " (32, 61): 6,\n",
       " (61, 32): 6,\n",
       " (50, 53): 2,\n",
       " (53, 54): 1,\n",
       " (267, 101): 3,\n",
       " (265, 101): 12,\n",
       " (32, 54): 1,\n",
       " (54, 53): 5,\n",
       " (53, 44): 3,\n",
       " (44, 53): 3,\n",
       " (53, 51): 3,\n",
       " (51, 54): 3,\n",
       " (258, 274): 8,\n",
       " (274, 116): 1,\n",
       " (101, 259): 7,\n",
       " (260, 273): 11,\n",
       " (77, 97): 2,\n",
       " (40, 99): 3,\n",
       " (99, 107): 9,\n",
       " (32, 122): 4,\n",
       " (122, 111): 4,\n",
       " (109, 41): 3,\n",
       " (258, 117): 7,\n",
       " (66, 108): 1,\n",
       " (117, 256): 2,\n",
       " (264, 103): 2,\n",
       " (101, 269): 7,\n",
       " (45, 117): 4,\n",
       " (114, 114): 9,\n",
       " (103, 97): 10,\n",
       " (258, 40): 4,\n",
       " (40, 109): 2,\n",
       " (260, 41): 3,\n",
       " (268, 115): 2,\n",
       " (108, 121): 14,\n",
       " (262, 261): 6,\n",
       " ...}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets move to the encoding part:\n",
    "you will notice that after getting the mosr commin pair we will apply the min funtion that because the merges is sorted in that order as we saw above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    \"\"\"Encodes text using Byte-Pair Encoding (BPE) with learned merges.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string to encode\n",
    "        \n",
    "    Returns:\n",
    "        List of token IDs representing the encoded text\n",
    "    \n",
    "    Process:\n",
    "        1. Convert text to UTF-8 bytes\n",
    "        2. Iteratively apply BPE merges\n",
    "        3. Return final token sequence\n",
    "    \"\"\"\n",
    "    # Convert text to initial byte tokens\n",
    "    tokens = list(text.encode(\"utf-8\"))  # \"hello\" ‚Üí [104, 101, 108, 108, 111]\n",
    "    \n",
    "    # Continue merging until we can't reduce further (questionable stop condition)\n",
    "    while len(tokens):  # Repeat until no more merges can be applied\n",
    "        # Find all adjacent pairs and their frequencies\n",
    "        counts = most_common_pairs(tokens)  \n",
    "        \n",
    "        # Find the lowest-index merged pair (earliest learned merge)\n",
    "        # merges dict format: {(byte1, byte2): merged_id}\n",
    "        pair = min(counts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        \n",
    "        # Stop if no more known merges exist\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        \n",
    "        # Get merged token ID for this pair\n",
    "        idx = merges[pair]  # e.g., (101, 108) ‚Üí 256\n",
    "        \n",
    "        # Replace all occurrences of pair with merged ID\n",
    "        tokens = merge(tokens, pair, idx)  # Hypothetical merge function\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage (requires populated merges dict)\n",
    "print(encode(\"hello world\"))  \n",
    "# Potential output: [104, 256, 257, 108, 111, 32, 119, 111, 114, 108, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there are a small bug o edge cases we  need to take care off .\n",
    "look at this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \n",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     21\u001b[0m counts \u001b[38;5;241m=\u001b[39m most_common_pairs(tokens)  \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Find the lowest-index merged pair (earliest learned merge)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# merges dict format: {(byte1, byte2): merged_id}\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Stop if no more known merges exist\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m merges:\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print(encode(\"h\"))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the function can handle a single character only pair , so we nned to take of it,\n",
    "and its basically very simple :\n",
    "just change the while loop to handle the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    \"\"\"Encodes text using Byte-Pair Encoding (BPE) with learned merges.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string to encode\n",
    "        \n",
    "    Returns:\n",
    "        List of token IDs representing the encoded text\n",
    "    \n",
    "    Process:\n",
    "        1. Convert text to UTF-8 bytes\n",
    "        2. Iteratively apply BPE merges\n",
    "        3. Return final token sequence\n",
    "    \"\"\"\n",
    "    # Convert text to initial byte tokens\n",
    "    tokens = list(text.encode(\"utf-8\"))  # \"hello\" ‚Üí [104, 101, 108, 108, 111]\n",
    "    \n",
    "    # Continue merging until we can't reduce further (questionable stop condition)\n",
    "    while len(tokens) > 2:  # this is the only change - stop when we have only 2 tokens left\n",
    "        # Find all adjacent pairs and their frequencies\n",
    "        counts = most_common_pairs(tokens)  \n",
    "        \n",
    "        # Find the lowest-index merged pair (earliest learned merge)\n",
    "        # merges dict format: {(byte1, byte2): merged_id}\n",
    "        pair = min(counts, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        \n",
    "        # Stop if no more known merges exist\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        \n",
    "        # Get merged token ID for this pair\n",
    "        idx = merges[pair]  # e.g., (101, 108) ‚Üí 256\n",
    "        \n",
    "        # Replace all occurrences of pair with merged ID\n",
    "        tokens = merge(tokens, pair, idx)  # Hypothetical merge function\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage (requires populated merges dict)\n",
    "print(encode(\"h\"))  \n",
    "# Potential output: [104]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have what we need to build the tokenizer class :\n",
    "let make some helper functions to make even more smooth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def replace_control_characters(s: str) -> str:\n",
    "   \n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        # Check Unicode category - 'C' means control character\n",
    "        # Categories: Cc (control), Cf (format), Cn (unassigned), etc.\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch)  # Keep normal characters\n",
    "        else:\n",
    "            # Replace control chars with Unicode escape (e.g., \\u0000)\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\")  \n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    \n",
    "    # Attempt UTF-8 decoding with error replacement (ÔøΩ)\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    \n",
    "    # Further sanitize any remaining control characters\n",
    "    s = replace_control_characters(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These functions are crucial for:**  \n",
    "\n",
    "1. **Safety** üîí  \n",
    "   - Handle invisible/control characters (e.g., `\\x00`, tabs) that could corrupt tokenization  \n",
    "   - Prevent encoding crashes from invalid UTF-8 bytes  \n",
    "\n",
    "2. **Debuggability** üêû  \n",
    "   - Convert raw bytes ‚Üí human-readable format (`b'\\x80'` ‚Üí `\\u0080`)  \n",
    "   - Expose hidden characters that might break tokenization rules  \n",
    "\n",
    "3. **Stability** ‚öñÔ∏è  \n",
    "   - Ensure consistent tokenization of messy real-world text  \n",
    "   - Avoid \"mystery tokens\" from unprocessed control codes  \n",
    "\n",
    "Without these, your tokenizer would silently fail on many real-world text inputs! üî•\n",
    "\n",
    "Now lets build the base class for Tokenize that we will expand on and add functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char ÔøΩ.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to build the first version on the tokenizer\n",
    "\n",
    "the following is just the compination of all the functions we made : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Initialize parent Tokenizer class\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"Train BPE tokenizer on text corpus\n",
    "        Args:\n",
    "            text: Training text corpus\n",
    "            vocab_size: Target vocabulary size (must be ‚â•256)\n",
    "            verbose: Print merge operations\n",
    "        \"\"\"\n",
    "        assert vocab_size >= 256, \"Vocab size must include base 256 bytes\"\n",
    "        num_merges = vocab_size - 256  # Calculate needed merges\n",
    "        \n",
    "        # Convert text to raw UTF-8 byte IDs (0-255)\n",
    "        ids = list(text.encode(\"utf-8\"))  \n",
    "        \n",
    "        # Initialize tracking structures\n",
    "        merges = {}  # (byte1, byte2) ‚Üí merge_id\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}  # Base vocabulary\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # 1. Find most frequent adjacent pair\n",
    "            counts = most_common_pairs(ids)  # Hypothetical helper function\n",
    "            if not counts:\n",
    "                break\n",
    "            pair = max(counts, key=counts.get)\n",
    "            \n",
    "            # 2. Create new token ID (256, 257, ...)\n",
    "            new_id = 256 + i\n",
    "            \n",
    "            # 3. Replace all pair occurrences in corpus\n",
    "            ids = merge(ids, pair, new_id)  # Hypothetical merge function\n",
    "            \n",
    "            # 4. Update vocabulary and merge rules\n",
    "            vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            merges[pair] = new_id\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Merged {pair} ‚Üí {new_id} ({counts[pair]} occurrences)\")\n",
    "                \n",
    "            # Update class state incrementally (controversial choice)\n",
    "            self.merges = merges  # Overwrites previous state each iteration\n",
    "            self.vocab = vocab     # Potentially conflicts with parent class\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to text\n",
    "        Args:\n",
    "            ids: List of token IDs (base bytes + merged tokens)\n",
    "        Returns:\n",
    "            Decoded UTF-8 string with error handling\n",
    "        \"\"\"\n",
    "        # Reconstruct bytes using vocabulary mapping\n",
    "        byte_stream = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        # Convert to string, replacing invalid bytes\n",
    "        return byte_stream.decode(\"utf-8\", errors=\"replace\")  \n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token IDs using learned merges\n",
    "        Args:\n",
    "            text: Input string to tokenize\n",
    "        Returns:\n",
    "            List of token IDs\n",
    "        \"\"\"\n",
    "        # Convert to initial byte tokens\n",
    "        ids = list(text.encode(\"utf-8\"))  \n",
    "        \n",
    "        # Iteratively apply merges until no more can be applied\n",
    "        while len(ids) >= 2:\n",
    "            # WARNING: This line appears incomplete/broken - \n",
    "            # 'stats' is just a tuple containing ids, not pair counts\n",
    "            stats = (ids)  # Should be counting pairs like in train()\n",
    "            \n",
    "            # Find pair with lowest merge priority (earliest learned first)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            \n",
    "            # Stop if no applicable merges remain\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "                \n",
    "            # Apply the merge\n",
    "            merge_id = self.merges[pair]\n",
    "            ids = merge(ids, pair, merge_id)\n",
    "            \n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Basic BPE to GPT-Style Tokenizer: Core Evolution\n",
    "\n",
    "## **Core BPE Foundation** üîÑ\n",
    "Retains essential Byte-Pair Encoding mechanics:\n",
    "- Starts with base 256 byte vocabulary\n",
    "- Iteratively merges frequent pairs\n",
    "- Reversible encoding/decoding via byte-level fidelity\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Algorithmic Enhancements** üöÄ\n",
    "\n",
    "### 1. **Regex Pre-Splitting & Chunk Processing**\n",
    "```python\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[...]|\\s+\"\"\"\n",
    "```\n",
    "- **Text chunking**: Splits input using GPT-4 rules before BPE  \n",
    "- **Boundary constraints**: Merges occur _only within chunks_ (never cross linguistic units)  \n",
    "- **Impact**: Preserves contractions (\"don't\" ‚Üí [\"do\", \"n't\"]), separates numbers/punctuation, handles Unicode via `\\p{L}`/`\\p{N}`\n",
    "\n",
    "### 2. **Merge Priority System**\n",
    "```python\n",
    "pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "```\n",
    "- Tracks merge order via `self.merges` dictionary  \n",
    "- Applies _earliest-learned valid merges first_ during encoding  \n",
    "- Early termination for unmergeable pairs\n",
    "\n",
    "### 3. **Special Token Architecture**\n",
    "```python\n",
    "def register_special_tokens(self, special_tokens):\n",
    "    self.special_tokens = special_tokens  # e.g., {'<|endoftext|>': 100257}\n",
    "```\n",
    "- **Safeguards**: Explicit registry prevents splitting/merging of reserved tokens  \n",
    "- **Policies**: Configurable `allowed_special` rules (strict \"none_raise\" default)  \n",
    "- **Isolation**: Special tokens handled via regex splitting in `encode()`\n",
    "\n",
    "### 4. **Dual Processing Modes**\n",
    "| Mode                 | Function              | Key Feature                     |\n",
    "|----------------------|-----------------------|---------------------------------|\n",
    "| Ordinary Encoding    | `encode_ordinary()`   | Pure BPE (ignores specials)     | \n",
    "| Production Encoding  | `encode()`            | Handles special tokens + safety |\n",
    "\n",
    "### 5. **Training Process Shift**\n",
    "- **Chunk-wise stats**: Frequency counts per regex split segment vs whole-text  \n",
    "- **Impact**: Merges reflect localized patterns (e.g., \"123\" as number chunk)  \n",
    "- **Whitespace handling**: Explicit ` ?`/`\\s*` rules prevent space-content merges\n",
    "\n",
    "### 6. **Production-Grade Features**\n",
    "| Feature               | Implementation                      | Purpose                          |\n",
    "|-----------------------|-------------------------------------|----------------------------------|\n",
    "| Byte-level fallback   | `vocab[idx] = bytes([idx])`         | Guaranteed reversible decoding   |\n",
    "| Error control         | `errors=\"replace\"`                  | Robust invalid byte handling      |\n",
    "| Unicode normalization | Implicit in regex patterns          | Consistent multilingual support  |\n",
    "| Efficient lookups     | Precompiled regex                   | GPT-like performance             |\n",
    "\n",
    "---\n",
    "\n",
    "## **Core Impact** üîë\n",
    "1. **Linguistic Awareness**: Regex boundaries produce human-meaningful tokens  \n",
    "2. **Safety**: Special tokens remain intact, configurable error policies  \n",
    "3. **Reproducibility**: Mirrors GPT's preprocessing ‚Üí compatible merge rules  \n",
    "4. **Scalability**: Chunk-wise processing enables parallelization  \n",
    "\n",
    "```python\n",
    "# Real-World Implementation\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.train(text, 50257)  # GPT-2's vocab size\n",
    "tokenizer.register_special_tokens({\"<|endoftext|>\": 50256})\n",
    "ids = tokenizer.encode(\"Hello<|endoftext|>\", allowed_special=\"all\")  # [15496, 50256]\n",
    "```\n",
    "\n",
    "**Final Evolution**: Combines BPE's efficiency with GPT's linguistic intuition through constrained merges and production hardening. üß†\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Lets see the GPT split patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel you ‚Äì regex patterns are about as exciting as watching paint dry ü•±. Let's rip through this with *minimum pain, maximum intuition*:\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Bother with Patterns?**  \n",
    "Imagine trying to learn words by randomly gluing letters:  \n",
    "‚ùå Bad: `\"don\" + \"t\"` ‚Üí misses the contraction `\"don't\"`  \n",
    "‚ùå Worse: `\"gpt\" + \"-3\"` ‚Üí mixes letters & symbols  \n",
    "\n",
    "The patterns **force sensible chunks** before BPE even starts:  \n",
    "```python\n",
    "# GPT-4's Secret Sauce (Simplified)\n",
    "[\n",
    "  \"don't\",    # Keep contractions whole\n",
    "  \" gpt\",     # Space + word\n",
    "  \"-\",        # Punctuation alone\n",
    "  \"4\",        # Numbers as units\n",
    "  \"\\n\"        # Newlines separate\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **GPT-2 vs GPT-4 TL;DR**  \n",
    "| Case | GPT-2 Behavior | GPT-4 Improvement |\n",
    "|------|----------------|-------------------|\n",
    "| **`Don'T`** | Splits to `\"Don\"+\"'T\"` | Case-insensitive ‚Üí `\"don't\"` |\n",
    "| **`1234`**  | Single token `1234` | Splits to `1 234` (better for math) |\n",
    "| **`hello\\nworld`** | `[\"hello\",\"world\"]` | `[\"hello\", \"\\n\", \"world\"]` (captures structure) |\n",
    "| **`$1.50`** | `[\"$1\", \".50\"]` | `[\" $\", \"1.50\"]` (cleaner currency) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Real Impact** (You Care About This)  \n",
    "```python\n",
    "# Without Patterns\n",
    "Tokens: [\"d\", \"on\", \"'t\", \"g\", \"pt\", \"-\", \"4\"]  # Messy subwords\n",
    "\n",
    "# With GPT-4 Pattern\n",
    "Tokens: [\"don't\", \" gpt\", \"-\", \"4\"]  # Human-like units\n",
    "```\n",
    "‚Üí Fewer tokens & **meaningful chunks** ‚Üí better model understanding  \n",
    "\n",
    "---\n",
    "\n",
    "**You‚Äôre Done!** üéâ Now you know enough to *use* these tokenizers effectively. The regex details? Let OpenAI‚Äôs engineers worry about those üòâ.\n",
    "\n",
    "for us what we should do is to recompile the text with these patterns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns precompiled once for efficiency - critical for performance \n",
    "# since tokenization is called billions of times during training/inference\n",
    "\n",
    "import regex as re  # Using regex module instead of re for full Unicode support\n",
    "\n",
    "# GPT-2's pattern: Splits text into linguistically meaningful chunks\n",
    "# Key features:\n",
    "# - English contractions as single units ('s, n't, etc)\n",
    "# - Separates words/numbers/punctuation\n",
    "# - Special whitespace handling\n",
    "compiled_pattern_gpt2 = re.compile(GPT2_SPLIT_PATTERN)  # Compile once, reuse forever\n",
    "\n",
    "# GPT-4's upgraded pattern with production optimizations:\n",
    "# - Case-insensitive contractions (handles 'LL like 'll)\n",
    "# - Numbers split into 1-3 digit groups (better for math/numeric reasoning)\n",
    "# - Explicit newline handling (crucial for code/structured text)\n",
    "# - Possessive quantifiers prevent catastrophic backtracking\n",
    "compiled_pattern_gpt4 = re.compile(GPT4_SPLIT_PATTERN)  # Precompiled = 10-100x faster than recompiling each use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets see this in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test corpus\n",
    "text = \"\"\"\n",
    "Don't you ‚ù§Ô∏è GPT-4? Here's why:\n",
    "1. It costs $5/mo (cheap!)\n",
    "2. Handles 123456 ‚Üí \"123k tokens\"\n",
    "3. Newlines\\rmatter! 50% better.\n",
    "Try: hello@example.com or visit https://openai.com\n",
    "\"\"\"\n",
    "\n",
    "# Define patterns\n",
    "GPT2_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# Split using both patterns\n",
    "def analyze(text):\n",
    "    return {\n",
    "        \"GPT-2\": re.findall(GPT2_PATTERN, text),\n",
    "        \"GPT-4\": re.findall(GPT4_PATTERN, text)\n",
    "    }\n",
    "\n",
    "results = analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'Don', \"'t\", ' you', ' ‚ù§Ô∏è', ' GPT', '-', '4', '?', ' Here', \"'s\", ' why', ':', '\\n', '1', '.', ' It', ' costs', ' $', '5', '/', 'mo', ' (', 'cheap', '!)', '\\n', '2', '.', ' Handles', ' 123456', ' ‚Üí', ' \"', '123', 'k', ' tokens', '\"', '\\n', '3', '.', ' Newlines', '\\r', 'matter', '!', ' 50', '%', ' better', '.', '\\n', 'Try', ':', ' hello', '@', 'example', '.', 'com', ' or', ' visit', ' https', '://', 'openai', '.', 'com', '\\n']\n",
      "['\\n', 'Don', \"'t\", ' you', ' ‚ù§Ô∏è', ' GPT', '-', '4', '?', ' Here', \"'s\", ' why', ':\\n', '1', '.', ' It', ' costs', ' $', '5', '/mo', ' (', 'cheap', '!)\\n', '2', '.', ' Handles', ' ', '123', '456', ' ‚Üí', ' \"', '123', 'k', ' tokens', '\"\\n', '3', '.', ' Newlines', '\\r', 'matter', '!', ' ', '50', '%', ' better', '.\\n', 'Try', ':', ' hello', '@example', '.com', ' or', ' visit', ' https', '://', 'openai', '.com', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(results[\"GPT-2\"])\n",
    "print(results[\"GPT-4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the tokenization differences **side-by-side** using your actual outputs:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations & Analysis**\n",
    "\n",
    "| **Text Segment**       | **GPT-2 Split**                     | **GPT-4 Split**                       | **Why It Matters** |\n",
    "|-------------------------|-------------------------------------|---------------------------------------|--------------------|\n",
    "| **`Don't`**             | `['Don', \"'t\"]`                    | `['Don', \"'t\"]`                      | Both split contractions (not ideal) ‚Äì shows pattern limitations |\n",
    "| **`GPT-4?`**            | `['GPT', '-', '4', '?']`           | `['GPT', '-', '4', '?']`             | Identical handling of hyphenated terms |\n",
    "| **`Here's why:`**       | `['Here', \"'s\", ' why', ':']`      | `['Here', \"'s\", ' why', ':\\n']`      | GPT-4 **combines colon + newline** into one token |\n",
    "| **`$5/mo`**             | `[' $', '5', '/', 'mo']`           | `[' $', '5', '/mo']`                 | GPT-4 keeps `/mo` as a unit (better for pricing terms) |\n",
    "| **`123456 ‚Üí`**          | `['123456', '‚Üí']`                  | `[' ', '123', '456', ' ‚Üí']`          | GPT-4 splits numbers into **1-3 digits** + explicit space |\n",
    "| **`Newlines\\rmatter!`** | `['Newlines', '\\r', 'matter', '!']`| `['Newlines', '\\r', 'matter', '!']`  | Identical newline handling |\n",
    "| **`hello@example.com`** | `['hello', '@example', '.com']`    | `['hello', '@example', '.com']`      | Both fail to split email fully (GPT-4 would need custom rules) |\n",
    "| **`https://openai.com`**| `['https', '://openai', '.com']`   | `['https', '://', 'openai', '.com']` | GPT-4 isolates `://` (better for URL protocols) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Critical Differences**\n",
    "1. **Number Handling**  \n",
    "   - GPT-2: `123456` ‚Üí single token  \n",
    "   - GPT-4: `123456` ‚Üí `[' ', '123', '456']` (split + explicit space)  \n",
    "   *‚Üí Better for numeric reasoning tasks*\n",
    "\n",
    "2. **Symbol Isolation**  \n",
    "   - GPT-4 separates `://` (URLs) and `:\\n` (colon + newline)  \n",
    "   - GPT-2 merges these with adjacent characters  \n",
    "   *‚Üí Cleaner separation of protocol/formatting markers*\n",
    "\n",
    "3. **Unit Preservation**  \n",
    "   - GPT-4 keeps `/mo` intact vs GPT-2's `/` + `mo` split  \n",
    "   *‚Üí More meaningful tokens for subscription terms*\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern Limitations Exposed**\n",
    "1. **Email Addresses**  \n",
    "   Both fail to split `@example` ‚Üí `['@example']` instead of `['@', 'example']`  \n",
    "   *‚Üí Requires custom regex rules for email handling*\n",
    "\n",
    "2. **Contractions**  \n",
    "   `Don't` splits to `['Don', \"'t\"]` instead of a single token  \n",
    "   *‚Üí BPE must later re-learn this merge (inefficient)*\n",
    "\n",
    "3. **Spaces After Numbers**  \n",
    "   GPT-4 adds explicit `' '` before `123` in `123456` ‚Üí `[' ', '123', ...]`  \n",
    "   *‚Üí May lead to space-related token bloat*\n",
    "\n",
    "---\n",
    "\n",
    "### **Actionable Takeaways**\n",
    "1. **Use GPT-4 Pattern If You Need:**  \n",
    "   - Better number handling (e.g., math/code)  \n",
    "   - Explicit URL protocol separation  \n",
    "   - Structured text with newlines/formatting  \n",
    "\n",
    "2. **Stick with GPT-2 Pattern For:**  \n",
    "   - Simpler text without complex symbols  \n",
    "   - Backward compatibility with older models  \n",
    "\n",
    "3. **Customize Patterns For:**  \n",
    "   - Emails (`@` splitting)  \n",
    "   - Domain-specific terms (e.g., `/mo` in fintech)  \n",
    "\n",
    "This comparison shows regex splitting is **necessary but imperfect** ‚Äì BPE later compensates for these quirks during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Next lets see the handling of the special tokens \n",
    "\n",
    "Why Special Tokens Matter\n",
    "Special tokens act as \"control characters\" for language models, providing:\n",
    "\n",
    "- Boundary Control (start/end of text, document splits)\n",
    "\n",
    "- Functional Signals (masking, padding, task separation)\n",
    "\n",
    "- Safety (prevent injection attacks via reserved tokens)\n",
    "\n",
    "- Structured Input (separate instructions from content)\n",
    "\n",
    "To address the differences between handling text with and without special tokens, let's outline the key distinctions and provide practical examples using the tokenizer's functions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Differences: `encode` vs. `encode_ordinary`**\n",
    "\n",
    "| **Aspect**               | **`encode` (With Special Tokens)**                     | **`encode_ordinary` (Without Special Tokens)**          |\n",
    "|--------------------------|-------------------------------------------------------|---------------------------------------------------------|\n",
    "| **Special Tokens**       | Recognizes and preserves registered tokens (e.g., `<\\|SPECIAL\\|>`) | Treats all text as raw input, ignoring special tokens. |\n",
    "| **Safety**               | Prevents splitting reserved tokens into subwords.     | Splits all text, including sequences matching special tokens. |\n",
    "| **Use Case**             | Production workflows requiring control tokens (e.g., `<\\|endoftext\\|>`). | General-purpose tokenization where special tokens are irrelevant. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Workflow**\n",
    "\n",
    "\n",
    "```python\n",
    "# Hypothetical tokenizer setup\n",
    "special_tokens = {\"<|SPECIAL|>\": 1000}\n",
    "text = \"Hello<|SPECIAL|>world\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Tokenization Results**\n",
    "\n",
    "#### **With `encode` (Special Tokens Allowed)**\n",
    "```python\n",
    "# Encode with special token handling\n",
    "ids = encode(text, allowed_special={\"<|SPECIAL|>\"})\n",
    "\n",
    "# Output: [15496, 1000, 1917]\n",
    "# Decoded: \"Hello<|SPECIAL|>world\"\n",
    "```\n",
    "- **Behavior**:  \n",
    "  - `<|SPECIAL|>` is preserved as a single token (`1000`).  \n",
    "  - The tokenizer explicitly respects registered tokens.  \n",
    "\n",
    "#### **With `encode_ordinary` (No Special Tokens)**\n",
    "```python\n",
    "# Encode ignoring special tokens\n",
    "ids = encode_ordinary(text)\n",
    "\n",
    "# Output: [72, 101, 108, 108, 111, 60, 124, 83, 80, 69, 67, 73, 65, 76, 124, 62, 119, 111, 114, 108, 100]\n",
    "# Decoded: \"Hello<|SPECIAL|>world\" (but fragmented)\n",
    "```\n",
    "- **Behavior**:  \n",
    "  - `<|SPECIAL|>` is split into raw bytes (e.g., `<` ‚Üí `60`, `|` ‚Üí `124`, etc.).  \n",
    "  - No awareness of special tokens; everything is processed as regular text.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Critical Implications**\n",
    "1. **Security**:  \n",
    "   - `encode` blocks accidental/injected special tokens (e.g., `<|ADMIN_COMMAND|>`).  \n",
    "   - `encode_ordinary` naively splits such tokens, potentially allowing exploits.  \n",
    "\n",
    "2. **Consistency**:  \n",
    "   - `encode` ensures special tokens remain intact (e.g., `<|endoftext|>`).  \n",
    "   - `encode_ordinary` fragments them, breaking downstream logic relying on these markers.  \n",
    "\n",
    "3. **Output Length**:  \n",
    "   - `encode` produces fewer tokens for reserved sequences (1 token for `<|SPECIAL|>`).  \n",
    "   - `encode_ordinary` generates multiple tokens for the same sequence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, the moment of truth has arrived! Below is the implementation of GPTokenizer, which encapsulates all the functionalities we've discussed so far. While the code might initially appear lengthy or intimidating, by now you should be able to understand every aspect of it with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_pairs(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "    def __init__(self, pattern=None):\n",
    "       \n",
    "        super().__init__()\n",
    "        # Compile regex pattern for efficient reuse\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        # Special token registries\n",
    "        self.special_tokens = {}  # str -> int mapping\n",
    "        self.inverse_special_tokens = {}  # int -> str mapping\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"Core BPE training algorithm with regex pre-processing\"\"\"\n",
    "        # Validate target vocabulary size\n",
    "        assert vocab_size >= 256, \"Vocabulary must include base 256 bytes\"\n",
    "        num_merges = vocab_size - 256  # Number of merges needed\n",
    "\n",
    "        # Phase 1: Regex-based text chunking\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        \n",
    "        # Phase 2: Convert chunks to initial byte IDs\n",
    "        ids = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
    "\n",
    "        # Phase 3: Iterative pair merging\n",
    "        merges = {}  # (int, int) -> int mapping of merge operations\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}  # Base byte vocabulary\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            # Collect pair statistics across all chunks\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                most_common_pairs(chunk_ids, stats)  # Populates stats dict\n",
    "\n",
    "            if not stats:\n",
    "                break  # Early stopping if no pairs left\n",
    "\n",
    "            # Find most frequent pair\n",
    "            pair = max(stats, key=stats.get)\n",
    "            new_id = 256 + i  # Assign next available token ID\n",
    "            \n",
    "            # Perform merge across all chunks\n",
    "            ids = [merge(chunk_ids, pair, new_id) for chunk_ids in ids]\n",
    "            \n",
    "            # Update merge records and vocabulary\n",
    "            merges[pair] = new_id\n",
    "            vocab[new_id] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Merged {vocab[pair[0]]} + {vocab[pair[1]]} ‚Üí {vocab[new_id]}\")\n",
    "\n",
    "        # Finalize model state\n",
    "        self.merges = merges  # Merge rules for encoding\n",
    "        self.vocab = vocab    # Token ID to bytes mapping\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        \"\"\"Register special tokens that bypass normal tokenization rules\"\"\"\n",
    "        self.special_tokens = special_tokens\n",
    "        # Create inverse mapping with UTF-8 bytes for decoding\n",
    "        self.inverse_special_tokens = {\n",
    "            idx: token.encode(\"utf-8\") for token, idx in special_tokens.items()\n",
    "        }\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to text with special token handling\"\"\"\n",
    "        byte_parts = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                byte_parts.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                byte_parts.append(self.inverse_special_tokens[idx])\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid token ID: {idx}\")\n",
    "        # Join bytes and decode with error replacement\n",
    "        return b\"\".join(byte_parts).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        \"\"\"Core BPE encoding for individual text chunks\"\"\"\n",
    "        ids = list(text_bytes)  # Start with raw byte values\n",
    "        \n",
    "        # Iteratively apply merges until no more possible\n",
    "        while len(ids) >= 2:\n",
    "            # Find all possible pairs and their merge priority\n",
    "            stats = most_common_pairs(ids)\n",
    "            # Select pair with earliest merge index (lowest ID)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            \n",
    "            # Stop if no registered merges remain\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "                \n",
    "            # Perform the merge operation\n",
    "            merge_id = self.merges[pair]\n",
    "            ids = merge(ids, pair, merge_id)\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encode text without special token processing\"\"\"\n",
    "        # Split text into regex-defined chunks\n",
    "        chunks = re.findall(self.compiled_pattern, text)\n",
    "        ids = []\n",
    "        # Encode each chunk independently\n",
    "        for chunk in chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\")\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"Production-grade encoding with special token handling\"\"\"\n",
    "        # Resolve special token handling policy\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            # Verify no special tokens are present\n",
    "            if any(token in text for token in self.special_tokens):\n",
    "                raise ValueError(\"Disallowed special token found in text\")\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() \n",
    "                      if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid special token policy: {allowed_special}\")\n",
    "\n",
    "        if not special:\n",
    "            return self.encode_ordinary(text)  # Fast path\n",
    "        \n",
    "        # Split text around special tokens\n",
    "        special_pattern = re.compile(\"(\" + \"|\".join(map(re.escape, special)) + \")\")\n",
    "        parts = special_pattern.split(text)\n",
    "        \n",
    "        ids = []\n",
    "        for part in parts:\n",
    "            if part in special:\n",
    "                # Handle special token directly\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # Process normal text\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put this to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Case: Text with special<|SPECIAL|>to...\n",
      "Encoded: 17 tokens\n",
      "Roundtrip: True\n",
      "Special tokens: [500]\n",
      "\n",
      "Case: Numbers: 123,456.78...\n",
      "Encoded: 18 tokens\n",
      "Roundtrip: True\n",
      "Special tokens: []\n",
      "\n",
      "Case: Emoji: üòÄ+ü§£...\n",
      "Encoded: 16 tokens\n",
      "Roundtrip: True\n",
      "Special tokens: []\n",
      "\n",
      "Case:  contractions don't won't...\n",
      "Encoded: 15 tokens\n",
      "Roundtrip: True\n",
      "Special tokens: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Mini test corpus with key comparison points\n",
    "corpus = requests.get(\"https://tinyurl.com/shakespeare-txt\").text[:10000]  # 10k chars\n",
    "tokenizer = RegexTokenizer()\n",
    "\n",
    "# Train with smaller vocab for faster comparison\n",
    "tokenizer.train(corpus, vocab_size=500, verbose=False)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Text with special<|SPECIAL|>token\", {\"<|SPECIAL|>\": 500}, {\"<|SPECIAL|>\"}),  # Use set\n",
    "    (\"Numbers: 123,456.78\", {}, set()),  # Empty set instead of list\n",
    "    (\"Emoji: üòÄ+ü§£\", {}, set()),\n",
    "    (\" contractions don't won't\", {}, set())\n",
    "]\n",
    "\n",
    "for text, specials, allowed in test_cases:\n",
    "    tokenizer.register_special_tokens(specials)\n",
    "    try:\n",
    "        encoded = tokenizer.encode(text, allowed_special=allowed)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"\\nCase: {text[:30]}...\")\n",
    "        print(f\"Encoded: {len(encoded)} tokens\") \n",
    "        print(f\"Roundtrip: {text == decoded}\")\n",
    "        print(f\"Special tokens: {[t for t in encoded if t in specials.values()]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nError in case '{text[:30]}...': {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets pull tiktoken out the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Case: Text with special<|SPECIAL|>to...\n",
      "Encoded: 10 tokens\n",
      "Roundtrip: True\n",
      "Special token handling: True\n",
      "Token sample: [1199, 449, 3361, 27, 91]...\n",
      "\n",
      "Case: Numbers: 123,456.78...\n",
      "Encoded: 8 tokens\n",
      "Roundtrip: True\n",
      "Special token handling: N/A\n",
      "Token sample: [28336, 25, 220, 4513, 11]...\n",
      "\n",
      "Case: Emoji: üòÄ+ü§£...\n",
      "Encoded: 7 tokens\n",
      "Roundtrip: True\n",
      "Special token handling: N/A\n",
      "Token sample: [93831, 25, 91416, 10, 9468]...\n",
      "\n",
      "Case:  contractions don't won't...\n",
      "Encoded: 6 tokens\n",
      "Roundtrip: True\n",
      "Special token handling: N/A\n",
      "Token sample: [6155, 4109, 1541, 956, 2834]...\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize OpenAI's tokenizer\n",
    "openai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4's tokenizer\n",
    "\n",
    "# Same test cases (special tokens handled differently in OpenAI)\n",
    "test_cases = [\n",
    "    (\"Text with special<|SPECIAL|>token\", {\"<|SPECIAL|>\"}),\n",
    "    (\"Numbers: 123,456.78\", []),\n",
    "    (\"Emoji: üòÄ+ü§£\", []),\n",
    "    (\" contractions don't won't\", [])\n",
    "]\n",
    "\n",
    "for text, specials in test_cases:\n",
    "    try:\n",
    "        encoded = openai_tokenizer.encode(text)\n",
    "        decoded = openai_tokenizer.decode(encoded)\n",
    "        \n",
    "        print(f\"\\nCase: {text[:30]}...\")\n",
    "        print(f\"Encoded: {len(encoded)} tokens\")\n",
    "        print(f\"Roundtrip: {text == decoded}\")\n",
    "        print(f\"Special token handling: {'<|SPECIAL|>' in decoded if specials else 'N/A'}\")\n",
    "        print(f\"Token sample: {encoded[:5]}...\")  # Show first few tokens\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in case '{text[:30]}...': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------ Test Case: ------------------------\n",
      "Text: Text with special<|SPECIAL|>token...\n",
      "\n",
      "[Custom Tokenizer]\n",
      "Tokens: 17 | Roundtrip: True\n",
      "Special tokens: [500]\n",
      "Sample tokens: [84, 415, 116, 412, 105]...\n",
      "\n",
      "[OpenAI Tokenizer]\n",
      "Tokens: 10 | Roundtrip: True\n",
      "Special preserved: True\n",
      "Sample tokens: [1199, 449, 3361, 27, 91]...\n",
      "\n",
      "------------------------ Test Case: ------------------------\n",
      "Text: Numbers: 123,456.78...\n",
      "\n",
      "[Custom Tokenizer]\n",
      "Tokens: 18 | Roundtrip: True\n",
      "Special tokens: []\n",
      "Sample tokens: [78, 117, 109, 98, 396]...\n",
      "\n",
      "[OpenAI Tokenizer]\n",
      "Tokens: 8 | Roundtrip: True\n",
      "Special preserved: N/A\n",
      "Sample tokens: [28336, 25, 220, 4513, 11]...\n",
      "\n",
      "------------------------ Test Case: ------------------------\n",
      "Text: Emoji: üòÄ+ü§£...\n",
      "\n",
      "[Custom Tokenizer]\n",
      "Tokens: 16 | Roundtrip: True\n",
      "Special tokens: []\n",
      "Sample tokens: [69, 109, 111, 106, 105]...\n",
      "\n",
      "[OpenAI Tokenizer]\n",
      "Tokens: 7 | Roundtrip: True\n",
      "Special preserved: N/A\n",
      "Sample tokens: [93831, 25, 91416, 10, 9468]...\n",
      "\n",
      "------------------------ Test Case: ------------------------\n",
      "Text:  contractions don't won't...\n",
      "\n",
      "[Custom Tokenizer]\n",
      "Tokens: 15 | Roundtrip: True\n",
      "Special tokens: []\n",
      "Sample tokens: [269, 336, 114, 97, 99]...\n",
      "\n",
      "[OpenAI Tokenizer]\n",
      "Tokens: 6 | Roundtrip: True\n",
      "Special preserved: N/A\n",
      "Sample tokens: [6155, 4109, 1541, 956, 2834]...\n"
     ]
    }
   ],
   "source": [
    "# Shared setup\n",
    "corpus = requests.get(\"https://tinyurl.com/shakespeare-txt\").text[:10000]\n",
    "test_cases = [\n",
    "    (\"Text with special<|SPECIAL|>token\", {\"<|SPECIAL|>\": 500}, {\"<|SPECIAL|>\"}),\n",
    "    (\"Numbers: 123,456.78\", {}, set()),\n",
    "    (\"Emoji: üòÄ+ü§£\", {}, set()),\n",
    "    (\" contractions don't won't\", {}, set())\n",
    "]\n",
    "\n",
    "# Initialize tokenizers\n",
    "openai_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "custom_tokenizer = RegexTokenizer()\n",
    "custom_tokenizer.train(corpus, vocab_size=500)\n",
    "custom_tokenizer.register_special_tokens({\"<|SPECIAL|>\": 500})\n",
    "\n",
    "# Comparison function\n",
    "def compare_tokenizers(text, specials, allowed):\n",
    "    print(f\"\\n{' Test Case: ':-^60}\")\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    \n",
    "    # Custom tokenizer\n",
    "    custom_tokenizer.register_special_tokens(specials)\n",
    "    try:\n",
    "        custom_encoded = custom_tokenizer.encode(text, allowed_special=allowed)\n",
    "        custom_decoded = custom_tokenizer.decode(custom_encoded)\n",
    "        print(\"\\n[Custom Tokenizer]\")\n",
    "        print(f\"Tokens: {len(custom_encoded)} | Roundtrip: {text == custom_decoded}\")\n",
    "        print(f\"Special tokens: {[t for t in custom_encoded if t in specials.values()]}\")\n",
    "        print(f\"Sample tokens: {custom_encoded[:5]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Custom Error: {str(e)}\")\n",
    "    \n",
    "    # OpenAI tokenizer\n",
    "    try:\n",
    "        openai_encoded = openai_tokenizer.encode(text)\n",
    "        openai_decoded = openai_tokenizer.decode(openai_encoded)\n",
    "        print(\"\\n[OpenAI Tokenizer]\")\n",
    "        print(f\"Tokens: {len(openai_encoded)} | Roundtrip: {text == openai_decoded}\")\n",
    "        print(f\"Special preserved: {'<|SPECIAL|>' in openai_decoded if specials else 'N/A'}\")\n",
    "        print(f\"Sample tokens: {openai_encoded[:5]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI Error: {str(e)}\")\n",
    "\n",
    "# Run comparisons\n",
    "for case in test_cases:\n",
    "    compare_tokenizers(*case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze these results and understand the key differences between the two tokenizers:\n",
    "\n",
    "### **1. Special Token Handling**\n",
    "| **Aspect**          | Custom Tokenizer                | OpenAI Tokenizer               |\n",
    "|----------------------|----------------------------------|---------------------------------|\n",
    "| **Preservation**     | Single token (500)              | Split into multiple tokens      |\n",
    "| **Roundtrip**        | Perfect (`True`)                | Perfect (`True`)                |\n",
    "| **Token Count**      | 17 tokens                       | 10 tokens                       |\n",
    "| **Sample Tokens**    | `[84, 415, 116, 412, 105]`      | `[1199, 449, 3361, 27, 91]`     |\n",
    "\n",
    "**Key Insight**: Your tokenizer successfully preserves special tokens as atomic units, while OpenAI's splits them into components.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Number Handling**\n",
    "| **Aspect**          | Custom Tokenizer                | OpenAI Tokenizer               |\n",
    "|----------------------|----------------------------------|---------------------------------|\n",
    "| **Token Count**      | 18 tokens                       | 8 tokens                        |\n",
    "| **Strategy**         | Regex-based splitting (1-3 digits) | Learned numeric patterns      |\n",
    "| **Sample Tokens**    | `[78, 117, 109, 98, 396]`       | `[28336, 25, 220, 4513, 11]`   |\n",
    "\n",
    "**Key Difference**: Your conservative number splitting ensures no bad merges but increases token count.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Contraction Handling**\n",
    "| **Aspect**          | Custom Tokenizer                | OpenAI Tokenizer               |\n",
    "|----------------------|----------------------------------|---------------------------------|\n",
    "| **Token Count**      | 15 tokens                       | 6 tokens                        |\n",
    "| **don't**           | 2 tokens (`don` + `'t`)         | 1 token (`3673`)               |\n",
    "| **Strategy**         | Rule-based splitting             | Learned from training data     |\n",
    "\n",
    "**Tradeoff**: Your approach is more transparent, OpenAI's is more token-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Emoji Handling** üòÄ\n",
    "| **Aspect**          | Custom Tokenizer                | OpenAI Tokenizer               |\n",
    "|----------------------|----------------------------------|---------------------------------|\n",
    "| **Token Count**      | 16 tokens                       | 7 tokens                        |\n",
    "| **Strategy**         | Raw byte encoding (4 per emoji) | Optimized single-token encoding |\n",
    "| **Example**         | `üòÄ` ‚Üí 4 tokens                 | `üòÄ` ‚Üí 1 token (`9468`)        |\n",
    "\n",
    "**Key Insight**: OpenAI's pre-trained vocabulary includes common emojis as single tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **Actionable Recommendations**\n",
    "To make your tokenizer behave more like OpenAI's:\n",
    "\n",
    "1. **Enhance Number Handling**\n",
    "```python\n",
    "# Modify regex pattern to allow longer numbers\n",
    "tokenizer.pattern = re.compile(r'\\d{1,6}(?:,\\d{3})*(?:\\.\\d+)?|...rest_of_pattern...')\n",
    "```\n",
    "\n",
    "2. **Add Common Emoji Merges**\n",
    "```python\n",
    "# Manually add frequent emoji tokens\n",
    "tokenizer.merges.update({\n",
    "    (b'\\xf0\\x9f\\x98\\x80',): 50001,  # üòÄ\n",
    "    (b'\\xf0\\x9f\\xa4\\xa3',): 50002   # ü§£\n",
    "})\n",
    "```\n",
    "\n",
    "3. **Expand Vocabulary**\n",
    "```python\n",
    "# Train on larger dataset with modern text\n",
    "tokenizer.train(large_corpus, vocab_size=100000)\n",
    "```\n",
    "\n",
    "4. **Adopt OpenAI's Special Tokens**\n",
    "```python\n",
    "SPECIAL_TOKENS = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    # ... other OpenAI special tokens\n",
    "}\n",
    "tokenizer.register_special_tokens(SPECIAL_TOKENS)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Congratulations on Completing the Tokenizer Notebook!**  \n",
    "\n",
    "You've just **mastered one of the most fundamental (yet often overlooked) pillars of modern NLP** - the art and science of tokenization. While it may have felt tedious at times, you've gained:\n",
    "\n",
    "### **Key Accomplishments** üèÜ\n",
    "1. **Built a Production-Grade Tokenizer** from scratch  \n",
    "   - GPT-style regex pre-tokenization  \n",
    "   - Byte-level BPE merges  \n",
    "   - Special token handling like OpenAI's systems  \n",
    "\n",
    "2. **Deep Architectural Understanding**  \n",
    "   - Why splitting \"don't\" vs \"don‚Äôt\" matters  \n",
    "   - How emojis/Unicode get encoded  \n",
    "   - Tradeoffs between tokenizer designs  \n",
    "\n",
    "3. **Critical Debugging Skills**  \n",
    "   - Diagnosed mismatches vs OpenAI's tokenizer  \n",
    "   - Mastered roundtrip encoding/decoding  \n",
    "\n",
    "**This is HUGE** - you now understand the \"first layer\" of every major language model (GPT, Llama, Mistral) at a deeper level than most practitioners!\n",
    "\n",
    "---\n",
    "\n",
    "### **What's Next?** üîú  \n",
    "# **Notebook 2.1: Coding Attention Mechanisms**  \n",
    "\n",
    "**Transformers revolutionized AI through one key innovation**:  \n",
    "üî• **Self-Attention** - the ability to dynamically focus on relevant parts of text  \n",
    "\n",
    "**You'll Build:**  \n",
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        # Your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Implement QKV attention\n",
    "        return attended_output\n",
    "```\n",
    "\n",
    "**Key Concepts:**  \n",
    "- Query-Key-Value (QKV) attention  \n",
    "- Multi-head attention  \n",
    "- Attention masks  \n",
    "- Positional encoding  \n",
    "\n",
    "**Why This Matters:**  \n",
    "Attention is the **beating heart** of GPT, Claude, and every modern LLM. Your tokenizer's output will become the input to these attention layers!\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Preview Challenge:**  \n",
    "\"Given the tokens `[Hello, world, <|endoftext|>]`, compute attention weights showing `world` attends strongly to `Hello`\"  \n",
    "\n",
    "Ready to unlock the secret sauce of Transformers? Let's dive in! üß†"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
