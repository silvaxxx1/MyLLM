{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we can quantize ? \n",
    "1- the wieghts\n",
    "2- the activation \n",
    "\n",
    "after quantization we get : \n",
    "- samller models  \n",
    "- speed gain \n",
    "- faster operation \n",
    "\n",
    "quantization after the training called POST-TRAINING QUANTIZATION \n",
    "\n",
    "\n",
    "## linear quantization is a popluar method of quantization \n",
    "\n",
    "in linear quantization we map high presision value to lower\n",
    "like fp32 to int 8 \n",
    "\n",
    "Formula : \n",
    "\n",
    "r = s(q - z)\n",
    "\n",
    "where : \n",
    "s : scale (as the original tensor)\n",
    "z : zero point (the quantized tensor)\n",
    "\n",
    "getting q : \n",
    "\n",
    "form the original formaula above \n",
    "\n",
    "r/s = q - z \n",
    "\n",
    "q = r/s + z \n",
    "\n",
    "q = round(r/s + z)\n",
    "q = int (round(r/s + z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_Q(\n",
    "        tensor,\n",
    "        scale,\n",
    "        zero,\n",
    "        dtype = torch.int8\n",
    "):\n",
    "    \n",
    "    scaled_shifted_tensor = tensor / scale + zero \n",
    "    rounded = torch.round(scaled_shifted_tensor)\n",
    "\n",
    "    q_min = torch.iinfo(dtype).min \n",
    "    q_max = torch.iinfo(dtype).max \n",
    "\n",
    "    q_tensor = rounded.clamp(q_min, q_max).to(dtype)\n",
    "\n",
    "    return q_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.tensor(\n",
    "    [[191.5 , -13.5, 728.6],\n",
    "     [92.14, 295.5, -184],\n",
    "     [0 , 684.6 , 245.5]\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -15,  -74,  127],\n",
      "        [ -44,   14, -123],\n",
      "        [ -70,  126,    0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "scale = 3.5 \n",
    "zero = -70 \n",
    "\n",
    "quantized_tensor = linear_Q(test_tensor,scale,zero)\n",
    "\n",
    "print(quantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 192.5000,  -14.0000,  689.5000],\n",
       "        [  91.0000,  294.0000, -185.5000],\n",
       "        [   0.0000,  686.0000,  245.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequant_tensor = scale * (quantized_tensor.float() - zero)\n",
    "dequant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 192.5000,  -14.0000, -206.5000],\n",
       "        [  91.0000,  294.0000, -185.5000],\n",
       "        [   0.0000, -210.0000,  245.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequant_tensor = scale * (quantized_tensor- zero)\n",
    "dequant_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_DQ(quantized_tensor, scale, zero):\n",
    "    return scale * (quantized_tensor- zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting helper function for quantization error \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear quantization mao [rmin , rmax] to the [qmin , qmax]\n",
    "\n",
    "rmin = s(qmin - z)\n",
    "rmax = s(qmax - z)\n",
    "\n",
    "so for s : \n",
    "\n",
    "s = (rmax -rmin) / (qmax -qmin)\n",
    "\n",
    "and for z: \n",
    "\n",
    "z = int(round(qmin - rmin / s) --> same data type as quantized tensor\n",
    "\n",
    "\n",
    "for the prevuos example find s and z ? \n",
    "\n",
    "\n",
    "handling the edge cases for z \n",
    "what happen if tyhe zero value is out of range \n",
    "\n",
    "if z > qmax >> z = qmax \n",
    "if z < qmin >> z = qmin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_and_zero_values(\n",
    "        r_tensor,\n",
    "        dtype = torch.int8\n",
    "):\n",
    "    q_max , q_min = torch.iinfo(dtype).max , torch.iinfo(dtype).min \n",
    "    r_min , r_max  = r_tensor.min().item() , r_tensor.max().item()\n",
    "\n",
    "    scale = (r_max - r_min) / (q_max - q_min) \n",
    "    z = q_min - (r_min/scale)\n",
    "\n",
    "    if q_min <=z <= q_max :\n",
    "        z = int(round(z))\n",
    "    elif z < q_min : \n",
    "        z = q_min \n",
    "    elif z < q_max : \n",
    "        z = q_max\n",
    "\n",
    "    return scale , z \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.578823433670343\n",
      "-77\n"
     ]
    }
   ],
   "source": [
    "new_scale , new_zero = get_scale_and_zero_values(test_tensor, torch.int8)\n",
    "\n",
    "print(new_scale)\n",
    "print(new_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor = linear_Q(test_tensor, new_scale, new_zero)\n",
    "dequant_tensor = linear_DQ (quantized_tensor, new_scale, new_zero) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -23,  -81,  127],\n",
      "        [ -51,    6, -128],\n",
      "        [ -77,  114,   -8]], dtype=torch.int8)\n",
      "tensor([[ 193.2565,  -14.3153, -186.0988],\n",
      "        [  93.0494,  297.0423, -182.5200],\n",
      "        [   0.0000, -232.6235,  246.9388]])\n"
     ]
    }
   ],
   "source": [
    "print(quantized_tensor)\n",
    "print(dequant_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the error again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a general linear quantazation funcion:\n",
    "def linear_quantizer(tensor,dtype = torch.int8):\n",
    "    scale , zero_point = get_scale_and_zero_values(tensor, dtype=dtype)\n",
    "    quantized_tensor = linear_Q(tensor, scale , zero_point, dtype=dtype)\n",
    "    return quantized_tensor , scale , zero_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0478,  1.7570, -0.7924, -1.0906],\n",
       "        [-0.4930,  0.0235, -0.6162,  0.5439],\n",
       "        [-0.5460,  0.5273,  0.8849, -0.6643],\n",
       "        [-0.2852, -1.2923,  0.5774, -0.5834]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_tensor = torch.randn((4,4))\n",
    "r_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor , scale , zero = linear_quantizer(r_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -24,  127,  -86, -111],\n",
       "        [ -61,  -18,  -72,   25],\n",
       "        [ -66,   24,   54,  -76],\n",
       "        [ -44, -128,   28,  -69]], dtype=torch.int8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011958020808649997"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SEMMETRICS VS ASYMMERTICS MODE : \n",
    "\n",
    "def symmetric_q_scale(tensor , dtype = torch.int8):\n",
    "    r_max = tensor.abs().max().item() \n",
    "    q_max = torch.iinfo(dtype).max \n",
    "    return r_max / q_max \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5755, -0.9131,  1.6683, -0.3982],\n",
       "        [ 1.0982,  0.8709, -0.1981,  0.4716],\n",
       "        [ 0.6617, -0.8226, -0.4381, -0.0852],\n",
       "        [ 0.5221, -1.0901,  2.5888,  1.7138]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.randn((4,4))\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020384512548371564\n"
     ]
    }
   ],
   "source": [
    "print(symmetric_q_scale(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_sym(tensor , dtype = torch.int8):\n",
    "    scale = symmetric_q_scale(tensor)\n",
    "    quantized_tensor = linear_Q(tensor, scale,zero=0,dtype=dtype)\n",
    "    return quantized_tensor , scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28, -45,  82, -20],\n",
       "        [ 54,  43, -10,  23],\n",
       "        [ 32, -40, -21,  -4],\n",
       "        [ 26, -53, 127,  84]], dtype=torch.int8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor , scale = linear_q_sym(test_tensor)\n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def quantization_error():\n",
    "## the trade off and the usew cases of the two types of quantization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when using asymm qi=uantization the q range is fully utilized \n",
    "- in symm if the float points is bias towrd one side  (like relu activation) the quantized range will be also bias toward one side \n",
    "- symm is more simplier \n",
    "- also more memeory effience since we dont store the zero value (z = 0)\n",
    "\n",
    "in practice : \n",
    "- symm for 8 bit \n",
    "- for lower bit like 2 and 4 use asymm \n",
    "\n",
    "\n",
    "Type of quantization methos : \n",
    "- per tensor quan \n",
    "- per channel quan \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Per Channel Quantization\n",
    "## we need to store the z and scale for every row (if we decide to use the row) or alonmg the coloums \n",
    "## the memory required is low and we use this normally with 8 bit quantization \n",
    "import torch\n",
    "\n",
    "def linear_q_sym_per_channel(tensor,\n",
    "                             dim,\n",
    "                             dtype=torch.int8):\n",
    "    # tensor: Input tensor of shape (N, C, H, W) or any arbitrary shape\n",
    "    # dim: The dimension along which quantization is performed\n",
    "    # dtype: Data type for quantization (default: int8)\n",
    "\n",
    "    output_dim = tensor.shape[dim]  # Size of the selected dimension\n",
    "    scale = torch.zeros(output_dim)  # Shape: (output_dim,)\n",
    "\n",
    "    for index in range(output_dim):\n",
    "        sub_tensor = tensor.select(dim, index)  # Shape: (remaining dimensions after selecting along 'dim')\n",
    "        scale[index] = symmetric_q_scale(sub_tensor, dtype=dtype)  # Compute scale for the sub-tensor\n",
    "\n",
    "        scale_shape = [1] * tensor.dim()  # Create shape list with all ones\n",
    "        scale_shape[dim] = -1  # Set the selected dimension to -1 (broadcasting)\n",
    "        scale = scale.view(scale_shape)  # Reshape scale tensor to match broadcasting shape\n",
    "\n",
    "        quantized_tensor = linear_Q(tensor, scale, zero=0, dtype=dtype)  \n",
    "        # quantized_tensor: Same shape as input tensor\n",
    "\n",
    "        return quantized_tensor, scale  # quantized_tensor shape: same as tensor, scale shape: broadcasted shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.tensor(\n",
    "    [[191.5 , -13.5, 728.6],\n",
    "     [92.14, 295.5, -184],\n",
    "     [0 , 684.6 , 245.5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "dim = 0\n",
    "\n",
    "out_dim = test_tensor.shape[dim]\n",
    "out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = torch.zeros(out_dim)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.7370, 2.3268, 5.3906])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index in range(out_dim):\n",
    "    sub_tensor = test_tensor.select(dim , index)\n",
    "    # print(sub_tensor)\n",
    "    scale[index] = symmetric_q_scale(sub_tensor)\n",
    "    # print(scale[index])\n",
    "\n",
    "scale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_shape = [1] * test_tensor.dim()\n",
    "scale_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_shape[dim] = -1\n",
    "scale_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.7370],\n",
       "        [2.3268],\n",
       "        [5.3906]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = scale.view(scale_shape)\n",
    "scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 33,  -2, 127],\n",
       "        [ 40, 127, -79],\n",
       "        [  0, 127,  46]], dtype=torch.int8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor = linear_Q(test_tensor, scale, zero=0) \n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  33,   -2,  127],\n",
       "        [ 127,  127, -128],\n",
       "        [   0,  127,  127]], dtype=torch.int8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor_0 , scale_0 = linear_q_sym_per_channel(test_tensor, dim=0)\n",
    "quantized_tensor_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 127, -128,  127],\n",
       "        [  61,  127, -128],\n",
       "        [   0,  127,  127]], dtype=torch.int8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor_1, scale_1 = linear_q_sym_per_channel(test_tensor, dim=1)\n",
    "quantized_tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "\n",
    "\n",
    "def quantization_error(tensor, dequantized_tensor):\n",
    "    return (dequantized_tensor - tensor).abs().square().mean()\n",
    "\n",
    "\n",
    "def linear_q_with_scale_and_zero_point(\n",
    "    r_tensor, scale, zero_point, dtype=torch.int8):\n",
    "    \"\"\"\n",
    "    Performs simple linear quantization given\n",
    "    the scale and zero-point.\n",
    "    \"\"\"\n",
    "\n",
    "    # scale tensor and add the zero point\n",
    "    scaled_and_shifted_tensor = r_tensor / scale + zero_point\n",
    "\n",
    "    # round the tensor \n",
    "    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n",
    "\n",
    "    # we need to clamp to the min/max value of the specified dtype\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    q_tensor = rounded_tensor.clamp(q_min, q_max).to(dtype)\n",
    "    return q_tensor\n",
    "\n",
    "\n",
    "def linear_dequantization(quantized_tensor, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Linear de-quantization\n",
    "    \"\"\"\n",
    "    dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n",
    "\n",
    "    return dequantized_tensor\n",
    "\n",
    "def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of tensors using seaborn\n",
    "    \"\"\"\n",
    "    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "def plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype = torch.int8, n_bits = 8):\n",
    "    \"\"\"\n",
    "    A method that plots 4 matrices, the original tensor, the quantized tensor\n",
    "    the de-quantized tensor and the error tensor.\n",
    "    \"\"\"\n",
    "    # Get a figure of 4 plots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "\n",
    "    # Plot the first matrix\n",
    "    plot_matrix(original_tensor, axes[0], 'Original Tensor', cmap=ListedColormap(['white']))\n",
    "\n",
    "    # Get the quantization range and plot the quantized tensor\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    plot_matrix(quantized_tensor, axes[1], f'{n_bits}-bit Linear Quantized Tensor', vmin=q_min, vmax=q_max, cmap='coolwarm')\n",
    "\n",
    "    # Plot the de-quantized tensors\n",
    "    plot_matrix(dequantized_tensor, axes[2], 'Dequantized Tensor', cmap='coolwarm')\n",
    "\n",
    "    # Get the quantization errors\n",
    "    q_error_tensor = abs(original_tensor - dequantized_tensor)\n",
    "    plot_matrix(q_error_tensor, axes[3], 'Quantization Error Tensor', cmap=ListedColormap(['white']))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_q_scale_and_zero_point(r_tensor, dtype=torch.int8):\n",
    "    \"\"\"\n",
    "    Get quantization parameters (scale, zero point) \n",
    "    for a floating point tensor\n",
    "    \"\"\"\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    r_min, r_max = r_tensor.min().item(), r_tensor.max().item()\n",
    "\n",
    "    scale = (r_max-r_min)/(q_max-q_min)\n",
    "\n",
    "    zero_point = q_min-(r_min/scale)\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < q_min or zero_point > q_max:\n",
    "        zero_point = q_min\n",
    "    else:\n",
    "        # round and cast to int\n",
    "        zero_point = int(round(zero_point))\n",
    "    return scale, zero_point\n",
    "\n",
    "def linear_quantization(r_tensor, n_bits, dtype=torch.int8):\n",
    "    \"\"\"\n",
    "    linear quantization\n",
    "    \"\"\"\n",
    "\n",
    "    scale, zero_point = get_q_scale_and_zero_point(r_tensor)\n",
    "    \n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(\n",
    "        r_tensor, scale=scale, zero_point=zero_point, dtype=dtype)\n",
    "\n",
    "    return quantized_tensor, scale, zero_point\n",
    "\n",
    "\n",
    "############# From the previous lesson(s) of \"Linear Quantization II\"\n",
    "def linear_q_symmetric_per_channel(r_tensor, dim, dtype=torch.int8):\n",
    "    \n",
    "    output_dim = r_tensor.shape[dim]\n",
    "    # store the scales\n",
    "    scale = torch.zeros(output_dim)\n",
    "\n",
    "    for index in range(output_dim):\n",
    "        sub_tensor = r_tensor.select(dim, index)\n",
    "        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n",
    "\n",
    "    # reshape the scale\n",
    "    scale_shape = [1] * r_tensor.dim()\n",
    "    scale_shape[dim] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(\n",
    "        r_tensor, scale=scale, zero_point=0, dtype=dtype)\n",
    "   \n",
    "    return quantized_tensor, scale\n",
    "\n",
    "\n",
    "def get_q_scale_symmetric(tensor, dtype=torch.int8):\n",
    "    r_max = tensor.abs().max().item()\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "\n",
    "    # return the scale\n",
    "    return r_max/q_max\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the error for the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_symmetric_per_group(tensor, group_size,\n",
    "                                 dtype=torch.int8):\n",
    "    \n",
    "    t_shape = tensor.shape\n",
    "    assert t_shape[1] % group_size == 0\n",
    "    assert tensor.dim() == 2\n",
    "    \n",
    "    tensor = tensor.view(-1, group_size)\n",
    "    \n",
    "    quantized_tensor, scale = linear_q_symmetric_per_channel(\n",
    "                                tensor, dim=0, dtype=dtype)\n",
    "    \n",
    "    quantized_tensor = quantized_tensor.view(t_shape)\n",
    "    \n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dequantization_per_group(quantized_tensor, scale, \n",
    "                                    group_size):\n",
    "    \n",
    "    q_shape = quantized_tensor.shape\n",
    "    quantized_tensor = quantized_tensor.view(-1, group_size)\n",
    "    \n",
    "    dequantized_tensor = linear_dequantization(quantized_tensor, \n",
    "                                               scale, 0)\n",
    "    \n",
    "    dequantized_tensor = dequantized_tensor.view(q_shape)\n",
    "    \n",
    "    return dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9585, 0.3314, 0.5001, 0.3210, 0.7089, 0.4766],\n",
       "        [0.9081, 0.2450, 0.5245, 0.2804, 0.9393, 0.9100],\n",
       "        [0.0133, 0.1854, 0.5191, 0.5546, 0.5556, 0.5216],\n",
       "        [0.1969, 0.8969, 0.6583, 0.8039, 0.6270, 0.2543],\n",
       "        [0.8023, 0.1053, 0.9845, 0.9394, 0.3305, 0.9571],\n",
       "        [0.8021, 0.1984, 0.8703, 0.3725, 0.7622, 0.9905]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor =torch.rand((6,6))\n",
    "group_size = 3 \n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor , scale = linear_q_symmetric_per_group(random_tensor, group_size)\n",
    "dequant_tensor  = linear_dequantization_per_group(quantized_tensor, scale, group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[127,  44,  66,  57, 127,  85],\n",
       "        [127,  34,  73,  38, 127, 123],\n",
       "        [  3,  45, 127, 127, 127, 119],\n",
       "        [ 28, 127,  93, 127,  99,  40],\n",
       "        [103,  14, 127, 125,  44, 127],\n",
       "        [117,  29, 127,  48,  98, 127]], dtype=torch.int8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9585, 0.3321, 0.4981, 0.3182, 0.7089, 0.4745],\n",
       "        [0.9081, 0.2431, 0.5220, 0.2810, 0.9393, 0.9097],\n",
       "        [0.0123, 0.1839, 0.5191, 0.5556, 0.5556, 0.5206],\n",
       "        [0.1977, 0.8969, 0.6568, 0.8039, 0.6267, 0.2532],\n",
       "        [0.7984, 0.1085, 0.9845, 0.9420, 0.3316, 0.9571],\n",
       "        [0.8018, 0.1987, 0.8703, 0.3743, 0.7643, 0.9905]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequant_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_tensor, scale = linear_q_symmetric_per_group(\n",
    "    test_tensor, group_size=group_size)\n",
    "\n",
    "dequantized_tensor = linear_dequantization_per_group(\n",
    "    quantized_tensor, scale, group_size=group_size)\n",
    "\n",
    "plot_quantization_errors(\n",
    "    test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization Error : 1.7589133977890015\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Quantization Error : \\\n",
    "{quantization_error(test_tensor, dequantized_tensor)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## coding a custom 8bit quantizer :\n",
    "## this quantizer is model agnostic meaning it can be used for any model langauge , vision , or multimodal\n",
    "## we will use the linear quantizer scheme to quantize the weights of the model \n",
    "\n",
    "## the steps for the project : \n",
    "## creating a W8A16LinearLayer class to store 8 bits weights and scales \n",
    "## replacing the torch linear layer with the W8A16LinearLayer class \n",
    "## creating a function to quantize the weights of the model \n",
    "## testing the implemetaion \n",
    "\n",
    "## W8A16LinearLayer class : \n",
    "## * w8_16_forward function -> weight , input , scale , bias (optional)\n",
    "\n",
    "\n",
    "## some randon tensor for testing\n",
    "\n",
    "random_int8 = torch.randint(-128, 127, (32, 16), dtype=torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0452, -0.2850,  0.2928, -0.1139, -0.0410,  0.2362, -0.2939, -0.2566,\n",
      "          0.1713,  0.2796],\n",
      "        [-0.1370, -0.1102,  0.1843, -0.1955,  0.2612, -0.0980, -0.0192, -0.1338,\n",
      "         -0.2708, -0.1222],\n",
      "        [ 0.2873,  0.1881,  0.2249, -0.2059, -0.1164,  0.0050, -0.0307, -0.2393,\n",
      "         -0.2448, -0.2674],\n",
      "        [-0.1245,  0.2401, -0.0790,  0.2627,  0.0135, -0.1685, -0.1371, -0.3098,\n",
      "          0.0167, -0.2767],\n",
      "        [ 0.2001,  0.1852, -0.1193,  0.2139, -0.3126,  0.1457, -0.2942,  0.0532,\n",
      "          0.0291, -0.0789]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2001,  0.1408, -0.0970,  0.1433,  0.1441], requires_grad=True)]\n",
      "tensor([[ 1.7564e+00,  7.1541e-01,  3.5405e-01, -1.6998e+00,  2.9643e+00,\n",
      "         -5.1511e-01, -2.4580e-01, -8.9450e-01, -1.0912e+00,  3.9150e-01],\n",
      "        [-1.4919e+00, -3.1828e-01,  1.3454e+00,  5.6489e-01, -3.7040e-01,\n",
      "          5.8199e-01, -2.3028e-02,  2.7163e-01,  2.3809e+00, -1.3139e-03],\n",
      "        [-2.6784e+00, -5.2689e-01, -9.5282e-01,  1.6004e+00, -1.3158e+00,\n",
      "         -3.9214e-01,  2.2702e-01, -2.0198e-01,  9.1026e-01, -8.4354e-01],\n",
      "        [ 1.2020e+00,  1.4291e+00,  1.5296e+00,  7.5364e-01,  1.3392e+00,\n",
      "          1.3696e+00,  7.4863e-01,  4.3696e-01, -2.2336e-01, -8.1840e-03],\n",
      "        [ 6.5678e-01, -1.9570e-01,  2.1241e-01,  6.6417e-01, -5.8573e-01,\n",
      "         -1.7925e-01, -2.4133e-01, -1.3782e+00, -8.9132e-01, -2.3864e-01]])\n",
      "tensor([-0.8011, -1.0394,  0.2010,  0.2721, -2.3183])\n"
     ]
    }
   ],
   "source": [
    "## whats is the diffrent between nn.linear and F.linear ? \n",
    "## look at this example :\n",
    "# Using nn.Linear (recommended)\n",
    "\n",
    "linear_layer = nn.Linear(in_features=10, out_features=5)  # Creates and registers parameters\n",
    "input_tensor = torch.randn(20, 10) #batch size 20, input features 10\n",
    "output = linear_layer(input_tensor) #batch size 20, output features 5\n",
    "\n",
    "print(list(linear_layer.parameters())) #prints the weight and bias of the linear layer\n",
    "\n",
    "# Using F.linear\n",
    "w = torch.randn(5, 10)  # You must create the weight tensor\n",
    "b = torch.randn(5)      # And the bias tensor (optional)\n",
    "output_f = F.linear(input_tensor, w, b)\n",
    "\n",
    "print(w) #prints the weight tensor\n",
    "print(b)   #prints the bias tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 196.0000,  264.0000,  264.0000,  214.0000,  135.0000,  -14.0625,\n",
       "          159.0000, -114.5000, -280.0000,  156.0000, -115.0000,  328.0000,\n",
       "           77.0000,  394.0000,  147.0000, -126.0000,   11.3125, -286.0000,\n",
       "         -217.0000,  149.0000, -147.0000,  314.0000,  137.0000,   -7.6250,\n",
       "          264.0000,   75.0000,  109.5000,  251.0000,  180.0000, -536.0000,\n",
       "           91.5000,  -74.5000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using nn.Linear (recommended) and cASTING the weights to bfloat16 (same as the hiden states)\n",
    "F.linear(random_hs, random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -95.0000,   111.0000,   253.0000,   -50.7500,  -220.0000,   -15.6875,\n",
       "           137.0000,   181.0000,    82.5000,  -222.0000,   199.0000,  -102.5000,\n",
       "           -44.7500, -1096.0000,   104.0000,   179.0000,   -13.2500,  -181.0000,\n",
       "            32.5000,   -50.0000,   185.0000,    40.7500,    51.7500,     3.9844,\n",
       "          -194.0000,    57.7500,    73.5000,   -64.5000,   143.0000,   356.0000,\n",
       "           174.0000,    85.0000]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9062e-03, -4.6400e+02,  3.0800e+02, -3.6600e+02, -6.0400e+02,\n",
       "         -6.9500e+01, -3.1200e+02, -5.5250e+01, -6.6800e+02, -1.4800e+02,\n",
       "         -5.5200e+02, -4.2812e+00, -2.6125e+01,  7.3600e+02, -4.1250e+01,\n",
       "          3.7600e+02, -1.7125e+01, -1.3500e+02,  2.0700e+02, -9.8500e+01,\n",
       "          2.1625e+01,  3.2600e+02, -1.0480e+03,  1.6250e+01, -1.9000e+01,\n",
       "          2.9500e+01, -8.6400e+02,  8.0500e+01, -9.7200e+02, -2.3400e+02,\n",
       "          3.7200e+02, -6.5200e+02]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notice that its imporant for the shape of the sacle and bias to be the same as the output of the linear layer so ypou can broadcast the addition correctly\n",
    "\n",
    "## lets combine this into function:\n",
    "def w8_16_forward(weight, input, scale, bias=None):\n",
    "    casted_weight = weight.to(input.dtype) \n",
    "    output = F.linear(input, casted_weight) * scale\n",
    "    if bias is not None:\n",
    "        output += bias\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with bias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9062e-03, -4.6400e+02,  3.0800e+02, -3.6600e+02, -6.0400e+02,\n",
       "         -6.9500e+01, -3.1200e+02, -5.5250e+01, -6.6800e+02, -1.4800e+02,\n",
       "         -5.5200e+02, -4.2812e+00, -2.6125e+01,  7.3600e+02, -4.1250e+01,\n",
       "          3.7600e+02, -1.7125e+01, -1.3500e+02,  2.0700e+02, -9.8500e+01,\n",
       "          2.1625e+01,  3.2600e+02, -1.0480e+03,  1.6250e+01, -1.9000e+01,\n",
       "          2.9500e+01, -8.6400e+02,  8.0500e+01, -9.7200e+02, -2.3400e+02,\n",
       "          3.7200e+02, -6.5200e+02]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"with bias:\")\n",
    "w8_16_forward(random_int8, random_hs, scales, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without bias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0078e-01, -4.6400e+02,  3.0800e+02, -3.6600e+02, -6.0400e+02,\n",
       "         -6.9000e+01, -3.1200e+02, -5.3750e+01, -6.6800e+02, -1.4900e+02,\n",
       "         -5.5200e+02, -3.7344e+00, -2.5500e+01,  7.3600e+02, -4.0750e+01,\n",
       "          3.7800e+02, -1.6625e+01, -1.3500e+02,  2.0600e+02, -9.7500e+01,\n",
       "          2.1750e+01,  3.2400e+02, -1.0480e+03,  1.6250e+01, -1.8750e+01,\n",
       "          2.9500e+01, -8.6400e+02,  8.0000e+01, -9.7200e+02, -2.3300e+02,\n",
       "          3.7200e+02, -6.5200e+02]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"without bias:\")\n",
    "w8_16_forward(random_int8, random_hs, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next lets consider the process \n",
    "\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        ## this method is wrong but it shows the idea\n",
    "        self.int8_weight = nn.Parameter(torch.randint(-128, 127, (out_features, in_features)).to(dtype=torch.int8))\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try ti init the class and see what will happen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mW8A16LinearLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m, in \u001b[0;36mW8A16LinearLayer.__init__\u001b[0;34m(self, in_features, out_features, bias, dtype)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features, out_features, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint8_weight \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m127\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/parameter.py:46\u001b[0m, in \u001b[0;36mParameter.__new__\u001b[0;34m(cls, data, requires_grad)\u001b[0m\n\u001b[1;32m     42\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01mis\u001b[39;00m Parameter:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# For ease of BC maintenance, keep this path for standard Tensor.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Eventually (tm), we should change the behavior for standard Tensor to match.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m t \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "layer = W8A16LinearLayer(16, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see you get A RuntimeError , that because pytorch nn.Parameter automatically assign the tensor to the computaional graph and try to get the gradient , but there is no gradient for int8 in oytorch yet! \n",
    "\n",
    "so what we can do ? \n",
    "\n",
    "see the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weight\",\n",
    "                            torch.randint(-128, 127, (out_features, in_features)).to(dtype=torch.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = W8A16LinearLayer(16, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error ! , the method register_buffer just save a buffer with no gradient\n",
    "\n",
    "lets continue : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weight\",\n",
    "                            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.randn((out_features)\n",
    "                             , dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros((1, out_features)\n",
    "                                 , dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "dummy_instance = W8A16LinearLayer(16, 32)\n",
    "print(dummy_instance.int8_weight.shape)\n",
    "print( dummy_instance.scale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good now we need the forward pass , which we alredy made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weight\",\n",
    "                            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.randn((out_features)\n",
    "                             , dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros((1, out_features)\n",
    "                                 , dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return w8_16_forward(self.int8_weight, x, self.scale, self.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_instance = W8A16LinearLayer(16, 32)\n",
    "dummy_hs = torch.randn(1, 6, 16)\n",
    "\n",
    "dummy_instance(dummy_hs).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make quantize and dequantize method , plus the workflow to quantize a model should as follow : \n",
    "\n",
    "we detect all the linear layer on the model \n",
    "\n",
    "we loop over thejm and replace the standard nn.linear with the custom made class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"int8_weight\",\n",
    "                            torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.randn((out_features)\n",
    "                             , dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros((1, out_features)\n",
    "                                 , dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weight):\n",
    "        ## cast to fp32 is recommended for stability\n",
    "        w_fp32 = weight.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127 \n",
    "        scales = scales.to(weight.dtype) \n",
    "\n",
    "        int8_weight = torch.round(w_fp32 / scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weight = int8_weight\n",
    "        self.scale = scales\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_16_forward(self.int8_weight, input, self.scale, self.bias)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before quantization:0 tensor([[ -57,   12,   31,   80],\n",
      "        [  85,   11,   70,  -29],\n",
      "        [ -17,    6,   60,  -14],\n",
      "        [ -40,   -6, -121, -108],\n",
      "        [ -24,   47,  -68,  -79],\n",
      "        [ -22,  -10,  119,   -2],\n",
      "        [ -23,  -67,  -64, -121],\n",
      "        [-110,  -27,  -18,   31]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "dummy_instance = W8A16LinearLayer(4 , 8)\n",
    "\n",
    "print(\"before quantization:0\", dummy_instance.int8_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after quantization:0 tensor([[ -41,  -46,   39,   -4,  -40,  127,  -49,   41],\n",
      "        [ -10,   22,  127,   18, -102,  -29,   36,  -61],\n",
      "        [  21,   14,  -50,    1,  -72,  -68,   70, -127],\n",
      "        [  32,   -6,  -42,   18,  -61,   52,  127,   57]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\n",
    "\n",
    "dummy_instance.quantize(random_matrix)\n",
    "print(\"after quantization:0\", dummy_instance.int8_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_instance.scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_instance.int8_weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the quantization_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0036, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_error = (random_matrix - dummy_instance.int8_weight * dummy_instance.scale.unsqueeze(1)).abs().mean()\n",
    "quantization_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ready to make Quantizer (a quantization pipline) which will iterate over the original linear model and replace them with the int8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_module(module,\n",
    "                target_class, \n",
    "                module_name_to_exclude):\n",
    "    \n",
    "    for name, child in module.named_children():\n",
    "\n",
    "        # if the layer is a linear layer and not in the exclusion list\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            new_module = target_class(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                child.bias is not None,\n",
    "                child.weight.dtype\n",
    "            )\n",
    "            setattr(module, name, new_module)\n",
    "            if old_bias is not None:\n",
    "                getattr(module , name).bias = old_bias \n",
    "        \n",
    "        else : \n",
    "            # recursively call the function\n",
    "            replace_module(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LETS MAKE A DUMMY MODEL (LIKE MINI GPT)\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Embedding(16, 32)\n",
    "        # with bias\n",
    "        self.l2 = nn.Linear(32, 64)\n",
    "        # without bias\n",
    "        self.l3 = nn.Linear(64, 128, bias=False)\n",
    "        self.l4 = nn.Linear(128, 256, bias=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model: DummyModel(\n",
      "  (l1): Embedding(16, 32)\n",
      "  (l2): W8A16LinearLayer()\n",
      "  (l3): W8A16LinearLayer()\n",
      "  (l4): Linear(in_features=128, out_features=256, bias=False)\n",
      ")\n",
      "unquantized model: DummyModel(\n",
      "  (l1): Embedding(16, 32)\n",
      "  (l2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=128, bias=False)\n",
      "  (l4): Linear(in_features=128, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_1 = DummyModel()\n",
    "model_2 = DummyModel()\n",
    "\n",
    "replace_module(model_1, W8A16LinearLayer, [\"l4\"])\n",
    "\n",
    "print(\"quantized model:\",model_1)\n",
    "\n",
    "print (\"unquantized model:\",model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_module_and_quantize(module,\n",
    "                target_class, \n",
    "                module_name_to_exclude):\n",
    "    \n",
    "    for name, child in module.named_children():\n",
    "\n",
    "        # if the layer is a linear layer and not in the exclusion list\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(\n",
    "                child.in_features,\n",
    "                child.out_features,\n",
    "                child.bias is not None,\n",
    "                child.weight.dtype\n",
    "            )\n",
    "            setattr(module, name, new_module)\n",
    "            getattr(module , name).quantize(old_weight)\n",
    "\n",
    "            if old_bias is not None:\n",
    "                getattr(module , name).bias = old_bias \n",
    "        \n",
    "        else : \n",
    "            # recursively call the function\n",
    "            replace_module_and_quantize(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model: DummyModel(\n",
      "  (l1): Embedding(16, 32)\n",
      "  (l2): W8A16LinearLayer()\n",
      "  (l3): W8A16LinearLayer()\n",
      "  (l4): Linear(in_features=128, out_features=256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_3 = DummyModel()\n",
    "\n",
    "replace_module_and_quantize(model_3, W8A16LinearLayer, [\"l4\"])\n",
    "\n",
    "print(\"quantized model:\",model_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the implentaion on a real model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 21:11:40.713174: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from UTILS.load_weights import download_and_load_gpt2 , load_weights_into_gpt\n",
    "from UTILS.model import GPTModel\n",
    "\n",
    "## Choose the model to use\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "## Base configuration settings for the model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Size of the vocabulary used by the model\n",
    "    \"context_length\": 1024,  # Maximum context length the model can handle\n",
    "    \"drop_rate\": 0.0,        # Dropout rate for regularization\n",
    "    \"qkv_bias\": True         # Whether to use bias terms in query, key, and value projections\n",
    "}\n",
    "\n",
    "## Dictionary containing configurations for different GPT-2 model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},      # Config for small model\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},    # Config for medium model\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},     # Config for large model\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},       # Config for extra-large model\n",
    "}\n",
    "\n",
    "## Update the BASE_CONFIG with parameters specific to the chosen model\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized model: GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): W8A16LinearLayer()\n",
      "        (W_key): W8A16LinearLayer()\n",
      "        (W_value): W8A16LinearLayer()\n",
      "        (out_proj): W8A16LinearLayer()\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): W8A16LinearLayer()\n",
      "          (1): GELU()\n",
      "          (2): W8A16LinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_module_and_quantize(model, W8A16LinearLayer, [\"out_head\"]) \n",
    "\n",
    "print(\"quantized model:\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def top_p_logits(logits, p=0.5):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    sorted_probs[cumulative_probs > p] = 0\n",
    "    filtered_logits = torch.zeros_like(logits).to(logits.device)  # Move to same device\n",
    "    filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_probs)\n",
    "    return filtered_logits\n",
    "\n",
    "def generate(model, prompt, max_new_tokens, context_size, tokenizer, temperature=0.0, top_k=None, top_p=None, eos=None):\n",
    "    # Detect device\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "\n",
    "    # Encode and move input to the correct device\n",
    "    idx = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    idx_gen = idx.clone()  # Start with the prompt indices\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx_gen[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)  # Forward pass\n",
    "            logits = logits[:, -1, :]  # Take the last token's logits\n",
    "            \n",
    "            # Apply top-k sampling\n",
    "            if top_k is not None:\n",
    "                top_k_values, _ = torch.topk(logits, k=top_k)\n",
    "                min_value = top_k_values[:, -1].unsqueeze(1)  \n",
    "                logits = torch.where(logits < min_value, torch.tensor(float('-inf')).to(device), logits)\n",
    "\n",
    "            # Apply top-p sampling\n",
    "            if top_p is not None:\n",
    "                logits = top_p_logits(logits, p=top_p)  \n",
    "\n",
    "            # Apply temperature\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)  # Sample token\n",
    "            else: \n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # Take max logit\n",
    "\n",
    "            # EOS handling\n",
    "            if eos is not None and torch.equal(idx_next, torch.tensor(eos).to(device)):\n",
    "                break\n",
    "            \n",
    "            # Append new token\n",
    "            idx_gen = torch.cat((idx_gen, idx_next), dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx_gen.squeeze(0).tolist())  # Convert tokens back to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
