{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook 6.2: Understanding KV Cache for Efficient Transformer Inference üöÄ**  \n",
    "\n",
    "## **Introduction üìö**  \n",
    "\n",
    "Welcome to **Notebook 6.2**, where we dive deep into **KV (Key-Value) Caching**, a crucial optimization technique for making transformer-based models more efficient during inference. üéâ\n",
    "\n",
    "In **Notebook 6.1**, we explored **decoding strategies** like greedy search, beam search, and top-k sampling while running inference on a transformer model. However, we noticed a key challenge: **as sequence length increases, inference slows down significantly** due to repeated attention computations.  \n",
    "\n",
    "This is where **KV Caching** comes to the rescue! üöÄ Instead of recomputing attention keys and values for every token in the sequence, KV caching **stores and reuses** previously computed states‚Äîleading to **massive speedups** in autoregressive decoding (like in GPT models).  \n",
    "\n",
    "![Decoding Strategies Overview](images/kv.jpg)  \n",
    "\n",
    "\n",
    "### **What‚Äôs Inside? üîç**  \n",
    "\n",
    "1Ô∏è‚É£ **Reviewing Standard Inference (from Notebook 6.1) ‚è≥**  \n",
    "   - A quick recap of how transformers generate text **without KV caching**.  \n",
    "   - Understanding why inference **becomes slower** as the sequence grows.  \n",
    "\n",
    "2Ô∏è‚É£ **How KV Caching Works: Storing and Reusing Attention States üì¶**  \n",
    "   - We'll break down **how transformers compute attention** and **where KV caching fits in**.  \n",
    "   - You'll see how **storing past keys and values** helps speed up token generation.  \n",
    "\n",
    "3Ô∏è‚É£ **Implementing KV Cache in a Transformer Decoder ‚ö°**  \n",
    "   - We'll modify our model to **store past keys & values** in a cache.  \n",
    "   - Instead of recomputing everything, the model will **only process new tokens** efficiently.  \n",
    "\n",
    "4Ô∏è‚É£ **Slicing and Updating KV Cache: Hands-on Exploration üî¨**  \n",
    "   - Understanding how to **slice, update, and retrieve** keys/values from the cache.  \n",
    "   - We‚Äôll visualize **tensor slicing** and its role in maintaining an efficient cache.  \n",
    "\n",
    "5Ô∏è‚É£ **Benchmarking Speed: With and Without KV Caching üöÄ**  \n",
    "   - We‚Äôll compare inference speeds **with and without KV caching** to see the real impact.  \n",
    "   - Expect **significant improvements**, especially for long sequences!  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Why This Notebook Matters üí°**  \n",
    "\n",
    "KV caching is one of the most important optimizations for **deploying transformers in real-time applications**. By the end of this notebook, you'll:  \n",
    "\n",
    "‚úÖ Understand **why inference slows down** in transformers without caching.  \n",
    "‚úÖ Learn how **KV caching reduces redundant computations**.  \n",
    "‚úÖ Implement **a transformer with KV caching** step-by-step.  \n",
    "‚úÖ Benchmark and **see massive speed improvements** for text generation.  \n",
    "\n",
    "üöÄ **Let‚Äôs unlock faster inference with KV Cache!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x78c5f7fc7050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "torch.manual_seed(0) # For reproducibility\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recalls that in the previous notebook, we delve into the decoding strategies like greedy search, beam search, and top-k sampling. We also noticed that as the sequence length increases, the inference slows down significantly due to repeated attention computations. This is where KV caching comes to the rescue! Instead of recomputing attention keys and values for every token in the sequence, KV caching stores and reuses previously computed states, leading to massive speedups in autoregressive decoding (like in GPT models).üöÄ\n",
    "\n",
    "Before we delve into KV caching, let's quickly review how transformers generate text during inference without caching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self , model_name : str ='gpt2-medium') -> None:\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cpu\").to(self.device)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode(ids)\n",
    "\n",
    "    def get_next_token_prob(self, input_ids: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids=input_ids).logits\n",
    "        logits = logits[0, -1, :]\n",
    "        return logits\n",
    "    \n",
    "class GreedySampler(Sampler):\n",
    "    def __call__(self, prompt, max_new_tokens=10):\n",
    "        predictions = []\n",
    "        result = prompt\n",
    "        # generate until max_len\n",
    "        for i in range(max_new_tokens):\n",
    "            \n",
    "            print(f\"step {i} input: {result}\")\n",
    "            input_ids = self.encode(result)\n",
    "            next_token_probs = self.get_next_token_prob(input_ids=input_ids)\n",
    "            \n",
    "            # choose the token with the highest probability\n",
    "            id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "            # convert to token and add new token to text\n",
    "            result += self.decode(id)\n",
    "            \n",
    "            predictions.append(next_token_probs[id].item())\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a text generation pipeline using a causal language model (like GPT-2). The `Sampler` class handles tokenization, encoding, decoding, and extracting next-token probabilities. The `GreedySampler` class extends `Sampler` to generate text using **greedy decoding**, where at each step, it picks the most likely next token. It continues this process for a specified number of steps (`max_new_tokens`), appending each selected token to the prompt to form a generated sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 23:35:52.260101: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-17 23:35:52.729196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739824553.189824   94028 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739824553.363533   94028 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 23:35:54.731814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 input: Large Language Models are a type of AI model that\n",
      "step 1 input: Large Language Models are a type of AI model that can\n",
      "step 2 input: Large Language Models are a type of AI model that can be\n",
      "step 3 input: Large Language Models are a type of AI model that can be used\n",
      "step 4 input: Large Language Models are a type of AI model that can be used to\n",
      "step 5 input: Large Language Models are a type of AI model that can be used to model\n",
      "step 6 input: Large Language Models are a type of AI model that can be used to model language\n",
      "step 7 input: Large Language Models are a type of AI model that can be used to model language.\n",
      "step 8 input: Large Language Models are a type of AI model that can be used to model language. They\n",
      "step 9 input: Large Language Models are a type of AI model that can be used to model language. They are\n",
      "Large Language Models are a type of AI model that can be used to model language. They are based\n"
     ]
    }
   ],
   "source": [
    "gen = GreedySampler() \n",
    "prompt = \"Large Language Models are a type of AI model that\"\n",
    "print(gen(prompt, max_new_tokens=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ The Power of KV Caching in Efficient Inference  \n",
    "\n",
    "Do you see the problem that KV caching can solve?  \n",
    "\n",
    "As the number of input tokens grows during inference, the computational cost (FLOPs ‚Äì Floating Point Operations) **increases significantly**. This is because each new token requires recomputing attention scores over all previous tokens.  \n",
    "\n",
    "**KV caching** solves this problem by **storing** the hidden representations of previously computed key-value pairs. Instead of recomputing them for every new token, the model reuses cached values‚Äî**reducing redundant computations and speeding up inference!** ‚ö°\n",
    "\n",
    "Now lets take a look at the original attention mechanism in transformers to understand how KV caching fits in.\n",
    "Here we looking at a single head instead of multiple heads for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "embed_size = 768   # Size of the embedding vector for each token\n",
    "block_size = 64    # Maximum sequence length for attention\n",
    "head_size = 64     # Size of each attention head\n",
    "dropout = 0.1      # Dropout rate to prevent overfitting\n",
    "\n",
    "# Define a single attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # Linear layers for key, query, and value transformations\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for causal masking (prevents attention to future tokens)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch size, sequence length, embedding size\n",
    "\n",
    "        # Compute key, query, and value projections\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores (scaled dot product attention)\n",
    "        wei = q @ k.transpose(2, 1) / self.head_size**0.5  # (B, T, T)\n",
    "\n",
    "        # Apply causal masking to prevent attending to future tokens\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=2)  # (B, T, T)\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Compute the final weighted sum of values\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Attention Head & KV Caching  \n",
    "\n",
    "### üîπ Computing Attention Scores  \n",
    "- **Key (K), Query (Q), and Value (V)** are generated via linear layers with shape **(B, T, C)**, where **C** is the head size.  \n",
    "- The **dot product** of **Q** and **K** produces a weight matrix **(B, T, T)**, capturing token relationships‚Äî**higher scores indicate stronger relevance**.  \n",
    "- To prevent excessively large values that hinder optimization, we apply scaling:  \n",
    "\n",
    "  \\[\n",
    "  \\frac{1}{\\sqrt{\\text{head\\_size}}}\n",
    "  \\]\n",
    "\n",
    "  followed by **softmax** to normalize attention weights.  \n",
    "\n",
    "### üîπ Enforcing Causality  \n",
    "- A **lower triangular mask** ensures each token **attends only to previous tokens**, preserving **auto-regressive generation** in models like GPT.  \n",
    "\n",
    "#### üîç Illustration of Auto-Regressive Attention  \n",
    "![Attention Mechanism](images/atten1.gif)  \n",
    "\n",
    "---\n",
    "\n",
    "## üèéÔ∏è Optimizing Inference with KV Caching  \n",
    "\n",
    "### üîπ The Problem: Redundant Computation  \n",
    "In vanilla transformers, every token recomputes **Key-Value (KV) pairs** at every step, leading to inefficiencies in long sequences.  \n",
    "\n",
    "### üîπ KV Caching: The Fix  \n",
    "1Ô∏è‚É£ Instead of processing tokens **incrementally**, we process **one token at a time**. Previously, **all past tokens were needed** to regenerate Value tokens. With KV caching, we store **past K, V pairs**, allowing us to compute attention efficiently.  \n",
    "\n",
    "2Ô∏è‚É£ **Masking is no longer required.** Since we now pass **a single query token**, the attention matrix **(QK·µÄ)** reduces from **(B, T, T)** to **(B, 1, T)**, eliminating redundant masking operations.  \n",
    "\n",
    "KV caching significantly speeds up inference, making transformers more scalable. Let's dive into its implementation! üöÄ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a single attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        # Linear layers for key, query, and value transformations\n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "\n",
    "        # Lower triangular matrix for causal masking (prevents attention to future tokens)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize KV cache (Added initialization)\n",
    "        self.k_cache = None  # (Added initialization)\n",
    "        self.v_cache = None  # (Added initialization)\n",
    "        self.cache_index = 0  # (Added initialization)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch size, sequence length, embedding size\n",
    "\n",
    "        # Compute key, query, and value projections\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "\n",
    "        # Initialize the KV cache if it is None (Added check and initialization)\n",
    "        if self.k_cache is None or self.v_cache is None:\n",
    "            self.k_cache = torch.zeros(B, block_size, self.head_size, device=x.device)  # (Added initialization)\n",
    "            self.v_cache = torch.zeros(B, block_size, self.head_size, device=x.device)  # (Added initialization)\n",
    "            self.cache_index = 0  # (Added initialization)\n",
    "\n",
    "        # Update the KV cache in-place (Added cache update logic)\n",
    "        if self.cache_index + T <= block_size:\n",
    "            # If there is space in the cache, add the new keys and values\n",
    "            self.k_cache[:, self.cache_index:self.cache_index + T, :] = k  # (Cache update)\n",
    "            self.v_cache[:, self.cache_index:self.cache_index + T, :] = v  # (Cache update)\n",
    "        else:\n",
    "            # If the cache is full, shift the old tokens and add new ones\n",
    "            shift = self.cache_index + T - block_size  # (Added shift calculation)\n",
    "            self.k_cache[:, :-shift, :] = self.k_cache[:, shift:, :].clone()  # (Cache shift)\n",
    "            self.v_cache[:, :-shift, :] = self.v_cache[:, shift:, :].clone()  # (Cache shift)\n",
    "            self.k_cache[:, -T:, :] = k  # Place new tokens at the end of the cache\n",
    "            self.v_cache[:, -T:, :] = v  # Place new tokens at the end of the cache\n",
    "\n",
    "        # Update the cache index (Added cache index update)\n",
    "        self.cache_index = min(self.cache_index + T, block_size)\n",
    "\n",
    "        # Compute attention scores using cached keys and values (No change)\n",
    "        wei = q @ self.k_cache.transpose(2, 1) / self.head_size**0.5  # (B, T, block_size)\n",
    "\n",
    "        # Apply causal masking to ensure autoregressive behavior\n",
    "        mask = self.tril[:T, :block_size].to(x.device)  # Shape (T, block_size)\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))  # Broadcast mask across the batch\n",
    "        wei = F.softmax(wei, dim=2)  # Apply softmax across the attention scores\n",
    "\n",
    "        # Apply dropout to attention weights (No change)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Compute the final weighted sum of values (No change)\n",
    "        out = wei @ self.v_cache  # (B, T, head_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Changes\n",
    "\n",
    "The following changes were applied to the original `Head` class implementation to incorporate the KV (Key-Value) cache mechanism for more efficient attention computation:\n",
    "\n",
    "#### 1. **KV Cache Initialization**:\n",
    "   - **Purpose**: The KV cache stores the computed keys and values for efficient reuse during sequential token processing.\n",
    "   - **Change**: I added a check in the `forward` method to initialize the `k_cache` and `v_cache` if they are `None` (i.e., the first time the function is run or the cache hasn't been initialized yet).\n",
    "   ```python\n",
    "   if self.k_cache is None or self.v_cache is None:\n",
    "       self.k_cache = torch.zeros(B, block_size, self.head_size, device=x.device)\n",
    "       self.v_cache = torch.zeros(B, block_size, self.head_size, device=x.device)\n",
    "       self.cache_index = 0\n",
    "   ```\n",
    "   - **Explanation**: This ensures that the cache is only initialized once and stored on the same device as the input tensor `x`.\n",
    "\n",
    "#### 2. **Cache Update Logic**:\n",
    "   - **Purpose**: Efficiently update the KV cache with new keys and values while maintaining the previously stored values.\n",
    "   - **Change**: Added logic to update the `k_cache` and `v_cache` in-place.\n",
    "     - If there is enough space in the cache (`cache_index + T <= block_size`), the new keys and values are directly inserted at the current position in the cache.\n",
    "     - If the cache is full, older values are shifted out, and new values are inserted at the end.\n",
    "   ```python\n",
    "   if self.cache_index + T <= block_size:\n",
    "       self.k_cache[:, self.cache_index:self.cache_index + T, :] = k\n",
    "       self.v_cache[:, self.cache_index:self.cache_index + T, :] = v\n",
    "   else:\n",
    "       shift = self.cache_index + T - block_size\n",
    "       self.k_cache[:, :-shift, :] = self.k_cache[:, shift:, :].clone()\n",
    "       self.v_cache[:, :-shift, :] = self.v_cache[:, shift:, :].clone()\n",
    "       self.k_cache[:, -T:, :] = k\n",
    "       self.v_cache[:, -T:, :] = v\n",
    "   ```\n",
    "   - **Explanation**: This mechanism ensures that only the most recent keys and values are retained in the cache while older values are discarded, making the process memory-efficient.\n",
    "\n",
    "#### 3. **Cache Index Update**:\n",
    "   - **Purpose**: Track the current position in the cache for where to insert new keys and values.\n",
    "   - **Change**: After updating the cache, the `cache_index` is updated to reflect the new position.\n",
    "   ```python\n",
    "   self.cache_index = min(self.cache_index + T, block_size)\n",
    "   ```\n",
    "   - **Explanation**: This ensures that the cache index is incremented and doesn't exceed the maximum block size.\n",
    "\n",
    "And this an illustration of the KV cache mechanism in action:\n",
    "\n",
    "![Attention Mechanism](images/atten.gif)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple illustration of how KV cache works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_cache Before:\n",
      " tensor([[[4., 9., 3.],\n",
      "         [0., 3., 9.],\n",
      "         [7., 3., 7.]]])\n",
      "k_cache After:\n",
      " tensor([[[0., 3., 9.],\n",
      "         [7., 3., 7.],\n",
      "         [7., 3., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "# Example to illustrate how the cache transforms\n",
    "\n",
    "k_cache = torch.zeros(1, 3, 3)\n",
    "v_cache = torch.zeros(1, 3, 3)\n",
    "\n",
    "steps = 3\n",
    "for i in range(steps):\n",
    "    k_cache[:, i, :] = torch.randint(10, (1, 3))\n",
    "print(\"k_cache Before:\\n\", k_cache)\n",
    "\n",
    "shift = 1\n",
    "k_cache[:, :-shift, :] = k_cache[:, shift:, :].clone()\n",
    "v_cache[:, :-shift, :] = v_cache[:, shift:, :].clone()\n",
    "print(\"k_cache After:\\n\", k_cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the heaed class with KV cache mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test scenario\n",
    "batch_size = 2  # Number of sequences in the batch\n",
    "seq_length = 4  # Sequence length (number of tokens)\n",
    "x = torch.randn(batch_size, seq_length, embed_size)  # Random input tensor\n",
    "\n",
    "# Create an instance of the Head class\n",
    "head = Head(head_size=head_size)\n",
    "\n",
    "# Run a forward pass\n",
    "output = head(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is it guys‚Ä¶ \n",
    "\n",
    "That‚Äôs how one would implement KV Cache into the Attention Mechanism. Now let‚Äôs move on to see how fast it makes the inference.\n",
    "\n",
    "## Inference with and without KV-Cache\n",
    "So far we‚Äôve discussed the working and implementation of the KV-Cache optimization technique. Below is the code for our previous GPT implementation updated with KV-Cache. If you are new to my page you don‚Äôt need to go through every line, as the only thing that concerns this article is the Attention Head Class and the generate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from UTILS.load_weights import download_and_load_gpt2 , load_weights_into_gpt\n",
    "from UTILS.model import GPTModel\n",
    "\n",
    "## Choose the model to use\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "## Base configuration settings for the model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Size of the vocabulary used by the model\n",
    "    \"context_length\": 1024,  # Maximum context length the model can handle\n",
    "    \"drop_rate\": 0.0,        # Dropout rate for regularization\n",
    "    \"qkv_bias\": True         # Whether to use bias terms in query, key, and value projections\n",
    "}\n",
    "\n",
    "## Dictionary containing configurations for different GPT-2 model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},      # Config for small model\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},    # Config for medium model\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},     # Config for large model\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},       # Config for extra-large model\n",
    "}\n",
    "\n",
    "## Update the BASE_CONFIG with parameters specific to the chosen model\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets borrow this code from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "\n",
    "def top_p_logits(logits, p=0.5):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    sorted_probs[cumulative_probs > p] = 0\n",
    "    filtered_logits = torch.zeros_like(logits).to(logits.device)  # Move to same device\n",
    "    filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_probs)\n",
    "    return filtered_logits\n",
    "\n",
    "def generate(model, prompt, max_new_tokens, context_size, tokenizer, temperature=0.0, top_k=None, top_p=None, eos=None):\n",
    "    # Detect device\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "\n",
    "    # Encode and move input to the correct device\n",
    "    idx = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    idx_gen = idx.clone()  # Start with the prompt indices\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx_gen[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)  # Forward pass\n",
    "            logits = logits[:, -1, :]  # Take the last token's logits\n",
    "            \n",
    "            # Apply top-k sampling\n",
    "            if top_k is not None:\n",
    "                top_k_values, _ = torch.topk(logits, k=top_k)\n",
    "                min_value = top_k_values[:, -1].unsqueeze(1)  \n",
    "                logits = torch.where(logits < min_value, torch.tensor(float('-inf')).to(device), logits)\n",
    "\n",
    "            # Apply top-p sampling\n",
    "            if top_p is not None:\n",
    "                logits = top_p_logits(logits, p=top_p)  \n",
    "\n",
    "            # Apply temperature\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)  # Sample token\n",
    "            else: \n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # Take max logit\n",
    "\n",
    "            # EOS handling\n",
    "            if eos is not None and torch.equal(idx_next, torch.tensor(eos).to(device)):\n",
    "                break\n",
    "            \n",
    "            # Append new token\n",
    "            idx_gen = torch.cat((idx_gen, idx_next), dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx_gen.squeeze(0).tolist())  # Convert tokens back to text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model without KV cache\n",
    "here im using my laptop with gtx 1650 gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "I have a dream that ____ is going to show up in the Oval Office tomorrow with a massive heart attack and be like, \"What the hell am I going to do?\"\n",
      "\n",
      "AP Bill Clinton.\n",
      "\n",
      "KIDS: The only thing that saved us from Great Depression was ‚Äî the only thing that prevented World War II from happening was ‚Äî\n",
      "\n",
      "CONAN: I just don't think there's any question that you were...\n",
      "\n",
      "KIDS: ‚Äî the person who was able to get the economy going again.\n",
      "\n",
      "CONAN: You know, I think while his public image was somewhat tarnished by his scandals, I think he did some good things during that time. I think he was a good fit for the public service. That wasn't to say that we didn't trust him with that responsibility.\n",
      "\n",
      "KIDS: I think if you look at the people around him, they were OK with the way he ran things.\n",
      "\n",
      "(LAUGHTER)\n",
      "\n",
      "Conan: But, you know, I do think public service is a - you know, is an important and valuable part of life, but people have grown up in a different time when people weren't served well. I think that's such a - a big part of our - a big part of losing seniors: If I'm going to harm somebody, why should I have to do something is because I am a public servant and you're not.\n",
      "\n",
      "KIDS: I think that's why we need to be looking at a whole host of problems. So, Bill, thank you very much for talking with us tonight.\n",
      "\n",
      "CONAN: Bill Clinton is on \"News Trades.\"\n",
      "\n",
      "Now, my guest is David, he has a piece up on \"The Daily Show\" called, \"If I Get Acquainted With The Fool's Own Scheme.\" He's an investigative journalist and took a look at a Canadian scam called \"the paid spin doctors.\" You can find it here. He's also been talking to us about why he's voting for Hillary Clinton. So, we'll talk about that next.\n",
      "\n",
      "This is SCIENCE FRIDAY from NPR News.\n",
      "\n",
      "Copyright ¬© 2012 NPR. All rights reserved. Visit our website terms of use and permissions pages at www.npr.org for further information.\n",
      "\n",
      "NPR transcripts are created on a rush deadline by Verb8tm, Inc., an NPR contractor, and produced using a proprietary transcription process developed with NPR. This text may not be in its final form and may be updated or revised in the future. Accuracy and availability may vary. The authoritative record of NPR's programming is the audio record.<|endoftext|>In this video, I discuss ways to improve the human male's sex drive including, TLC, hot tubs, experience, etc., and that's what I do.\n",
      "\n",
      "I have also been doing 5 sessions in my basement for the past month, and I've been enjoying myself so much that I'll be doing more of the videos for a while. I also do a couple of focused groups (by Erotic Focus) each in my home and\n",
      "\n",
      "work.\n",
      "\n",
      "If you're interested in getting involved, feel free to email me at chris@chrisjeffries.com\n",
      "\n",
      "Hot Tub Play\n",
      "\n",
      "It's very easy to get distracted by the heavy stuff when you're just watching TV. But talking to a beautiful woman in the hot tub with a hot shower and a tall stack of towels is the best reason to play with your body and mind. My wife and I can't stand the lowdown game that comes with sex in the heat. We love watching our husbands play rough and dirty with their vibrators, fingers, and toys and we enjoy the erotic tension between lustful men looking at each other and wanting cum and the desire for us to give it to them. It's so hot when you can play with your body with your partner's intimate touch and excitement. When we have sex, we don't just like to have sex. We want to please each other, and find ways to please each other in a way that the other can find pleasurable. One thing that I am really proud of is that we do not have a sex therapist. We have a very physical and authentic sex therapist and the sex is first and foremost about us. We don't have to worry about finding a sex therapist who is right for you or for your needs. We can decide that we don't need one because we are going to find one that we trust to help us make the most pleasurable sex for us. I give myself permission to do things that I can't do in the bedroom. In the hot tub and in life, I will always know that I am doing something that is truly pleasurable and that I'm doing it for me, for my partner, and for my body.\n",
      "\n",
      "~ Stephanie Jeffries\n",
      "\n",
      "A fantastic resource for hot\n",
      "Inference Time: 201.0470 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# Detect device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to detected device\n",
    "gpt = model.to(device)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# Tokenize input and move to device\n",
    "prompt = \"I have a dream that \"\n",
    "input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run text generation\n",
    "generated_text = generate(\n",
    "    model=gpt,  \n",
    "    prompt=prompt,\n",
    "    max_new_tokens=1000,\n",
    "    context_size=512,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.75,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print generated text\n",
    "print(generated_text)\n",
    "\n",
    "# Print benchmark results\n",
    "print(f\"Inference Time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model the genration took 184.6997 seconds to generate 1000 tokens\n",
    "\n",
    "Let see how the model performs with KV cache\n",
    "\n",
    "But first we need to define the model with KV cache , since the model doesn't have KV cache by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention 0:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 1:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 2:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 3:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 4:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 5:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 6:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 7:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 8:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 9:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 10:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 11:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 12:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 13:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 14:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 15:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 16:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 17:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 18:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 19:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 20:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 21:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 22:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n",
      "MultiHeadAttention 23:\n",
      "MultiHeadAttention(\n",
      "  (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing and printing the MultiHeadAttention layers of the model\n",
    "for i, block in enumerate(model.trf_blocks):\n",
    "    # Each block is a TransformerBlock, and its attention layer is in the 'att' attribute\n",
    "    multihead_attention = block.att\n",
    "    print(f\"MultiHeadAttention {i}:\")\n",
    "    print(multihead_attention)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # We'll create the mask dynamically in forward pass instead\n",
    "        self.max_length = context_length\n",
    "\n",
    "    def forward(self, x, past_kvs=None):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Create queries, keys, values\n",
    "        queries = self.W_query(x)  # (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Handle KV cache\n",
    "        if past_kvs is not None:\n",
    "            past_keys, past_values = past_kvs\n",
    "            keys = torch.cat([past_keys, keys], dim=2)\n",
    "            values = torch.cat([past_values, values], dim=2)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        seq_len = keys.size(2)  # Total sequence length including past\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)  # (b, num_heads, num_tokens, seq_len)\n",
    "\n",
    "        # Create causal mask dynamically\n",
    "        device = queries.device\n",
    "        mask = torch.ones((1, 1, num_tokens, seq_len), dtype=torch.bool, device=device)\n",
    "        mask = torch.triu(mask.squeeze(0).squeeze(0), diagonal=1)\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Apply mask and scaling\n",
    "        attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
    "        attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values)  # (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "        # Reshape output\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous()  # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = context_vec.view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec, (keys, values)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x, past_kvs=None):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x, new_kvs = self.att(x, past_kvs)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x, new_kvs\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx, past_kvs=None):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        new_past_kvs = []\n",
    "        for i, trf_block in enumerate(self.trf_blocks):\n",
    "            x, kv = trf_block(x, past_kvs[i] if past_kvs is not None else None)\n",
    "            new_past_kvs.append(kv)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        # Return logits and updated KV cache\n",
    "        return logits, new_past_kvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 00:09:07.300536: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-18 00:09:07.313200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739826547.335308  101344 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739826547.341483  101344 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 00:09:07.362836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from UTILS.load_weights import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "## Choose the model to use\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "## Base configuration settings for the model\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Size of the vocabulary used by the model\n",
    "    \"context_length\": 1024,  # Maximum context length the model can handle\n",
    "    \"drop_rate\": 0.0,        # Dropout rate for regularization\n",
    "    \"qkv_bias\": True         # Whether to use bias terms in query, key, and value projections\n",
    "}\n",
    "\n",
    "## Dictionary containing configurations for different GPT-2 model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},      # Config for small model\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},    # Config for medium model\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},     # Config for large model\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},       # Config for extra-large model\n",
    "}\n",
    "\n",
    "## Update the BASE_CONFIG with parameters specific to the chosen model\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model_cache = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model_cache, params)  # Load weights into model_cache\n",
    "model_cache.eval()  # Set the model in evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "\n",
    "def top_p_logits(logits, p=0.5):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    sorted_probs[cumulative_probs > p] = 0\n",
    "    filtered_logits = torch.zeros_like(logits).to(logits.device)  # Move to same device\n",
    "    filtered_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_probs)\n",
    "    return filtered_logits\n",
    "\n",
    "def generate(model, prompt, max_new_tokens, context_size, tokenizer, temperature=0.0, top_k=None, top_p=None, eos=None):\n",
    "    # Detect device\n",
    "    device = next(model.parameters()).device  # Get model's device\n",
    "\n",
    "    # Encode and move input to the correct device\n",
    "    idx = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    idx_gen = idx.clone()  # Start with the prompt indices\n",
    "    kv_cache = None  # Initialize KV cache as None for the first pass\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx_gen[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, kv_cache = model(idx_cond, kv_cache)  # Pass KV cache to model's forward pass\n",
    "\n",
    "            # Ensure logits and idx_next have matching shapes\n",
    "            batch_size = logits.size(0)  # Get batch size\n",
    "            vocab_size = logits.size(-1)  # Get vocabulary size (50257)\n",
    "            \n",
    "            # Handle the attention mask size issue by creating a dynamic mask\n",
    "            num_tokens = idx_cond.size(1)\n",
    "            mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1).to(device)\n",
    "            mask = mask.bool()  # Create the attention mask for causal attention\n",
    "\n",
    "            # Apply the mask\n",
    "            logits[:, :, :num_tokens].masked_fill_(mask, -float('inf'))\n",
    "            logits = logits[:, -1, :]  # Take the last token's logits\n",
    "\n",
    "            # Apply top-k sampling\n",
    "            if top_k is not None:\n",
    "                top_k_values, _ = torch.topk(logits, k=top_k)\n",
    "                min_value = top_k_values[:, -1].unsqueeze(1)  \n",
    "                logits = torch.where(logits < min_value, torch.tensor(float('-inf')).to(device), logits)\n",
    "\n",
    "            # Apply top-p sampling\n",
    "            if top_p is not None:\n",
    "                logits = top_p_logits(logits, p=top_p)  \n",
    "\n",
    "            # Apply temperature\n",
    "            if temperature > 0.0:\n",
    "                logits = logits / temperature\n",
    "                probs = torch.softmax(logits, dim=-1)  # Convert to probabilities\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)  # Sample token\n",
    "            else: \n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # Take max logit\n",
    "\n",
    "            # EOS handling\n",
    "            if eos is not None and torch.equal(idx_next, torch.tensor(eos).to(device)):\n",
    "                break\n",
    "            \n",
    "            # Append new token to the generated sequence\n",
    "            idx_gen = torch.cat((idx_gen, idx_next), dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx_gen.squeeze(0).tolist())  # Convert tokens back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 19.12 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 646.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Run the generation function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your loaded model here\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Measure end time\u001b[39;00m\n\u001b[1;32m     34\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, prompt, max_new_tokens, context_size, tokenizer, temperature, top_k, top_p, eos)\u001b[0m\n\u001b[1;32m     25\u001b[0m idx_cond \u001b[38;5;241m=\u001b[39m idx_gen[:, \u001b[38;5;241m-\u001b[39mcontext_size:]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     logits, kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass KV cache to model's forward pass\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Ensure logits and idx_next have matching shapes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Get batch size\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 161\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[0;34m(self, in_idx, past_kvs)\u001b[0m\n\u001b[1;32m    159\u001b[0m new_past_kvs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, trf_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrf_blocks):\n\u001b[0;32m--> 161\u001b[0m     x, kv \u001b[38;5;241m=\u001b[39m \u001b[43mtrf_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_kvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kvs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     new_past_kvs\u001b[38;5;241m.\u001b[39mappend(kv)\n\u001b[1;32m    164\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, past_kvs)\u001b[0m\n\u001b[1;32m    123\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 125\u001b[0m x, new_kvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_kvs\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_resid(x)\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut  \u001b[38;5;66;03m# Add the original input back\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MyLLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, past_kvs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Calculate attention scores\u001b[39;00m\n\u001b[1;32m     43\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m keys\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Total sequence length including past\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m \u001b[43mqueries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, num_heads, num_tokens, seq_len)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Create causal mask dynamically\u001b[39;00m\n\u001b[1;32m     47\u001b[0m device \u001b[38;5;241m=\u001b[39m queries\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 19.12 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.08 GiB is allocated by PyTorch, and 646.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Assuming you're using tiktoken for GPT-2 (this may vary depending on your tokenizer)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Check if GPU is available\n",
    "# Example prompt text\n",
    "prompt = \"I have a dream that \"\n",
    "model_cache = model_cache.to(device)  # Move model to device\n",
    "\n",
    "# Handle EOS token\n",
    "eos_token = \"<|endoftext|>\"  # Define the EOS token\n",
    "eos_token_id = tokenizer.encode(eos_token, allowed_special={eos_token})[0]  # Allow EOS token\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Run the generation function\n",
    "generated_text = generate(\n",
    "    model=model_cache,  # Your loaded model here\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=1000,\n",
    "    context_size=512,\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0/75,\n",
    "    top_k=None,\n",
    "    top_p=None,\n",
    "    eos=eos_token_id\n",
    ")\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print generated text\n",
    "print(generated_text)\n",
    "\n",
    "# Print benchmark results\n",
    "print(f\"Inference Time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
