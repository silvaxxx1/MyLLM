{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 6.4: Practical Guide to LLM Quantization Techniques and Frameworks**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "üéâ Welcome to **Notebook 6.4**, where we dive deeper into **quantization techniques** specifically for **large language models (LLMs)**! üöÄ In the previous notebook (**Notebook 6.3**), we implemented a **linear 8-bit quantizer** that was **model-agnostic** and observed the performance improvements for **transformer models**. But that was just the beginning! While **linear quantization** is a great first step, as models keep scaling up and growing larger, we encounter **outliers**‚Äîwhich presents new challenges. \n",
    "\n",
    "üí° **The Challenge?**  \n",
    "As **transformer models** continue to scale up, **outliers** become a major concern. Outliers, in this context, are values that deviate significantly from the expected norm, and when dealing with quantization, they can cause inefficiencies and performance degradation. In simple terms, **outliers** are data points that are much larger or smaller than the rest of the data, often leading to **poor approximation** during the quantization process. The traditional **linear quantization** method struggles with these extreme values, resulting in **limited performance** as the model size increases.  \n",
    "\n",
    "üí° **The Solution?**  \n",
    "To address these issues, we need more advanced quantization techniques that can better handle the presence of **outliers**. This is where **Post-Training Quantization (PTQ)** techniques come into play. PTQ offers powerful methods to optimize models after they have been trained, without requiring retraining. PTQ can adapt to the outlier problem and **maintain model accuracy** even with larger and more complex models.\n",
    "\n",
    "### **Why This Notebook?**  \n",
    "In this notebook, we will explore **advanced quantization techniques** that tackle the **outlier problem** and utilize **Post-Training Quantization (PTQ)**. Specifically, we will look into techniques and frameworks that focus on **dynamic** and **static quantization** methods, both of which are highly effective for **large-scale models** like LLMs. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Q_TECH.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## **What‚Äôs Inside?**\n",
    "\n",
    "üîç Here‚Äôs what we will cover in this notebook:  \n",
    "\n",
    "### **1Ô∏è‚É£ Understanding the Outlier Problem in Quantization**  \n",
    "‚úÖ What are **outliers**, and why do they affect **large model quantization**?  \n",
    "‚úÖ How do **outliers** impact performance when using **linear quantization**?  \n",
    "\n",
    "### **2Ô∏è‚É£ Overview of Quantization Techniques for LLMs**  \n",
    "‚úÖ **QAT (Quantization-Aware Training)** vs **PTQ (Post-Training Quantization)**  \n",
    "‚úÖ When to use **PTQ** and why it‚Äôs well-suited for large models  \n",
    "\n",
    "### **3Ô∏è‚É£ Static vs Dynamic Quantization**  \n",
    "‚úÖ Understanding the difference between **static** and **dynamic quantization**  \n",
    "‚úÖ How these methods work with **Post-Training Quantization**  \n",
    "\n",
    "### **4Ô∏è‚É£ Exploring PTQ Frameworks and Tools**  \n",
    "‚úÖ Overview of popular **PTQ frameworks** and **tools**  \n",
    "‚úÖ How these frameworks help manage outliers and improve model efficiency  \n",
    "\n",
    "### **5Ô∏è‚É£ Implementing PTQ in Large Language Models**  \n",
    "‚úÖ Practical guide to applying PTQ to **transformer models**  \n",
    "‚úÖ Step-by-step walkthrough using a **PTQ framework**  \n",
    "\n",
    "## **Why This Notebook Matters**\n",
    "\n",
    "Large language models are becoming increasingly powerful, but their deployment in resource-constrained environments is a challenge. **Quantization** is key to **making these models smaller, faster, and more efficient**, but as models scale, the **outlier problem** becomes more prominent. In this notebook, you will:\n",
    "\n",
    "‚úÖ Learn about the **outlier problem** in large-scale model quantization  \n",
    "‚úÖ Understand the distinction between **QAT** and **PTQ**.\n",
    "‚úÖ Gain hands-on experience with **static** and **dynamic quantization** techniques  \n",
    "‚úÖ Explore leading **PTQ frameworks** and how they handle outliers  \n",
    "‚úÖ Apply PTQ techniques to **real-world transformer models**  \n",
    "\n",
    "## **Looking Ahead**\n",
    "\n",
    "With the techniques and frameworks covered in this notebook, you‚Äôll be able to optimize **large language models** with **Post-Training Quantization** and make them more deployable. But we‚Äôre not stopping here‚Äîthere are still many advanced quantization strategies to explore in future notebooks, as the field is rapidly evolving.\n",
    "\n",
    "Ready to tackle the **outlier problem** and unlock the true potential of **large language model quantization**? Let‚Äôs dive in! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Difference Between QAT and PTQ, Static and Dynamic Quantization**\n",
    "\n",
    "### **QAT (Quantization-Aware Training) vs PTQ (Post-Training Quantization)**\n",
    "\n",
    "- **QAT (Quantization-Aware Training)**:\n",
    "  - **Definition**: QAT involves simulating quantization during the training process. The model learns to adapt to the quantization noise by training with quantized weights and activations.\n",
    "  - **Use Case**: Best suited when training a model from scratch or fine-tuning it. The model becomes aware of the quantization effects, leading to better accuracy after quantization.\n",
    "  - **Pros**: Retains model accuracy after quantization.\n",
    "  - **Cons**: Requires retraining, which can be computationally expensive and time-consuming.\n",
    "\n",
    "- **PTQ (Post-Training Quantization)**:\n",
    "  - **Definition**: PTQ quantizes a pre-trained model without the need for retraining. This is done after the model has been fine-tuned, typically by converting the weights to lower precision (e.g., INT8).\n",
    "  - **Use Case**: Ideal for deploying pre-trained models efficiently without needing retraining.\n",
    "  - **Pros**: Faster and less computationally expensive than QAT.\n",
    "  - **Cons**: May lead to a slight drop in accuracy due to the lack of fine-tuning for quantization.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/QAT_PTQ.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Static Quantization vs Dynamic Quantization**\n",
    "\n",
    "- **Static Quantization**:\n",
    "  - **Definition**: Static quantization involves calibrating the model with a representative dataset to determine the optimal quantization parameters (like scaling factors) for weights and activations.\n",
    "  - **Use Case**: Used in situations where the entire model is quantized upfront and requires careful tuning.\n",
    "  - **Pros**: Results in the most efficient models with maximum compression.\n",
    "  - **Cons**: Requires calibration data, which may not always be available.\n",
    "\n",
    "- **Dynamic Quantization**:\n",
    "  - **Definition**: Dynamic quantization performs quantization dynamically during inference, adjusting the precision of activations and weights as needed.\n",
    "  - **Use Case**: Ideal when you want quick deployment without requiring a calibration step.\n",
    "  - **Pros**: Easier to implement and faster than static quantization.\n",
    "  - **Cons**: Might be less efficient in terms of memory and speed compared to static quantization.\n",
    "\n",
    "### **Our Focus: PTQ for LLM Deployment**\n",
    "\n",
    "In this notebook, we will focus on **Post-Training Quantization (PTQ)**, as our goal is to optimize **large language models (LLMs)** for **deployment after fine-tuning**. PTQ allows us to efficiently reduce model size and improve inference speed without retraining the model, which is key for deploying large models in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1- BitsAndBytes**\n",
    "\n",
    "- **Definition**: `BitsAndBytes` is a library designed for efficient model quantization, supporting **Post-Training Quantization (PTQ)** and **Quantization-Aware Training (QAT)**.\n",
    "  \n",
    "- **Core Functionality**:\n",
    "  - **Quantization**: Provides advanced techniques such as 8-bit and mixed precision quantization for optimizing large models.\n",
    "  - **Efficient Memory Usage**: Reduces model size and memory footprint without sacrificing accuracy.\n",
    "  - **Performance Optimization**: Increases inference speed by quantizing weights and activations to lower bit precision.\n",
    "\n",
    "- **Under the Hood**:\n",
    "  - **Vector-wise Quantization (Dot Product)**: Uses **vector-wise quantization** where the weights are quantized using **dot products**, improving both memory efficiency and computational speed.\n",
    "  - **Mixed Precision**: Supports **mixed precision quantization**, combining high-precision weights with low-precision activations for optimal performance and accuracy.\n",
    "  - **Quantize Optimizers**: Utilizes specialized optimizers to adapt weights during quantization and fine-tuning to minimize performance loss.\n",
    "  - **NF4 (Static Quantization for QLoRA Fine-Tuning)**: Implements **NF4 quantization** for fine-tuning large models under **QLoRA** (Quantized Low-Rank Adaptation) using **static quantization** techniques to further optimize large-scale models.\n",
    "  \n",
    "- **Support for Multiple Frameworks**: Compatible with PyTorch, TensorFlow, and other ML frameworks for flexible deployment.\n",
    "\n",
    "- the simplest way to use it is within the transformer library as follow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1541748046875 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the original model with no BitsandBites and see the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.466552734375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a big diffrent is size \n",
    "\n",
    "Additionally BitsAndBytes even let you load the model in 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Usage**  \n",
    "\n",
    "You can experiment with different **4-bit quantization** variants, such as:  \n",
    "\n",
    "- **NF4 (Normalized Float 4, default)** ‚Äì Recommended for better performance based on theoretical and empirical results.  \n",
    "- **FP4 (Pure Floating-Point 4-bit Quantization)** ‚Äì An alternative option.  \n",
    "\n",
    "### **Additional Optimization Options**  \n",
    "- **`bnb_4bit_use_double_quant`**: Enables a second quantization pass, reducing memory usage by an additional **0.4 bits per parameter**.  \n",
    "- **Compute Precision**: While weights are stored in **4-bit**, computations can still be performed in higher precision:\n",
    "  - **`float16`** (Faster training, commonly used)  \n",
    "  - **`bfloat16`**  \n",
    "  - **`float32`** (Default)  \n",
    "\n",
    "Using **`float16`** as the compute type speeds up matrix multiplication and training.\n",
    "\n",
    "### **Example: Load a Model in 4-bit Using NF4 & Double Quantization**  \n",
    "\n",
    "To customize these parameters, we use `BitsAndBytesConfig` from **Hugging Face Transformers**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "         bnb_4bit_quant_type=\"nf4\", ## the type of quantization\n",
    "        bnb_4bit_use_double_quant=True, ## double quant\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, ## bfloat for computaion during inference\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finding the Optimal Configuration**  \n",
    "üí° **There‚Äôs no one-size-fits-all setup!** You should experiment with:  \n",
    "‚úÖ **Quantization type** (`nf4` vs `fp4`)  \n",
    "‚úÖ **Nested quantization** (`bnb_4bit_use_double_quant=True/False`)  \n",
    "‚úÖ **Compute dtype** (`float16`, `bfloat16`, or `float32`)  \n",
    "\n",
    "By tuning these parameters, you can **find the sweet spot** that balances **accuracy, speed, and memory usage**, aligning with your **hardware capabilities** and **deployment goals**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
