{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 6.4: Practical Guide to LLM Quantization Techniques and Frameworks**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "üéâ Welcome to **Notebook 6.4**, where we dive deeper into **quantization techniques** specifically for **large language models (LLMs)**! üöÄ In the previous notebook (**Notebook 6.3**), we implemented a **linear 8-bit quantizer** that was **model-agnostic** and observed the performance improvements for **transformer models**. But that was just the beginning! While **linear quantization** is a great first step, as models keep scaling up and growing larger, we encounter **outliers**‚Äîwhich presents new challenges. \n",
    "\n",
    "üí° **The Challenge?**  \n",
    "As **transformer models** continue to scale up, **outliers** become a major concern. Outliers, in this context, are values that deviate significantly from the expected norm, and when dealing with quantization, they can cause inefficiencies and performance degradation. In simple terms, **outliers** are data points that are much larger or smaller than the rest of the data, often leading to **poor approximation** during the quantization process. The traditional **linear quantization** method struggles with these extreme values, resulting in **limited performance** as the model size increases.  \n",
    "\n",
    "üí° **The Solution?**  \n",
    "To address these issues, we need more advanced quantization techniques that can better handle the presence of **outliers**. This is where **Post-Training Quantization (PTQ)** techniques come into play. PTQ offers powerful methods to optimize models after they have been trained, without requiring retraining. PTQ can adapt to the outlier problem and **maintain model accuracy** even with larger and more complex models.\n",
    "\n",
    "### **Why This Notebook?**  \n",
    "In this notebook, we will explore **advanced quantization techniques** that tackle the **outlier problem** and utilize **Post-Training Quantization (PTQ)**. Specifically, we will look into techniques and frameworks that focus on **dynamic** and **static quantization** methods, both of which are highly effective for **large-scale models** like LLMs. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Q_TECH.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## **What‚Äôs Inside?**\n",
    "\n",
    "üîç Here‚Äôs what we will cover in this notebook:  \n",
    "\n",
    "### **1Ô∏è‚É£ Understanding the Outlier Problem in Quantization**  \n",
    "‚úÖ What are **outliers**, and why do they affect **large model quantization**?  \n",
    "‚úÖ How do **outliers** impact performance when using **linear quantization**?  \n",
    "\n",
    "### **2Ô∏è‚É£ Overview of Quantization Techniques for LLMs**  \n",
    "‚úÖ **QAT (Quantization-Aware Training)** vs **PTQ (Post-Training Quantization)**  \n",
    "‚úÖ When to use **PTQ** and why it‚Äôs well-suited for large models  \n",
    "\n",
    "### **3Ô∏è‚É£ Static vs Dynamic Quantization**  \n",
    "‚úÖ Understanding the difference between **static** and **dynamic quantization**  \n",
    "‚úÖ How these methods work with **Post-Training Quantization**  \n",
    "\n",
    "### **4Ô∏è‚É£ Exploring PTQ Frameworks and Tools**  \n",
    "‚úÖ Overview of popular **PTQ frameworks** and **tools**  \n",
    "‚úÖ How these frameworks help manage outliers and improve model efficiency  \n",
    "\n",
    "### **5Ô∏è‚É£ Implementing PTQ in Large Language Models**  \n",
    "‚úÖ Practical guide to applying PTQ to **transformer models**  \n",
    "‚úÖ Step-by-step walkthrough using a **PTQ framework**  \n",
    "\n",
    "## **Why This Notebook Matters**\n",
    "\n",
    "Large language models are becoming increasingly powerful, but their deployment in resource-constrained environments is a challenge. **Quantization** is key to **making these models smaller, faster, and more efficient**, but as models scale, the **outlier problem** becomes more prominent. In this notebook, you will:\n",
    "\n",
    "‚úÖ Learn about the **outlier problem** in large-scale model quantization  \n",
    "‚úÖ Understand the distinction between **QAT** and **PTQ**.\n",
    "‚úÖ Gain hands-on experience with **static** and **dynamic quantization** techniques  \n",
    "‚úÖ Explore leading **PTQ frameworks** and how they handle outliers  \n",
    "‚úÖ Apply PTQ techniques to **real-world transformer models**  \n",
    "\n",
    "## **Looking Ahead**\n",
    "\n",
    "With the techniques and frameworks covered in this notebook, you‚Äôll be able to optimize **large language models** with **Post-Training Quantization** and make them more deployable. But we‚Äôre not stopping here‚Äîthere are still many advanced quantization strategies to explore in future notebooks, as the field is rapidly evolving.\n",
    "\n",
    "Ready to tackle the **outlier problem** and unlock the true potential of **large language model quantization**? Let‚Äôs dive in! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Difference Between QAT and PTQ, Static and Dynamic Quantization**\n",
    "\n",
    "### **QAT (Quantization-Aware Training) vs PTQ (Post-Training Quantization)**\n",
    "\n",
    "- **QAT (Quantization-Aware Training)**:\n",
    "  - **Definition**: QAT involves simulating quantization during the training process. The model learns to adapt to the quantization noise by training with quantized weights and activations.\n",
    "  - **Use Case**: Best suited when training a model from scratch or fine-tuning it. The model becomes aware of the quantization effects, leading to better accuracy after quantization.\n",
    "  - **Pros**: Retains model accuracy after quantization.\n",
    "  - **Cons**: Requires retraining, which can be computationally expensive and time-consuming.\n",
    "\n",
    "- **PTQ (Post-Training Quantization)**:\n",
    "  - **Definition**: PTQ quantizes a pre-trained model without the need for retraining. This is done after the model has been fine-tuned, typically by converting the weights to lower precision (e.g., INT8).\n",
    "  - **Use Case**: Ideal for deploying pre-trained models efficiently without needing retraining.\n",
    "  - **Pros**: Faster and less computationally expensive than QAT.\n",
    "  - **Cons**: May lead to a slight drop in accuracy due to the lack of fine-tuning for quantization.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/QAT_PTQ.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Static Quantization vs Dynamic Quantization**\n",
    "\n",
    "- **Static Quantization**:\n",
    "  - **Definition**: Static quantization involves calibrating the model with a representative dataset to determine the optimal quantization parameters (like scaling factors) for weights and activations.\n",
    "  - **Use Case**: Used in situations where the entire model is quantized upfront and requires careful tuning.\n",
    "  - **Pros**: Results in the most efficient models with maximum compression.\n",
    "  - **Cons**: Requires calibration data, which may not always be available.\n",
    "\n",
    "- **Dynamic Quantization**:\n",
    "  - **Definition**: Dynamic quantization performs quantization dynamically during inference, adjusting the precision of activations and weights as needed.\n",
    "  - **Use Case**: Ideal when you want quick deployment without requiring a calibration step.\n",
    "  - **Pros**: Easier to implement and faster than static quantization.\n",
    "  - **Cons**: Might be less efficient in terms of memory and speed compared to static quantization.\n",
    "\n",
    "### **Our Focus: PTQ for LLM Deployment**\n",
    "\n",
    "In this notebook, we will focus on **Post-Training Quantization (PTQ)**, as our goal is to optimize **large language models (LLMs)** for **deployment after fine-tuning**. PTQ allows us to efficiently reduce model size and improve inference speed without retraining the model, which is key for deploying large models in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1- BitsAndBytes**\n",
    "\n",
    "- **Definition**: `BitsAndBytes` is a library designed for efficient model quantization, supporting **Post-Training Quantization (PTQ)** and **Quantization-Aware Training (QAT)**.\n",
    "  \n",
    "- **Core Functionality**:\n",
    "  - **Quantization**: Provides advanced techniques such as 8-bit and mixed precision quantization for optimizing large models.\n",
    "  - **Efficient Memory Usage**: Reduces model size and memory footprint without sacrificing accuracy.\n",
    "  - **Performance Optimization**: Increases inference speed by quantizing weights and activations to lower bit precision.\n",
    "\n",
    "  <p align=\"center\">\n",
    "  <img src=\"images/bnb.jpeg\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "- **Under the Hood**:\n",
    "  - **Vector-wise Quantization (Dot Product)**: Uses **vector-wise quantization** where the weights are quantized using **dot products**, improving both memory efficiency and computational speed.\n",
    "  - **Mixed Precision**: Supports **mixed precision quantization**, combining high-precision weights with low-precision activations for optimal performance and accuracy.\n",
    "  - **Quantize Optimizers**: Utilizes specialized optimizers to adapt weights during quantization and fine-tuning to minimize performance loss.\n",
    "  - **NF4 (Static Quantization for QLoRA Fine-Tuning)**: Implements **NF4 quantization** for fine-tuning large models under **QLoRA** (Quantized Low-Rank Adaptation) using **static quantization** techniques to further optimize large-scale models.\n",
    "- **Support for Multiple Frameworks**: Compatible with PyTorch, TensorFlow, and other ML frameworks for flexible deployment.\n",
    "\n",
    "- the simplest way to use it is within the transformer library as follow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1541748046875 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the original model with no BitsandBites and see the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.466552734375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a big diffrent is size \n",
    "\n",
    "Additionally BitsAndBytes even let you load the model in 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Usage**  \n",
    "\n",
    "You can experiment with different **4-bit quantization** variants, such as:  \n",
    "\n",
    "- **NF4 (Normalized Float 4, default)** ‚Äì Recommended for better performance based on theoretical and empirical results.  \n",
    "- **FP4 (Pure Floating-Point 4-bit Quantization)** ‚Äì An alternative option.  \n",
    "\n",
    "### **Additional Optimization Options**  \n",
    "- **`bnb_4bit_use_double_quant`**: Enables a second quantization pass, reducing memory usage by an additional **0.4 bits per parameter**.  \n",
    "- **Compute Precision**: While weights are stored in **4-bit**, computations can still be performed in higher precision:\n",
    "  - **`float16`** (Faster training, commonly used)  \n",
    "  - **`bfloat16`**  \n",
    "  - **`float32`** (Default)  \n",
    "\n",
    "Using **`float16`** as the compute type speeds up matrix multiplication and training.\n",
    "\n",
    "### **Example: Load a Model in 4-bit Using NF4 & Double Quantization**  \n",
    "\n",
    "To customize these parameters, we use `BitsAndBytesConfig` from **Hugging Face Transformers**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "         bnb_4bit_quant_type=\"nf4\", ## the type of quantization\n",
    "        bnb_4bit_use_double_quant=True, ## double quant\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, ## bfloat for computaion during inference\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.GPTQ\n",
    "\n",
    "GPTQ (Generalized Post-Training Quantization) is an advanced quantization technique designed to efficiently reduce the precision of large language models (LLMs) while maintaining high accuracy. It enables **fast inference** and **lower memory usage** by using **4-bit weight quantization with error correction**.\n",
    "\n",
    "\n",
    "  <p align=\"center\">\n",
    "  <img src=\"images/gptq.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "## The problem : \n",
    "Traditional quantization methods (like uniform or na√Øve rounding) can introduce significant errors when converting high-precision weights (e.g., FP16 or FP32) into low-bit representations (e.g., INT4). These errors can accumulate and cause a noticeable drop in performance.\n",
    "\n",
    "## Soultion: \n",
    "GPTQ solves this issue using Optimal Brain Surgeon (OBS) principles, which adjust other weights after quantizing each weight to minimize the overall error. The key idea is:\n",
    "\n",
    "- **Quantize One Weight at a Time**: Instead of naive rounding, GPTQ processes weights sequentially to minimize overall error.\n",
    "- **Error Correction via Hessian Approximation**: Adjusts remaining weights dynamically to compensate for quantization error.\n",
    "- **Optimal Brain Surgeon (OBS) Principles**: Uses second-order optimization for best quantization results.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "GPTQ minimizes the activation reconstruction error:\n",
    "```python\n",
    "# X: Input activations\n",
    "# W: Original weight matrix\n",
    "# W_tilde: Quantized version\n",
    "\n",
    "def activation_reconstruction_error(X, W, W_tilde):\n",
    "    return torch.norm(X @ W - X @ W_tilde, p='fro')**2\n",
    "```\n",
    "Instead of a naive quantization:\n",
    "```python\n",
    "# Naive quantization\n",
    "W_tilde[i, j] = quantize(W[i, j])\n",
    "```\n",
    "GPTQ finds the best quantized weight while adjusting other weights using an **Hessian-based update**:\n",
    "```python\n",
    "# Hessian-based update\n",
    "w_new = w_old - torch.inverse(H) @ e_j * (q_j - w_j)\n",
    "```\n",
    "where `H` is the Hessian approximation capturing sensitivity.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Compute Activations**: Use a small dataset to estimate activations `X`.\n",
    "2. **Estimate Hessian**: Compute `H = (X.T @ X) / N`.\n",
    "3. **Iterate Over Weights**:\n",
    "   - Find the best quantized value `q_j` for each weight.\n",
    "   - Update remaining weights using Hessian inverse correction.\n",
    "4. **Repeat Until All Weights Are Quantized**.\n",
    "\n",
    "\n",
    "## **Benefits of GPTQ**\n",
    "‚úÖ **4-bit Quantization** with minimal accuracy loss  \n",
    "‚úÖ **Faster Inference** due to smaller models  \n",
    "‚úÖ **Memory Efficient** for edge and server deployment  \n",
    "‚úÖ **Better Than Naive Rounding** through Hessian correction  \n",
    "\n",
    "\n",
    "Enough theory lets dive into the code : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize (in this case, GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to load the model and the tokenizer. The tokenizer is loaded using the classic AutoTokenizer class from the transformers library. On the other hand, we need to pass a specific configuration (BaseQuantizeConfig) to load the model.\n",
    "\n",
    "In this configuration, we can specify the number of bits to quantize (here, bits=4) and the group size (size of the lazy batch). Note that this group size is optional: we could also use one set of parameters for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost (especially with group_size=1024). The damp_percent value is here to help the Cholesky reformulation and should not be changed.\n",
    "\n",
    "Finally, the desc_act (also called act order) is a tricky parameter. It allows you to process rows based on decreasing activation, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won‚Äôt use it here (it will probably be fixed in the future, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954e9cfe12d42a484638f726303785e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358df74bbff342de93dc8742a4526e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afae28d138f4ecba2503708eb1c08ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019025f183ff4e39964483dcb33097a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dc07a972674a77a886927678b8a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99121cb5aa464da8bf96e0e93453bf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58c57bc8ed34d3e952eaf5c48dd7b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load quantize config, model and tokenizer\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,\n",
    ")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization process relies heavily on samples to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality.\n",
    "\n",
    "In the context of this article, we utilize the C4 (Colossal Clean Crawled Corpus) dataset to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large-scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option.\n",
    "\n",
    "In the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadb1be269f142b39fc3b14150f692d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642da0dd35304fa9ba692e3dce7c2634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c4-train.00001-of-01024.json.gz:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6531da6500f48d689f8be38972a6066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load data and tokenize examples\n",
    "n_samples = 1024\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use OpenAI Triton, a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/12\n",
      "INFO - Quantizing attn.c_attn in layer 1/12...\n",
      "INFO - Quantizing attn.c_proj in layer 1/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 1/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 1/12...\n",
      "INFO - Start quantizing layer 2/12\n",
      "INFO - Quantizing attn.c_attn in layer 2/12...\n",
      "INFO - Quantizing attn.c_proj in layer 2/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 2/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 2/12...\n",
      "INFO - Start quantizing layer 3/12\n",
      "INFO - Quantizing attn.c_attn in layer 3/12...\n",
      "INFO - Quantizing attn.c_proj in layer 3/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 3/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 3/12...\n",
      "INFO - Start quantizing layer 4/12\n",
      "INFO - Quantizing attn.c_attn in layer 4/12...\n",
      "INFO - Quantizing attn.c_proj in layer 4/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 4/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 4/12...\n",
      "INFO - Start quantizing layer 5/12\n",
      "INFO - Quantizing attn.c_attn in layer 5/12...\n",
      "INFO - Quantizing attn.c_proj in layer 5/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 5/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 5/12...\n",
      "INFO - Start quantizing layer 6/12\n",
      "INFO - Quantizing attn.c_attn in layer 6/12...\n",
      "INFO - Quantizing attn.c_proj in layer 6/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 6/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 6/12...\n",
      "INFO - Start quantizing layer 7/12\n",
      "INFO - Quantizing attn.c_attn in layer 7/12...\n",
      "INFO - Quantizing attn.c_proj in layer 7/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 7/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 7/12...\n",
      "INFO - Start quantizing layer 8/12\n",
      "INFO - Quantizing attn.c_attn in layer 8/12...\n",
      "INFO - Quantizing attn.c_proj in layer 8/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 8/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 8/12...\n",
      "INFO - Start quantizing layer 9/12\n",
      "INFO - Quantizing attn.c_attn in layer 9/12...\n",
      "INFO - Quantizing attn.c_proj in layer 9/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 9/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 9/12...\n",
      "INFO - Start quantizing layer 10/12\n",
      "INFO - Quantizing attn.c_attn in layer 10/12...\n",
      "INFO - Quantizing attn.c_proj in layer 10/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 10/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 10/12...\n",
      "INFO - Start quantizing layer 11/12\n",
      "INFO - Quantizing attn.c_attn in layer 11/12...\n",
      "INFO - Quantizing attn.c_proj in layer 11/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 11/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 11/12...\n",
      "INFO - Start quantizing layer 12/12\n",
      "INFO - Quantizing attn.c_attn in layer 12/12...\n",
      "INFO - Quantizing attn.c_proj in layer 12/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 12/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 12/12...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 44s, sys: 29min 34s, total: 42min 19s\n",
      "Wall time: 42min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-GPTQ/tokenizer_config.json',\n",
       " 'gpt2-GPTQ/special_tokens_map.json',\n",
       " 'gpt2-GPTQ/vocab.json',\n",
       " 'gpt2-GPTQ/merges.txt',\n",
       " 'gpt2-GPTQ/added_tokens.json',\n",
       " 'gpt2-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After , the model can loaded from the output directory using the AutoGPTQForCausalLM and AutoTokenizer classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the model working properly after the quantization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream to make it to one of the first theaters in the country. They are not even close and I have such great plans. I never met anyone here before and it is as good as my dreams. People just need to learn how to be the biggest stars that they can.\"\n",
      "\n",
      "\"It took seven years to come together to create an audience that many will like. It is what I have always dreamed of,\" says his brother, Eric, of \"Avengers: Infinity\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=100)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs your enhanced Markdown with improved clarity, structure, and formatting:  \n",
    "\n",
    "---\n",
    "\n",
    "# **Quantization Techniques: A Review**  \n",
    "\n",
    "Before diving deeper into **GGML & Llama.cpp**, let's review the three fundamental **quantization techniques** used to optimize Large Language Models (LLMs) for efficient inference and fine-tuning.  \n",
    "\n",
    "### **1. NF4 (Normal Float 4-bit)**  \n",
    "NF4 is a **static quantization method** primarily used by **QLoRA** to load a model in **4-bit precision** for efficient fine-tuning.  \n",
    "\n",
    "- Utilized in **LoRA + PEFT** workflows.  \n",
    "- Reduces memory consumption while maintaining model quality.  \n",
    "- Commonly used for training or fine-tuning rather than direct inference.  \n",
    "\n",
    "### **2. GPTQ (Generalized Post-Training Quantization)**  \n",
    "GPTQ is a **post-training quantization (PTQ) method** designed to **compress LLMs without retraining** while minimizing performance degradation.  \n",
    "\n",
    "- Optimized for GPU-based inference.  \n",
    "- Uses an **approximate weight reconstruction algorithm** to reduce precision with minimal loss.  \n",
    "- Provides significant **speed and memory efficiency improvements** while maintaining accuracy.  \n",
    "\n",
    "### **3. GGML & GGUF (Next Section Preview)**  \n",
    "In the next section, we'll explore **GGML** and its successor **GGUF**, two quantization formats designed for efficient **CPU & GPU inference** using **Llama.cpp**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **GGML & Llama.cpp**  \n",
    "\n",
    "### **What is GGML?**  \n",
    "**GGML** (Georgi Gerganov Machine Learning) is a **high-performance C library** for machine learning, named after its creator **Georgi Gerganov**.  \n",
    "\n",
    "- Provides fundamental ML structures, including **tensor operations**.  \n",
    "- Introduces a specialized **binary format** to distribute LLMs efficiently.  \n",
    "- Originally focused on **CPU-based inference** for large models.  \n",
    "\n",
    "\n",
    "  <p align=\"center\">\n",
    "  <img src=\"images/llama_cpp.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "### **GGUF: The Next Evolution**  \n",
    "GGML models previously used a **.ggml** format, but they have now transitioned to **GGUF (GGML Unified Format)**:  \n",
    "\n",
    "- **Extensible design**: New features can be added without breaking compatibility.  \n",
    "- **Centralized metadata**: Stores special tokens, **RoPE scaling**, quantization details, etc.  \n",
    "- **Improved model compatibility** with tools like **llama.cpp** and other runtimes.  \n",
    "\n",
    "### **Llama.cpp: Efficient LLM Inference**  \n",
    "[Llama.cpp](https://github.com/ggerganov/llama.cpp) is an **optimized C++ library** for running LLaMA models efficiently.  \n",
    "\n",
    "- Originally designed for **CPU inference**, making LLMs accessible on lower-end hardware.  \n",
    "- Now supports **layer offloading to GPU**, allowing hybrid CPU+GPU execution.  \n",
    "- Highly optimized with **AVX, FMA, and ARM NEON** instructions for fast computation.  \n",
    "\n",
    "### **CPU + GPU Offloading: A Game Changer**  \n",
    "By **offloading specific layers** to the GPU, **llama.cpp** significantly accelerates inference while reducing memory constraints.  \n",
    "\n",
    "For example, on a **7B parameter model with 35 layers**, you can:  \n",
    "- Run **critical layers on the GPU** for acceleration.  \n",
    "- Keep **less demanding layers on the CPU** to conserve VRAM.  \n",
    "- Achieve **faster inference speeds** while running **larger models** on limited hardware.  \n",
    "\n",
    "So If the command line is your thing GGML & LLama.cpp is for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 45118, done.\u001b[K\n",
      "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
      "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
      "remote: Total 45118 (delta 157), reused 80 (delta 79), pack-reused 44876 (from 3)\u001b[K\n",
      "Receiving objects: 100% (45118/45118), 94.03 MiB | 1.00 MiB/s, done.\n",
      "Resolving deltas: 100% (32494/32494), done.\n",
      "/home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp\n",
      "/home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/build\n",
      "-- The C compiler identification is GNU 13.3.0\n",
      "-- The CXX compiler identification is GNU 13.3.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\") \n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Found CUDAToolkit: /usr/include (found version \"12.0.140\") \n",
      "-- CUDA Toolkit found\n",
      "-- Using CUDA architectures: native\n",
      "-- The CUDA compiler identification is NVIDIA 12.0.140\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "\u001b[0m-- CUDA host compiler is GNU 12.3.0\n",
      "\u001b[0m\n",
      "-- Including CUDA backend\n",
      "-- Configuring done (108.9s)\n",
      "-- Generating done (0.9s)\n",
      "-- Build files have been written to: /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/build\n",
      "[  0%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  0%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.43.0\") \n",
      "[  1%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[  1%] Built target build_info\n",
      "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[  2%] Built target sha1\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  2%] Built target sha256\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  3%] Built target xxhash\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  4%] Built target ggml-base\n",
      "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
      "[ 12%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[ 12%] Built target ggml-cpu\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o\u001b[0m\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n",
      "[ 37%] Built target ggml-cuda\n",
      "[ 37%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[ 38%] Built target ggml\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 39%] Built target llama-gguf\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 40%] Built target llama-gguf-hash\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 45%] Built target llama\n",
      "[ 45%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 48%] Built target test-c\n",
      "[ 48%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 48%] Built target llama-simple\n",
      "[ 48%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 48%] Built target llama-simple-chat\n",
      "[ 49%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
      "[ 50%] Built target llama-quantize-stats\n",
      "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 51%] Built target llava\n",
      "[ 51%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
      "[ 53%] Built target llava_shared\n",
      "[ 53%] Built target llava_static\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 53%] Built target common\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 55%] Built target test-grammar-parser\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 55%] Built target test-sampling\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 57%] Built target test-tokenizer-0\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 57%] Built target test-llama-grammar\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 57%] Built target test-tokenizer-1-bpe\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 58%] Built target test-tokenizer-1-spm\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 60%] Built target test-log\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 61%] Built target test-grammar-integration\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 62%] Built target test-arg-parser\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 63%] Built target test-chat-template\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 65%] Built target test-chat\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 65%] Built target test-gguf\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 67%] Built target test-model-load-cancel\n",
      "[ 67%] Built target test-autorelease\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 69%] Built target test-barrier\n",
      "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 69%] Built target test-json-schema-to-grammar\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 70%] Built target test-quantize-fns\n",
      "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 71%] Built target test-rope\n",
      "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 74%] Built target llama-batched-bench\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 75%] Built target llama-batched\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 75%] Built target test-quantize-perf\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 76%] Built target llama-gbnf-validator\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 77%] Built target llama-embedding\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 78%] Built target llama-eval-callback\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 78%] Built target llama-gguf-split\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
      "[ 80%] Built target llama-gritlm\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 80%] Built target test-backend-ops\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 81%] Built target llama-infill\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 81%] Built target llama-imatrix\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 82%] Built target llama-lookup-create\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 83%] Built target llama-lookahead\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 83%] Built target llama-lookup\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 84%] Built target llama-lookup-merge\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 86%] Built target llama-lookup-stats\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 87%] Built target llama-cli\n",
      "[ 87%] Built target llama-parallel\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 88%] Built target llama-passkey\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 89%] Built target llama-quantize\n",
      "[ 89%] Built target llama-retrieval\n",
      "[ 89%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 91%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[ 91%] Built target llama-bench\n",
      "[ 91%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
      "[ 91%] Built target llama-save-load-state\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 92%] Built target llama-perplexity\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 93%] Built target llama-speculative\n",
      "[ 93%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 93%] Built target llama-tokenize\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 95%] Built target llama-speculative-simple\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
      "[ 95%] Built target llama-run\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 96%] Built target llama-gen-docs\n",
      "[ 96%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 97%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 97%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 97%] Built target llama-cvector-generator\n",
      "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[ 97%] Built target llama-export-lora\n",
      "[ 97%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[ 98%] Built target llama-llava-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[ 98%] Built target llama-minicpmv-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 98%] Built target llama-qwen2vl-cli\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
      "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 98%] Built target llama-tts\n",
      "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[ 99%] Built target llama-llava-clip-quantize-cli\n",
      "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[100%] Built target llama-q8dot\n",
      "[100%] Built target llama-vdot\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[100%] Built target llama-server\n",
      "/home/silva/SILVA.AI/Projects/MyLLM101/notebooks\n",
      "/home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: sentencepiece~=0.2.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.49.0)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.6)\n",
      "Requirement already satisfied: torch~=2.2.1 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from -r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: filelock in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from jinja2->torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages (from sympy->torch~=2.2.1->-r /home/silva/SILVA.AI/Projects/MyLLM101/notebooks/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "/home/silva/SILVA.AI/Projects/MyLLM101/notebooks\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'EvolCodeLlama-7b'...\n",
      "remote: Enumerating objects: 35, done.\u001b[K\n",
      "remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (35/35), 483.38 KiB | 1.05 MiB/s, done.\n",
      "Filtering content:  80% (4/5), 153.13 MiB | 950.00 KiB/s\r"
     ]
    }
   ],
   "source": [
    "# Step 1: Define variables\n",
    "MODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n",
    "QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n",
    "\n",
    "# Step 2: Extract model name\n",
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Step 3: Install dependencies (remove sudo if running in container)\n",
    "!apt-get install -y git-lfs cmake  # Removed sudo\n",
    "\n",
    "# Step 4: Clean build llama.cpp with CUDA\n",
    "!rm -rf llama.cpp  # Force remove existing directory\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "!mkdir -p build \n",
    "%cd build\n",
    "!cmake .. -DGGML_CUDA=ON\n",
    "!cmake --build . --config Release -j 4\n",
    "%cd ../..\n",
    "\n",
    "# Step 5: Install Python requirements with correct path\n",
    "%cd llama.cpp\n",
    "!pip install -r requirements.txt  # Now in correct directory\n",
    "%cd ..\n",
    "\n",
    "# Step 6: Download model with proper cleanup\n",
    "!rm -rf {MODEL_NAME}  # Remove existing model dir\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/{MODEL_ID}\n",
    "\n",
    "# Step 7: Convert with correct script path\n",
    "fp16 = f\"{MODEL_NAME}/model.f16.gguf\"\n",
    "!python llama.cpp/scripts/convert-hf-to-gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
    "\n",
    "# Step 8: Quantize with correct binary path\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/model.{method}.gguf\"\n",
    "    !./llama.cpp/build/bin/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion and Quantization Pipeline\n",
    "\n",
    "This process sets up and converts a Hugging Face model to the GGUF format and applies quantization for optimized inference.\n",
    "\n",
    "### **Steps Summary:**\n",
    "1. **Define Variables**  \n",
    "   - Specifies the model to use (`EvolCodeLlama-7b`) and quantization methods (`q4_k_m`, `q5_k_m`)(recommended).\n",
    "\n",
    "2. **Install Dependencies**  \n",
    "   - Installs necessary packages: `git-lfs` (for handling large files) and `cmake` (for compiling dependencies).\n",
    "\n",
    "3. **Clone and Build `llama.cpp` with CUDA Support**  \n",
    "   - Retrieves the `llama.cpp` repository and compiles it with CUDA for GPU acceleration.\n",
    "\n",
    "4. **Install Python Requirements**  \n",
    "   - Installs required Python libraries from `requirements.txt`.\n",
    "\n",
    "5. **Download the Model from Hugging Face**  \n",
    "   - Uses `git lfs` to clone the model repository.\n",
    "\n",
    "6. **Convert the Model to GGUF Format**  \n",
    "   - Converts the original Hugging Face model into GGUF format for compatibility with `llama.cpp`.\n",
    "\n",
    "7. **Apply Quantization**  \n",
    "   - Uses `llama.cpp`'s `quantize` tool to generate optimized versions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ 4- ExLlamaV2: The Fastest Library to Run LLMs  \n",
    "\n",
    "GPTQ delivers amazing performance on GPUs. Compared to unquantized models, this method uses almost **3√ó less VRAM** while maintaining similar accuracy and faster generation. It has become so popular that it is now directly integrated into the **transformers** library.  \n",
    "\n",
    "## ‚ö° What is ExLlamaV2?  \n",
    "ExLlamaV2 is a library designed to **maximize performance** for GPTQ models. It features:  \n",
    "- **Optimized kernels** for faster inference  \n",
    "- **EXL2 quantization format**, offering greater flexibility in weight storage  \n",
    "- **Lower memory usage** with high performance  \n",
    "\n",
    "## üîß Installation  \n",
    "\n",
    "To install **ExLlamaV2**, run the following commands:  \n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/turboderp/exllamav2\n",
    "\n",
    "# Install the library\n",
    "pip install exllamav2\n",
    "```\n",
    "\n",
    "We download zephyr-7B-beta using the following command (this can take a while since the model is about 15 GB): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mention before the GPTQ algorithm  requires a calibration dataset, which is used to measure the impact of the quantization process by comparing the outputs of the base model and its quantized version. We will use the wikitext dataset and directly download the test file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git lfs install\n",
    "! git clone https://huggingface.co/HuggingFaceH4/zephyr-7b-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Converting Models with ExLlamaV2  \n",
    "\n",
    "Once ExLlamaV2 is installed, we can use the **`convert.py`** script to convert a model into the optimized EXL2 format.  \n",
    "\n",
    "## üéØ Required Arguments for Conversion  \n",
    "\n",
    "The script requires **four main parameters**:  \n",
    "\n",
    "- **`-i`** ‚Üí Path to the base model in **Hugging Face (HF) format (FP16)**.  \n",
    "- **`-o`** ‚Üí Path to the working directory where **temporary files and final output** will be stored.  \n",
    "- **`-c`** ‚Üí Path to the **calibration dataset** in **Parquet format**.  \n",
    "- **`-b`** ‚Üí Target **average bits per weight (bpw)** (e.g., `4.0` for 4-bit precision).  \n",
    "\n",
    "## üõ†Ô∏è Example Command  \n",
    "\n",
    "```bash\n",
    "python exllamav2/convert.py \\\n",
    "    -i path/to/base_model \\\n",
    "    -o path/to/output_directory \\\n",
    "    -c path/to/calibration_dataset.parquet \\\n",
    "    -b 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir quant\n",
    "! python python exllamav2/convert.py \\\n",
    "    -i base_model \\\n",
    "    -o quant \\\n",
    "    -c wikitext-test.parquet \\\n",
    "    -b 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñ•Ô∏è GPU Requirements for Quantization  \n",
    "\n",
    "- **7B models** need **~8GB VRAM**, while **70B models** require **~24GB VRAM**.  \n",
    "- On **Google Colab (T4 GPU)**, quantizing **zephyr-7b-beta** took **~2h 10min**.  \n",
    "\n",
    "# üîç Why Use EXL2 Over GPTQ?  \n",
    "\n",
    "**EXL2** improves **GPTQ** by offering:  \n",
    "‚úÖ Support for **2, 3, 4, 5, 6, and 8-bit quantization** (not just 4-bit).  \n",
    "‚úÖ **Mixed-precision layers**, preserving critical weights with higher bits.  \n",
    "‚úÖ **Adaptive error minimization**, optimizing quantization for accuracy & efficiency.  \n",
    "\n",
    "# üìä How Does ExLlamaV2 Optimize Quantization?  \n",
    "\n",
    "ExLlamaV2 **benchmarks multiple quantization parameters**, tracking errors & precision levels.  \n",
    "- Example: A **layer can mix 5% 3-bit & 95% 2-bit precision** for an **average 2.188 bpw**.  \n",
    "- Results are stored in **`measurement.json`**, aiding in optimal quantization selection.  \n",
    "\n",
    "\n",
    "## ü¶ô Running ExLlamaV2 for Inference\n",
    "Now that our model is quantized, we want to run it to see how it performs. Before that, we need to copy essential config files from the base_model directory to the new quant directory. Basically, we want every file that is not hidden (.*) or a safetensors file. Additionally, we don‚Äôt need the out_tensor directory that was created by ExLlamaV2 during quantization.\n",
    "\n",
    "In bash, you can implement this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf quant/out_tensor\n",
    "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Running Our EXL2 Model  \n",
    "\n",
    "Our **EXL2 model** is ready! The most straightforward way to run it is using the `test_inference.py` script from the **ExLlamaV2** repo.  \n",
    "\n",
    "### üèÉ Quick Inference Command:  \n",
    "```bash\n",
    "python exllamav2/test_inference.py -m quant/ -p \"I have a dream\"\n",
    "```\n",
    "‚úÖ **`-m quant/`**: Specifies the directory containing the quantized model.  \n",
    "‚úÖ **`-p \"I have a dream\"`**: Provides the input prompt for inference.  \n",
    "\n",
    "‚ö° **Performance**:  \n",
    "- **56.44 tokens/second** on a **T4 GPU**‚Äîfaster than other quantization methods like **GGUF/llama.cpp** or **GPTQ**.  \n",
    "- For a detailed comparison, check out this excellent [article by oobabooga](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/).  \n",
    "\n",
    "### üîç NF4 vs. GGML vs. GPTQ  \n",
    "<p align=\"center\">\n",
    "  <img src=\"images/compare.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó®Ô∏è Running EXL2 in Chat Mode  \n",
    "\n",
    "For a more **interactive** experience, use the `chat.py` script:  \n",
    "```bash\n",
    "python exllamav2/examples/chat.py -m quant -mode llama\n",
    "```\n",
    "‚úÖ **`-m quant`**: Specifies the quantized model directory.  \n",
    "‚úÖ **`-mode llama`**: Enables LLaMA-style chat functionality.  \n",
    "\n",
    "### üîó Integrations & Requirements  \n",
    "If you plan to use EXL2 models **regularly**, ExLlamaV2 is integrated into multiple backends, including:  \n",
    "- **oobabooga‚Äôs text generation web UI** üñ•Ô∏è  \n",
    "\n",
    "‚ö†Ô∏è **Performance Tip**:  \n",
    "To maximize efficiency, **FlashAttention 2** is required. This currently needs **CUDA 12.1** on Windows, which can be configured during installation.  \n",
    "\n",
    "\n",
    "# üì§ Uploading to Hugging Face Hub  \n",
    "Now that we‚Äôve tested the model, we‚Äôre ready to **upload it to Hugging Face**!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "notebook_login()\n",
    "api = HfApi()\n",
    "api.create_repo(\n",
    "    repo_id=f\"mlabonne/zephyr-7b-beta-5.0bpw-exl2\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "api.upload_folder(\n",
    "    repo_id=f\"mlabonne/zephyr-7b-beta-5.0bpw-exl2\",\n",
    "    folder_path=\"quant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Model Successfully Uploaded to Hugging Face Hub  \n",
    "\n",
    "The model is now available on **Hugging Face**! üöÄ  \n",
    "\n",
    "üìç **Find it here**:  \n",
    "üîó [Zephyr-7B-Beta-5.0bpw-EXL2](https://huggingface.co/mlabonne/zephyr-7b-beta-5.0bpw-exl2)  \n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Flexible & Hardware-Friendly Quantization  \n",
    "\n",
    "The notebook provides a **generalized approach** to quantization, allowing you to:  \n",
    "‚úÖ **Quantize different models** üß†  \n",
    "‚úÖ **Experiment with various `bpw` values** üéõÔ∏è  \n",
    "‚úÖ **Optimize for specific hardware** üíª  \n",
    "\n",
    "This makes it ideal for **tailoring models** to your **device‚Äôs capabilities**!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ **Conclusion: Mastering LLM Quantization in Practice**  \n",
    "\n",
    "Congratulations! üéâ You've successfully explored **practical quantization techniques** for **LLMs**, following up on the **foundations from Notebook 6.3**.  \n",
    "\n",
    "In this notebook, we implemented **Post-Training Quantization (PTQ)** using various frameworks:  \n",
    "‚úÖ **bitsandbytes** üèóÔ∏è  \n",
    "‚úÖ **GPTQ** üöÄ  \n",
    "‚úÖ **GGML & llama.cpp** ü¶ô  \n",
    "‚úÖ **ExLlamaV2** ‚ö° (the fastest!)  \n",
    "\n",
    "You've learned how to **optimize models for efficiency**, balancing memory savings with performance, and even experimented with **cutting-edge EXL2 quantization**.  \n",
    "\n",
    "üîπ **Next Steps?** Try different quantization settings, test models on your hardware, and explore deployment options.  \n",
    "\n",
    "üëè **Well done on making it this far!** You‚Äôre now equipped to apply quantization in real-world projects. üöÄ  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
