{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notebook 6.4: Practical Guide to LLM Quantization Techniques and Frameworks**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "üéâ Welcome to **Notebook 6.4**, where we dive deeper into **quantization techniques** specifically for **large language models (LLMs)**! üöÄ In the previous notebook (**Notebook 6.3**), we implemented a **linear 8-bit quantizer** that was **model-agnostic** and observed the performance improvements for **transformer models**. But that was just the beginning! While **linear quantization** is a great first step, as models keep scaling up and growing larger, we encounter **outliers**‚Äîwhich presents new challenges. \n",
    "\n",
    "üí° **The Challenge?**  \n",
    "As **transformer models** continue to scale up, **outliers** become a major concern. Outliers, in this context, are values that deviate significantly from the expected norm, and when dealing with quantization, they can cause inefficiencies and performance degradation. In simple terms, **outliers** are data points that are much larger or smaller than the rest of the data, often leading to **poor approximation** during the quantization process. The traditional **linear quantization** method struggles with these extreme values, resulting in **limited performance** as the model size increases.  \n",
    "\n",
    "üí° **The Solution?**  \n",
    "To address these issues, we need more advanced quantization techniques that can better handle the presence of **outliers**. This is where **Post-Training Quantization (PTQ)** techniques come into play. PTQ offers powerful methods to optimize models after they have been trained, without requiring retraining. PTQ can adapt to the outlier problem and **maintain model accuracy** even with larger and more complex models.\n",
    "\n",
    "### **Why This Notebook?**  \n",
    "In this notebook, we will explore **advanced quantization techniques** that tackle the **outlier problem** and utilize **Post-Training Quantization (PTQ)**. Specifically, we will look into techniques and frameworks that focus on **dynamic** and **static quantization** methods, both of which are highly effective for **large-scale models** like LLMs. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/Q_TECH.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## **What‚Äôs Inside?**\n",
    "\n",
    "üîç Here‚Äôs what we will cover in this notebook:  \n",
    "\n",
    "### **1Ô∏è‚É£ Understanding the Outlier Problem in Quantization**  \n",
    "‚úÖ What are **outliers**, and why do they affect **large model quantization**?  \n",
    "‚úÖ How do **outliers** impact performance when using **linear quantization**?  \n",
    "\n",
    "### **2Ô∏è‚É£ Overview of Quantization Techniques for LLMs**  \n",
    "‚úÖ **QAT (Quantization-Aware Training)** vs **PTQ (Post-Training Quantization)**  \n",
    "‚úÖ When to use **PTQ** and why it‚Äôs well-suited for large models  \n",
    "\n",
    "### **3Ô∏è‚É£ Static vs Dynamic Quantization**  \n",
    "‚úÖ Understanding the difference between **static** and **dynamic quantization**  \n",
    "‚úÖ How these methods work with **Post-Training Quantization**  \n",
    "\n",
    "### **4Ô∏è‚É£ Exploring PTQ Frameworks and Tools**  \n",
    "‚úÖ Overview of popular **PTQ frameworks** and **tools**  \n",
    "‚úÖ How these frameworks help manage outliers and improve model efficiency  \n",
    "\n",
    "### **5Ô∏è‚É£ Implementing PTQ in Large Language Models**  \n",
    "‚úÖ Practical guide to applying PTQ to **transformer models**  \n",
    "‚úÖ Step-by-step walkthrough using a **PTQ framework**  \n",
    "\n",
    "## **Why This Notebook Matters**\n",
    "\n",
    "Large language models are becoming increasingly powerful, but their deployment in resource-constrained environments is a challenge. **Quantization** is key to **making these models smaller, faster, and more efficient**, but as models scale, the **outlier problem** becomes more prominent. In this notebook, you will:\n",
    "\n",
    "‚úÖ Learn about the **outlier problem** in large-scale model quantization  \n",
    "‚úÖ Understand the distinction between **QAT** and **PTQ**.\n",
    "‚úÖ Gain hands-on experience with **static** and **dynamic quantization** techniques  \n",
    "‚úÖ Explore leading **PTQ frameworks** and how they handle outliers  \n",
    "‚úÖ Apply PTQ techniques to **real-world transformer models**  \n",
    "\n",
    "## **Looking Ahead**\n",
    "\n",
    "With the techniques and frameworks covered in this notebook, you‚Äôll be able to optimize **large language models** with **Post-Training Quantization** and make them more deployable. But we‚Äôre not stopping here‚Äîthere are still many advanced quantization strategies to explore in future notebooks, as the field is rapidly evolving.\n",
    "\n",
    "Ready to tackle the **outlier problem** and unlock the true potential of **large language model quantization**? Let‚Äôs dive in! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Difference Between QAT and PTQ, Static and Dynamic Quantization**\n",
    "\n",
    "### **QAT (Quantization-Aware Training) vs PTQ (Post-Training Quantization)**\n",
    "\n",
    "- **QAT (Quantization-Aware Training)**:\n",
    "  - **Definition**: QAT involves simulating quantization during the training process. The model learns to adapt to the quantization noise by training with quantized weights and activations.\n",
    "  - **Use Case**: Best suited when training a model from scratch or fine-tuning it. The model becomes aware of the quantization effects, leading to better accuracy after quantization.\n",
    "  - **Pros**: Retains model accuracy after quantization.\n",
    "  - **Cons**: Requires retraining, which can be computationally expensive and time-consuming.\n",
    "\n",
    "- **PTQ (Post-Training Quantization)**:\n",
    "  - **Definition**: PTQ quantizes a pre-trained model without the need for retraining. This is done after the model has been fine-tuned, typically by converting the weights to lower precision (e.g., INT8).\n",
    "  - **Use Case**: Ideal for deploying pre-trained models efficiently without needing retraining.\n",
    "  - **Pros**: Faster and less computationally expensive than QAT.\n",
    "  - **Cons**: May lead to a slight drop in accuracy due to the lack of fine-tuning for quantization.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/QAT_PTQ.png\" alt=\"Quantization Overview\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Static Quantization vs Dynamic Quantization**\n",
    "\n",
    "- **Static Quantization**:\n",
    "  - **Definition**: Static quantization involves calibrating the model with a representative dataset to determine the optimal quantization parameters (like scaling factors) for weights and activations.\n",
    "  - **Use Case**: Used in situations where the entire model is quantized upfront and requires careful tuning.\n",
    "  - **Pros**: Results in the most efficient models with maximum compression.\n",
    "  - **Cons**: Requires calibration data, which may not always be available.\n",
    "\n",
    "- **Dynamic Quantization**:\n",
    "  - **Definition**: Dynamic quantization performs quantization dynamically during inference, adjusting the precision of activations and weights as needed.\n",
    "  - **Use Case**: Ideal when you want quick deployment without requiring a calibration step.\n",
    "  - **Pros**: Easier to implement and faster than static quantization.\n",
    "  - **Cons**: Might be less efficient in terms of memory and speed compared to static quantization.\n",
    "\n",
    "### **Our Focus: PTQ for LLM Deployment**\n",
    "\n",
    "In this notebook, we will focus on **Post-Training Quantization (PTQ)**, as our goal is to optimize **large language models (LLMs)** for **deployment after fine-tuning**. PTQ allows us to efficiently reduce model size and improve inference speed without retraining the model, which is key for deploying large models in resource-constrained environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1- BitsAndBytes**\n",
    "\n",
    "- **Definition**: `BitsAndBytes` is a library designed for efficient model quantization, supporting **Post-Training Quantization (PTQ)** and **Quantization-Aware Training (QAT)**.\n",
    "  \n",
    "- **Core Functionality**:\n",
    "  - **Quantization**: Provides advanced techniques such as 8-bit and mixed precision quantization for optimizing large models.\n",
    "  - **Efficient Memory Usage**: Reduces model size and memory footprint without sacrificing accuracy.\n",
    "  - **Performance Optimization**: Increases inference speed by quantizing weights and activations to lower bit precision.\n",
    "\n",
    "- **Under the Hood**:\n",
    "  - **Vector-wise Quantization (Dot Product)**: Uses **vector-wise quantization** where the weights are quantized using **dot products**, improving both memory efficiency and computational speed.\n",
    "  - **Mixed Precision**: Supports **mixed precision quantization**, combining high-precision weights with low-precision activations for optimal performance and accuracy.\n",
    "  - **Quantize Optimizers**: Utilizes specialized optimizers to adapt weights during quantization and fine-tuning to minimize performance loss.\n",
    "  - **NF4 (Static Quantization for QLoRA Fine-Tuning)**: Implements **NF4 quantization** for fine-tuning large models under **QLoRA** (Quantized Low-Rank Adaptation) using **static quantization** techniques to further optimize large-scale models.\n",
    "  \n",
    "- **Support for Multiple Frameworks**: Compatible with PyTorch, TensorFlow, and other ML frameworks for flexible deployment.\n",
    "\n",
    "- the simplest way to use it is within the transformer library as follow : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear8bitLt(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear8bitLt(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1541748046875 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the original model with no BitsandBites and see the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.466552734375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a big diffrent is size \n",
    "\n",
    "Additionally BitsAndBytes even let you load the model in 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advanced Usage**  \n",
    "\n",
    "You can experiment with different **4-bit quantization** variants, such as:  \n",
    "\n",
    "- **NF4 (Normalized Float 4, default)** ‚Äì Recommended for better performance based on theoretical and empirical results.  \n",
    "- **FP4 (Pure Floating-Point 4-bit Quantization)** ‚Äì An alternative option.  \n",
    "\n",
    "### **Additional Optimization Options**  \n",
    "- **`bnb_4bit_use_double_quant`**: Enables a second quantization pass, reducing memory usage by an additional **0.4 bits per parameter**.  \n",
    "- **Compute Precision**: While weights are stored in **4-bit**, computations can still be performed in higher precision:\n",
    "  - **`float16`** (Faster training, commonly used)  \n",
    "  - **`bfloat16`**  \n",
    "  - **`float32`** (Default)  \n",
    "\n",
    "Using **`float16`** as the compute type speeds up matrix multiplication and training.\n",
    "\n",
    "### **Example: Load a Model in 4-bit Using NF4 & Double Quantization**  \n",
    "\n",
    "To customize these parameters, we use `BitsAndBytesConfig` from **Hugging Face Transformers**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model facebook/opt-125m loaded successfully with 8-bit quantization!\n",
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x OPTDecoderLayer(\n",
      "          (self_attn): OPTSdpaAttention(\n",
      "            (k_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n",
      "0.1146240234375 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Configure 8-bit quantization\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, ## load in 4 bits\n",
    "         bnb_4bit_quant_type=\"nf4\", ## the type of quantization\n",
    "        bnb_4bit_use_double_quant=True, ## double quant\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, ## bfloat for computaion during inference\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load the model with the quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config_8bit,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Model {model_name} loaded successfully with 8-bit quantization!\")\n",
    "print(model)\n",
    "\n",
    "print(model.get_memory_footprint() / 1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.GPTQ\n",
    "\n",
    "GPTQ (Generalized Post-Training Quantization) is an advanced quantization technique designed to efficiently reduce the precision of large language models (LLMs) while maintaining high accuracy. It enables **fast inference** and **lower memory usage** by using **4-bit weight quantization with error correction**.\n",
    "\n",
    "## The problem : \n",
    "Traditional quantization methods (like uniform or na√Øve rounding) can introduce significant errors when converting high-precision weights (e.g., FP16 or FP32) into low-bit representations (e.g., INT4). These errors can accumulate and cause a noticeable drop in performance.\n",
    "\n",
    "## Soultion: \n",
    "GPTQ solves this issue using Optimal Brain Surgeon (OBS) principles, which adjust other weights after quantizing each weight to minimize the overall error. The key idea is:\n",
    "\n",
    "- **Quantize One Weight at a Time**: Instead of naive rounding, GPTQ processes weights sequentially to minimize overall error.\n",
    "- **Error Correction via Hessian Approximation**: Adjusts remaining weights dynamically to compensate for quantization error.\n",
    "- **Optimal Brain Surgeon (OBS) Principles**: Uses second-order optimization for best quantization results.\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "GPTQ minimizes the activation reconstruction error:\n",
    "```python\n",
    "# X: Input activations\n",
    "# W: Original weight matrix\n",
    "# W_tilde: Quantized version\n",
    "\n",
    "def activation_reconstruction_error(X, W, W_tilde):\n",
    "    return torch.norm(X @ W - X @ W_tilde, p='fro')**2\n",
    "```\n",
    "Instead of a naive quantization:\n",
    "```python\n",
    "# Naive quantization\n",
    "W_tilde[i, j] = quantize(W[i, j])\n",
    "```\n",
    "GPTQ finds the best quantized weight while adjusting other weights using an **Hessian-based update**:\n",
    "```python\n",
    "# Hessian-based update\n",
    "w_new = w_old - torch.inverse(H) @ e_j * (q_j - w_j)\n",
    "```\n",
    "where `H` is the Hessian approximation capturing sensitivity.\n",
    "\n",
    "## **Algorithm Steps**\n",
    "1. **Compute Activations**: Use a small dataset to estimate activations `X`.\n",
    "2. **Estimate Hessian**: Compute `H = (X.T @ X) / N`.\n",
    "3. **Iterate Over Weights**:\n",
    "   - Find the best quantized value `q_j` for each weight.\n",
    "   - Update remaining weights using Hessian inverse correction.\n",
    "4. **Repeat Until All Weights Are Quantized**.\n",
    "\n",
    "\n",
    "## **Benefits of GPTQ**\n",
    "‚úÖ **4-bit Quantization** with minimal accuracy loss  \n",
    "‚úÖ **Faster Inference** due to smaller models  \n",
    "‚úÖ **Memory Efficient** for edge and server deployment  \n",
    "‚úÖ **Better Than Naive Rounding** through Hessian correction  \n",
    "\n",
    "\n",
    "Enough theory lets dive into the code : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize (in this case, GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to load the model and the tokenizer. The tokenizer is loaded using the classic AutoTokenizer class from the transformers library. On the other hand, we need to pass a specific configuration (BaseQuantizeConfig) to load the model.\n",
    "\n",
    "In this configuration, we can specify the number of bits to quantize (here, bits=4) and the group size (size of the lazy batch). Note that this group size is optional: we could also use one set of parameters for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost (especially with group_size=1024). The damp_percent value is here to help the Cholesky reformulation and should not be changed.\n",
    "\n",
    "Finally, the desc_act (also called act order) is a tricky parameter. It allows you to process rows based on decreasing activation, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won‚Äôt use it here (it will probably be fixed in the future, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f954e9cfe12d42a484638f726303785e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silva/anaconda3/envs/MyLLM/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358df74bbff342de93dc8742a4526e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afae28d138f4ecba2503708eb1c08ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019025f183ff4e39964483dcb33097a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dc07a972674a77a886927678b8a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99121cb5aa464da8bf96e0e93453bf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58c57bc8ed34d3e952eaf5c48dd7b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load quantize config, model and tokenizer\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,\n",
    ")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization process relies heavily on samples to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality.\n",
    "\n",
    "In the context of this article, we utilize the C4 (Colossal Clean Crawled Corpus) dataset to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large-scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option.\n",
    "\n",
    "In the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadb1be269f142b39fc3b14150f692d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642da0dd35304fa9ba692e3dce7c2634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c4-train.00001-of-01024.json.gz:   0%|          | 0.00/318M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6531da6500f48d689f8be38972a6066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load data and tokenize examples\n",
    "n_samples = 1024\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use OpenAI Triton, a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/12\n",
      "INFO - Quantizing attn.c_attn in layer 1/12...\n",
      "INFO - Quantizing attn.c_proj in layer 1/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 1/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 1/12...\n",
      "INFO - Start quantizing layer 2/12\n",
      "INFO - Quantizing attn.c_attn in layer 2/12...\n",
      "INFO - Quantizing attn.c_proj in layer 2/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 2/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 2/12...\n",
      "INFO - Start quantizing layer 3/12\n",
      "INFO - Quantizing attn.c_attn in layer 3/12...\n",
      "INFO - Quantizing attn.c_proj in layer 3/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 3/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 3/12...\n",
      "INFO - Start quantizing layer 4/12\n",
      "INFO - Quantizing attn.c_attn in layer 4/12...\n",
      "INFO - Quantizing attn.c_proj in layer 4/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 4/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 4/12...\n",
      "INFO - Start quantizing layer 5/12\n",
      "INFO - Quantizing attn.c_attn in layer 5/12...\n",
      "INFO - Quantizing attn.c_proj in layer 5/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 5/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 5/12...\n",
      "INFO - Start quantizing layer 6/12\n",
      "INFO - Quantizing attn.c_attn in layer 6/12...\n",
      "INFO - Quantizing attn.c_proj in layer 6/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 6/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 6/12...\n",
      "INFO - Start quantizing layer 7/12\n",
      "INFO - Quantizing attn.c_attn in layer 7/12...\n",
      "INFO - Quantizing attn.c_proj in layer 7/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 7/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 7/12...\n",
      "INFO - Start quantizing layer 8/12\n",
      "INFO - Quantizing attn.c_attn in layer 8/12...\n",
      "INFO - Quantizing attn.c_proj in layer 8/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 8/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 8/12...\n",
      "INFO - Start quantizing layer 9/12\n",
      "INFO - Quantizing attn.c_attn in layer 9/12...\n",
      "INFO - Quantizing attn.c_proj in layer 9/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 9/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 9/12...\n",
      "INFO - Start quantizing layer 10/12\n",
      "INFO - Quantizing attn.c_attn in layer 10/12...\n",
      "INFO - Quantizing attn.c_proj in layer 10/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 10/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 10/12...\n",
      "INFO - Start quantizing layer 11/12\n",
      "INFO - Quantizing attn.c_attn in layer 11/12...\n",
      "INFO - Quantizing attn.c_proj in layer 11/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 11/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 11/12...\n",
      "INFO - Start quantizing layer 12/12\n",
      "INFO - Quantizing attn.c_attn in layer 12/12...\n",
      "INFO - Quantizing attn.c_proj in layer 12/12...\n",
      "INFO - Quantizing mlp.c_fc in layer 12/12...\n",
      "INFO - Quantizing mlp.c_proj in layer 12/12...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 44s, sys: 29min 34s, total: 42min 19s\n",
      "Wall time: 42min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-GPTQ/tokenizer_config.json',\n",
       " 'gpt2-GPTQ/special_tokens_map.json',\n",
       " 'gpt2-GPTQ/vocab.json',\n",
       " 'gpt2-GPTQ/merges.txt',\n",
       " 'gpt2-GPTQ/added_tokens.json',\n",
       " 'gpt2-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After , the model can loaded from the output directory using the AutoGPTQForCausalLM and AutoTokenizer classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
      "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
      "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
      "2. You are using pytorch without CUDA support.\n",
      "3. CUDA and nvcc are not installed in your device.\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the model working properly after the quantization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream to make it to one of the first theaters in the country. They are not even close and I have such great plans. I never met anyone here before and it is as good as my dreams. People just need to learn how to be the biggest stars that they can.\"\n",
      "\n",
      "\"It took seven years to come together to create an audience that many will like. It is what I have always dreamed of,\" says his brother, Eric, of \"Avengers: Infinity\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=100)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
