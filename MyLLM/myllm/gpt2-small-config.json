{
    "name": "gpt2-small",
    "block_size": 1024,
    "vocab_size": 50257,
    "padded_vocab_size": 50257,
    "n_layer": 12,
    "n_head": 12,
    "n_embd": 768,
    "eps": 1e-05,
    "norm_class_name": "LayerNorm",
    "activation": "gelu",
    "mlp_class_name": "GptMLP",
    "scale_embeddings": true,
    "mlp_ratio": 4.0,
    "rotary_percentage": 0.0,
    "parallel_residual": false,
    "norm_eps": 1e-05,
    "dropout": 0.1,
    "bias": false,
    "learning_rate": 0.0003,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.999,
    "extra_params": {}
}