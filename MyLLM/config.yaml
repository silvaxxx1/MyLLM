# Configuration Management for Transformer-based Models

# This module defines a `Config` class for managing the configuration settings of various
# transformer-based models, such as GPT-2 and LLaMA. The `Config` class provides
# functionality for:

# 1. Core Parameters:
#     - block_size: Sequence length for input data.
#     - vocab_size: The number of tokens in the vocabulary.
#     - n_layer: Number of transformer layers.
#     - n_head: Number of attention heads.
#     - n_embd: Embedding dimensionality.
#     - Other model-specific parameters.

# 2. Architecture Variations:
#     - Options for different normalization layers (`LayerNorm`, `RMSNorm`).
#     - Activation functions like `gelu`, `relu`, etc.
#     - Customizable MLP configurations (`GptNeoxMLP`, `LLaMAMLP`, etc.).
#     - Parallel residual connections, rotary embeddings, and other LLaMA-specific features.

# 3. Hyperparameters:
#     - Dropout rate for regularization.
#     - Learning rate, weight decay, and optimizer settings (e.g., Adam optimizer).
#     - Additional model-specific hyperparameters like attention bias, dropout rates, and more.

# 4. Configuration Management:
#     - Save and load configurations from JSON files for easy configuration management.
#     - Dynamic updating of configuration parameters.

# 5. Validation:
#     - Validation checks to ensure compatibility of certain parameters (e.g., ensuring
#       that `n_embd` is divisible by `n_head` for correct attention behavior).

# 6. Trainable Parameters:
#     - A method to retrieve only the trainable parameters from the configuration
#       (those that are `int`, `float`, or `bool`).

# 7. Multi-Model Support:
#     - The ability to manage multiple model configurations (e.g., GPT-2, LLaMA) using
#       a configuration registry.

# ## Example Usage

# - **Creating a Config instance**:
#     `Config.from_name("gpt2-small")` creates a `Config` instance for the GPT-2 small model.

# - **Validating the configuration**:
#     `config.validate()` ensures that the configuration parameters are valid.

# - **Saving and loading configurations**:
#     `config.save("config.json")` saves the configuration to a JSON file, while
#     `Config.load("config.json")` loads it from a file.

# - **Updating configuration parameters**:
#     `config.update(block_size=2048, learning_rate=1e-4)` dynamically updates specific parameters.

# - **Getting trainable parameters**:
#     `config.get_trainable_params()` returns a dictionary of trainable parameters.

# ## Attributes
# - `name`: Name of the model configuration.
# - `block_size`: Size of each sequence block.
# - `vocab_size`: The size of the vocabulary.
# - `n_layer`: Number of transformer layers.
# - `n_head`: Number of attention heads.
# - `n_embd`: Dimensionality of the embedding layer.
# - `norm_class_name`: Type of normalization layer used (either `LayerNorm` or `RMSNorm`).
# - `activation`: Activation function used in the model.
# - `learning_rate`: Learning rate for optimization.
# - `weight_decay`: Weight decay used for regularization.
# - `beta1`, `beta2`: Adam optimizer parameters.

# ## Configuration Registry

# The module supports different configurations for various transformer models. You can access
# available configurations via the `available_configs()` method.

# Example configurations include:
# - `"gpt2-small"`, `"gpt2-medium"`, `"gpt2-large"`, `"gpt2-xl"`, `"llama2-7b"`, `"llama2-13b"`, etc.

# ---

# The `Config` class is designed for managing large-scale transformer models with flexible
# settings, allowing easy integration into training pipelines.

# config.py

# from dataclasses import dataclass, field
# from typing import Optional, Any, Dict, Literal, Type
# import json
# import torch

Config:
  name: ""
  block_size: 1024
  vocab_size: 50257
  padded_vocab_size: null
  n_layer: 12
  n_head: 12
  n_embd: 768
  eps: 1.0e-05
  head_size: null
  norm_class_name: "LayerNorm"
  activation: "gelu"
  mlp_class_name: "GptNeoxMLP"
  scale_embeddings: false
  mlp_ratio: 4.0
  lm_head_bias: false
  attention_bias: false
  bias: false
  mlp_hidden_size: null
  post_mlp_norm: false
  gelu_approx: "none"
  causal_attention: true
  rotary_percentage: 0.0
  parallel_residual: false
  shared_attention_norm: false
  norm_eps: 1.0e-05
  n_query_groups: 32
  norm_qk: false
  use_rope: false
  rope_base: 10000
  attention_scores_scalar: null
  softcapping_threshold: null
  attention_logit_softcapping: null
  post_attention_norm: false
  dropout: 0.1
  learning_rate: 0.0003
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.999
  extra_params: {}

# Configuration Registry
name_to_config:
  gpt2-small:
    name: "gpt2-small"
    block_size: 1024
    vocab_size: 50257
    n_layer: 12
    n_head: 12
    n_embd: 768
    norm_class_name: "LayerNorm"
    mlp_class_name: "GptMLP"
    activation: "gelu"
    scale_embeddings: true
  gpt2-medium:
    name: "gpt2-medium"
    block_size: 1024
    vocab_size: 50257
    n_layer: 24
    n_head: 16
    n_embd: 1024
    norm_class_name: "LayerNorm"
    mlp_class_name: "GptMLP"
    activation: "gelu"
    scale_embeddings: true
  gpt2-large:
    name: "gpt2-large"
    block_size: 1024
    vocab_size: 50257
    n_layer: 36
    n_head: 20
    n_embd: 1280
    norm_class_name: "LayerNorm"
    mlp_class_name: "GptMLP"
    activation: "gelu"
    scale_embeddings: true
  gpt2-xl:
    name: "gpt2-xl"
    block_size: 1024
    vocab_size: 50257
    n_layer: 48
    n_head: 25
    n_embd: 1600
    norm_class_name: "LayerNorm"
    mlp_class_name: "GptMLP"
    activation: "gelu"
    scale_embeddings: true
  llama2-7b:
    name: "llama2-7b"
    block_size: 4096
    vocab_size: 32000
    n_layer: 32
    n_head: 32
    n_embd: 4096
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  llama2-13b:
    name: "llama2-13b"
    block_size: 4096
    vocab_size: 32000
    n_layer: 40
    n_head: 40
    n_embd: 5120
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  llama3-1b:
    name: "llama3-1b"
    block_size: 8192
    vocab_size: 128256
    n_layer: 24
    n_head: 16
    n_embd: 2048
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  llama3-3b:
    name: "llama3-3b"
    block_size: 8192
    vocab_size: 128256
    n_layer: 32
    n_head: 32
    n_embd: 3072
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  llama3-8b:
    name: "llama3-8b"
    block_size: 8192
    vocab_size: 128256
    n_layer: 32
    n_head: 32
    n_embd: 4096
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  llama3-70b:
    name: "llama3-70b"
    block_size: 8192
    vocab_size: 128256
    n_layer: 80
    n_head: 64
    n_embd: 8192
    norm_class_name: "RMSNorm"
    mlp_class_name: "LLaMAMLP"
    rotary_percentage: 1.0
    parallel_residual: true
    norm_eps: 1.0e-05
  gpt-neox-125m:
    name: "gpt-neox-125m"
    block_size: 2048
    vocab_size: 50257
    n_layer: 12
    n_head: 12
    n_embd: 768
    norm_class_name: "LayerNorm"
    mlp_class_name: "GptNeoxMLP"
    activation: "gelu"
    scale_embeddings: true