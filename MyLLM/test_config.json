{
    "name": "gpt2-xl",
    "block_size": 1024,
    "vocab_size": 50257,
    "padded_vocab_size": 50257,
    "n_layer": 48,
    "n_head": 25,
    "n_embd": 1600,
    "eps": 1e-05,
    "head_size": 64,
    "norm_class_name": "LayerNorm",
    "activation": "gelu",
    "mlp_class_name": "GptMLP",
    "scale_embeddings": true,
    "mlp_ratio": 4.0,
    "lm_head_bias": false,
    "attention_bias": false,
    "bias": false,
    "mlp_hidden_size": 6400,
    "post_mlp_norm": false,
    "gelu_approx": "none",
    "causal_attention": true,
    "rotary_percentage": 0.0,
    "parallel_residual": false,
    "shared_attention_norm": false,
    "norm_eps": 1e-05,
    "n_query_groups": 25,
    "norm_qk": false,
    "use_rope": false,
    "rope_base": 10000,
    "attention_scores_scalar": null,
    "softcapping_threshold": null,
    "attention_logit_softcapping": null,
    "post_attention_norm": false,
    "dropout": 0.1,
    "learning_rate": 0.0003,
    "weight_decay": 0.1,
    "beta1": 0.9,
    "beta2": 0.999,
    "extra_params": {}
}